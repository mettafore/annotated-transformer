# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_attention_is_all_you_need_dup1.ipynb.

# %% auto 0
__all__ = ['attention', 'MultiHeadedAttention', 'PositionwiseFeedForward', 'PositionalEncoding', 'SublayerConnection',
           'EncoderLayer', 'clones', 'Encoder', 'DecoderLayer', 'Decoder', 'Embeddings', 'Generator', 'EncoderDecoder']

# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 2
import torch
import math
import copy

# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 24
def attention(Q, K, V, dropout, mask):
    sqrt_d_k = math.sqrt(K.size(-1))
    scores = Q @ K.transpose(-2,-1) / sqrt_d_k
    if mask is not None:
        scores = scores.masked_fill(mask==0, -1e9)
    attention_weights = torch.softmax(scores, dim=-1)
    attention_weights = dropout(attention_weights)
    output = attention_weights @ V
    return output

# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 91
class MultiHeadedAttention(torch.nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        super().__init__()
        assert d_model % h == 0  # d_model must be divisible by h
        
        self.d_k = d_model // h
        self.h = h
        self.dropout = torch.nn.Dropout(p=dropout)
        self.attn=None
        
        # Create 4 linear layers (Q, K, V projections + final output)
        # Hint: you can use a list or create them individually
        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(d_model, d_model) for x in range(4)])
    
    def forward(self, query, key, value, mask=None):
        # Step 1: Project query, key, value using the first 3 linear layers
        Q = self.linear_layers[0](query)
        K = self.linear_layers[1](key)
        V = self.linear_layers[2](value)
        # Step 2: Reshape to split into h heads
        batch_size = Q.size(0)
        Q = Q.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)
        K = K.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)
        V = V.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)
        # Step 3: Apply attention
        if mask is not None:
            mask = mask.unsqueeze(1)
        x = attention(Q,K,V, self.dropout, mask)
        # Step 4: Concatenate heads back together
        x = x.transpose(1,2).reshape(batch_size,-1, self.h * self.d_k)
        # Step 5: Apply final linear projection
        output = self.linear_layers[3](x)
        return output

# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 105
class PositionwiseFeedForward(torch.nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        # What do you need here?
        self.linear_layer = torch.nn.Linear(d_model, d_ff)
        self.relu = torch.nn.ReLU()
        self.dropout = torch.nn.Dropout(p=dropout)
        self.output_layer = torch.nn.Linear(d_ff, d_model)
        
    def forward(self, x):
        # What's the sequence of operations?
        x = self.linear_layer(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.output_layer(x)
        return x

# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 135
class PositionalEncoding(torch.nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        super().__init__()
        self.dropout = torch.nn.Dropout(p=dropout)
        
        # Create pe matrix
        pe = torch.zeros(max_len, d_model)

        even_indices = torch.arange(0, d_model, 2)
        
        # Create position vector - try this yourself!
        position = torch.arange(0, max_len).unsqueeze(1)
        # Calculate div_term - try this yourself!
        div_term = torch.exp(-even_indices*(torch.log(torch.tensor(10000)))/d_model)

        pe[:, ::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    def forward(self, x):
        seq_len = x.size(1)
        x = x + self.pe[:,:seq_len,:].requires_grad_(False)
        return self.dropout(x)

# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 156
class SublayerConnection(torch.nn.Module):
    "A residual connection followed by a layer norm"
    def __init__(self, size, dropout):
        super().__init__()
        # What do you need here?
        self.layer_norm = torch.nn.LayerNorm(size)
        self.dropout = torch.nn.Dropout(dropout)
        
    def forward(self, x, sublayer):
        # What's the operation?
        y = self.layer_norm(x)
        y = sublayer(y)
        y = self.dropout(y)
        return x + y

# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 162
class EncoderLayer(torch.nn.Module):
    def __init__(self, size, self_attn, feed_forward, dropout):
        super().__init__()
        # What do we need to store?
        self.size = size
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = torch.nn.ModuleList([SublayerConnection(size, dropout) for _ in range(2)])
    def forward(self, x, mask):
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        x = self.sublayer[1](x, lambda x: self.feed_forward(x))
        return x




# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 172
def clones(module, N):
    "Produce N identical layers"
    # How would you create N deep copies?
    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 180
class Encoder(torch.nn.Module):
    "Stack of N encoder layers"
    def __init__(self, layer, N):
        super().__init__()
        # What do you need here?
        self.encoders = clones(layer, N)
        self.layer_norm = torch.nn.LayerNorm(layer.size)
    def forward(self, x, mask):
        # What's the sequence of operations?
        for encoder in self.encoders:
            x = encoder(x, mask)
        return self.layer_norm(x)

# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 205
class DecoderLayer(torch.nn.Module):
    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        super().__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayers = clones(SublayerConnection(size, dropout), 3)
    
    def forward(self, x, memory, src_mask, tgt_mask):
        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
        x = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))
        x = self.sublayers[2](x, self.feed_forward)
        return x

# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 219
class Decoder(torch.nn.Module):
    def __init__(self, layer, N):
        super().__init__()
        # What do you need?
        self.layers = clones(layer, N)
        self.layer_norm = torch.nn.LayerNorm(layer.size)
        
    def forward(self, x, memory, src_mask, tgt_mask):
        # What's the sequence of operations?
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.layer_norm(x)
        

# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 231
class Embeddings(torch.nn.Module):
    def __init__(self, d_model, vocab):
        super().__init__()
        # What do you need here?
        self.d_model = d_model
        self.embedding = torch.nn.Embedding(vocab, d_model)
        
    def forward(self, x):
        # What's the operation?
        return self.embedding(x) * math.sqrt(self.d_model)


# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 241
class Generator(torch.nn.Module):
    def __init__(self, d_model, vocab):
        super().__init__()
        # What layer do you need?
        self.linear = torch.nn.Linear(d_model, vocab)
        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)
        
    def forward(self, x):
        # What operation?
        x = self.linear(x)
        return self.logsoftmax(x)



# %% ../nbs/02_attention_is_all_you_need_dup1.ipynb 258
class EncoderDecoder(torch.nn.Module):
    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        super().__init__()
        # Store all 5 components
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator
    def encode(self, src, src_mask):
        x = self.src_embed(src)
        return self.encoder(x, src_mask)
    def decode(self, memory, src_mask, tgt, tgt_mask):
        # What goes here?
        x = self.tgt_embed(tgt)
        return self.decoder(x, memory, src_mask, tgt_mask)
    def forward(self, src, tgt, src_mask, tgt_mask):
        # What goes here?
        memory = self.encode(src, src_mask)
        return self.decode(memory, src_mask, tgt, tgt_mask)
