{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# REFERENCE CODE - Complete implementations\n",
    "# Paper: https://arxiv.org/abs/1706.03762\n",
    "# Harvard NLP: https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "\n",
    "#| export\n",
    "\n",
    "import torch\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import html\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "#| export\n",
    "\n",
    "def attention(Q, K, V, dropout, mask):\n",
    "\n",
    "    sqrt_d_k = math.sqrt(K.size(-1))\n",
    "\n",
    "    scores = Q @ K.transpose(-2,-1) / sqrt_d_k\n",
    "\n",
    "    if mask is not None:\n",
    "\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "\n",
    "    attention_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "    attention_weights = dropout(attention_weights)\n",
    "\n",
    "    output = attention_weights @ V\n",
    "\n",
    "    return output\n",
    "#| export\n",
    "\n",
    "class MultiHeadedAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % h == 0  # d_model must be divisible by h\n",
    "\n",
    "        \n",
    "\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        self.h = h\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        self.attn=None\n",
    "\n",
    "        \n",
    "\n",
    "        # Create 4 linear layers (Q, K, V projections + final output)\n",
    "\n",
    "        # Hint: you can use a list or create them individually\n",
    "\n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(d_model, d_model) for x in range(4)])\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        # Step 1: Project query, key, value using the first 3 linear layers\n",
    "\n",
    "        Q = self.linear_layers[0](query)\n",
    "\n",
    "        K = self.linear_layers[1](key)\n",
    "\n",
    "        V = self.linear_layers[2](value)\n",
    "\n",
    "        # Step 2: Reshape to split into h heads\n",
    "\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        Q = Q.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "        K = K.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "        V = V.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "        # Step 3: Apply attention\n",
    "\n",
    "        if mask is not None:\n",
    "\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        x = attention(Q,K,V, self.dropout, mask)\n",
    "\n",
    "        # Step 4: Concatenate heads back together\n",
    "\n",
    "        x = x.transpose(1,2).reshape(batch_size,-1, self.h * self.d_k)\n",
    "\n",
    "        # Step 5: Apply final linear projection\n",
    "\n",
    "        output = self.linear_layers[3](x)\n",
    "\n",
    "        return output\n",
    "#| export\n",
    "\n",
    "class PositionwiseFeedForward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # What do you need here?\n",
    "\n",
    "        self.linear_layer = torch.nn.Linear(d_model, d_ff)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(d_ff, d_model)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # What's the sequence of operations?\n",
    "\n",
    "        x = self.linear_layer(x)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "#| export\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        \n",
    "\n",
    "        # Create pe matrix\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "\n",
    "\n",
    "        even_indices = torch.arange(0, d_model, 2)\n",
    "\n",
    "        \n",
    "\n",
    "        # Create position vector - try this yourself!\n",
    "\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "        # Calculate div_term - try this yourself!\n",
    "\n",
    "        div_term = torch.exp(-even_indices*(torch.log(torch.tensor(10000)))/d_model)\n",
    "\n",
    "\n",
    "\n",
    "        pe[:, ::2] = torch.sin(position * div_term)\n",
    "\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        x = x + self.pe[:,:seq_len,:].requires_grad_(False)\n",
    "\n",
    "        return self.dropout(x)\n",
    "#| export\n",
    "\n",
    "class SublayerConnection(torch.nn.Module):\n",
    "\n",
    "    \"A residual connection followed by a layer norm\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # What do you need here?\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(size)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "\n",
    "        # What's the operation?\n",
    "\n",
    "        y = self.layer_norm(x)\n",
    "\n",
    "        y = sublayer(y)\n",
    "\n",
    "        y = self.dropout(y)\n",
    "\n",
    "        return x + y\n",
    "#| export\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # What do we need to store?\n",
    "\n",
    "        self.size = size\n",
    "\n",
    "        self.self_attn = self_attn\n",
    "\n",
    "        self.feed_forward = feed_forward\n",
    "\n",
    "        self.sublayer = torch.nn.ModuleList([SublayerConnection(size, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "\n",
    "        x = self.sublayer[1](x, lambda x: self.feed_forward(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#| export\n",
    "\n",
    "def clones(module, N):\n",
    "\n",
    "    \"Produce N identical layers\"\n",
    "\n",
    "    # How would you create N deep copies?\n",
    "\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "#| export\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "\n",
    "    \"Stack of N encoder layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # What do you need here?\n",
    "\n",
    "        self.encoders = clones(layer, N)\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        # What's the sequence of operations?\n",
    "\n",
    "        for encoder in self.encoders:\n",
    "\n",
    "            x = encoder(x, mask)\n",
    "\n",
    "        return self.layer_norm(x)\n",
    "#| export\n",
    "\n",
    "class DecoderLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = size\n",
    "\n",
    "        self.self_attn = self_attn\n",
    "\n",
    "        self.src_attn = src_attn\n",
    "\n",
    "        self.feed_forward = feed_forward\n",
    "\n",
    "        self.sublayers = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "\n",
    "        x = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n",
    "\n",
    "        x = self.sublayers[2](x, self.feed_forward)\n",
    "\n",
    "        return x\n",
    "#| export\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # What do you need?\n",
    "\n",
    "        self.layers = clones(layer, N)\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(layer.size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "\n",
    "        # What's the sequence of operations?\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "\n",
    "        return self.layer_norm(x)\n",
    "\n",
    "        \n",
    "#| export\n",
    "\n",
    "class Embeddings(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # What do you need here?\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab, d_model)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # What's the operation?\n",
    "\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "#| export\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # What layer do you need?\n",
    "\n",
    "        self.linear = torch.nn.Linear(d_model, vocab)\n",
    "\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # What operation?\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return self.logsoftmax(x)\n",
    "\n",
    "\n",
    "\n",
    "#| export\n",
    "\n",
    "class EncoderDecoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Store all 5 components\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.src_embed = src_embed\n",
    "\n",
    "        self.tgt_embed = tgt_embed\n",
    "\n",
    "        self.generator = generator\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "\n",
    "        x = self.src_embed(src)\n",
    "\n",
    "        return self.encoder(x, src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "\n",
    "        # What goes here?\n",
    "\n",
    "        x = self.tgt_embed(tgt)\n",
    "\n",
    "        return self.decoder(x, memory, src_mask, tgt_mask)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "\n",
    "        # What goes here?\n",
    "\n",
    "        memory = self.encode(src, src_mask)\n",
    "\n",
    "        return self.decode(memory, src_mask, tgt, tgt_mask)\n",
    "#| export\n",
    "\n",
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "\n",
    "    # Create components\n",
    "\n",
    "    # Level 1\n",
    "\n",
    "    # Attention and Clones functions already created. As they are not classes, no need to create instances.\n",
    "\n",
    "    # Level 2\n",
    "\n",
    "    multi_head_attentions = [copy.deepcopy(MultiHeadedAttention(h, d_model, dropout)) for _ in range(3)]\n",
    "\n",
    "    feedforward_layers = [copy.deepcopy(PositionwiseFeedForward(d_model, d_ff, dropout)) for _ in range(2)]\n",
    "\n",
    "    positional_encoding = [PositionalEncoding(d_model, dropout) for _ in range(2)]\n",
    "\n",
    "    src_embedding_layer = Embeddings(d_model, src_vocab)\n",
    "\n",
    "    tgt_embeddings_layer = Embeddings(d_model, tgt_vocab)\n",
    "\n",
    "    src_embed = torch.nn.Sequential([src_embedding_layer, positional_encoding[0]])\n",
    "\n",
    "    tgt_embed = torch.nn.Sequential([tgt_embeddings_layer, positional_encoding[1]])\n",
    "\n",
    "    # Level 3\n",
    "\n",
    "    # Sublayer\n",
    "\n",
    "    # Level 4\n",
    "\n",
    "    encoder_layer = EncoderLayer(d_model, multi_head_attentions[0], \n",
    "\n",
    "                                feedforward_layers[0], dropout)\n",
    "\n",
    "    decoder_layer = DecoderLayer(d_model, multi_head_attentions[1], \n",
    "\n",
    "                                multi_head_attentions[2], feedforward_layers[1], \n",
    "\n",
    "                                dropout)\n",
    "\n",
    "    # Level 5 \n",
    "\n",
    "    encoder = Encoder(encoder_layer, N)\n",
    "\n",
    "    decoder = Decoder(decoder_layer, N)\n",
    "\n",
    "    generator = Generator(d_model, vocab)\n",
    "\n",
    "    # Assemble into EncoderDecoder\n",
    "\n",
    "    return EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator) \n",
    "\n",
    "    # Initialize parameters\n",
    "#| export\n",
    "\n",
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "\n",
    "    # Create components\n",
    "\n",
    "    # Level 1\n",
    "\n",
    "    # Attention and Clones functions already created. As they are not classes, no need to create instances.\n",
    "\n",
    "    # Level 2\n",
    "\n",
    "    multi_head_attentions = [copy.deepcopy(MultiHeadedAttention(h, d_model, dropout)) for _ in range(3)]\n",
    "\n",
    "    feedforward_layers = [copy.deepcopy(PositionwiseFeedForward(d_model, d_ff, dropout)) for _ in range(2)]\n",
    "\n",
    "    positional_encoding = [PositionalEncoding(d_model, dropout) for _ in range(2)]\n",
    "\n",
    "    src_embedding_layer = Embeddings(d_model, src_vocab)\n",
    "\n",
    "    tgt_embeddings_layer = Embeddings(d_model, tgt_vocab)\n",
    "\n",
    "    src_embed = torch.nn.Sequential(src_embedding_layer, positional_encoding[0])\n",
    "\n",
    "    tgt_embed = torch.nn.Sequential(tgt_embeddings_layer, positional_encoding[1])\n",
    "\n",
    "    # Level 3\n",
    "\n",
    "    # Sublayer\n",
    "\n",
    "    # Level 4\n",
    "\n",
    "    encoder_layer = EncoderLayer(d_model, multi_head_attentions[0], \n",
    "\n",
    "                                feedforward_layers[0], dropout)\n",
    "\n",
    "    decoder_layer = DecoderLayer(d_model, multi_head_attentions[1], \n",
    "\n",
    "                                multi_head_attentions[2], feedforward_layers[1], \n",
    "\n",
    "                                dropout)\n",
    "\n",
    "    # Level 5 \n",
    "\n",
    "    encoder = Encoder(encoder_layer, N)\n",
    "\n",
    "    decoder = Decoder(decoder_layer, N)\n",
    "\n",
    "    generator = Generator(d_model, tgt_vocab)\n",
    "\n",
    "    # Assemble into EncoderDecoder\n",
    "\n",
    "    encoder_decoder =  EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator) \n",
    "\n",
    "    # Initialize parameters\n",
    "\n",
    "    for p in encoder_decoder.parameters():\n",
    "\n",
    "        if p.dim() > 1:\n",
    "\n",
    "            torch.nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return encoder_decoder\n",
    "#| export\n",
    "\n",
    "def subsequent_mask(size):\n",
    "\n",
    "    # Create a lower triangular matrix\n",
    "\n",
    "    lower_t = torch.ones([size, size]).tril().bool()\n",
    "\n",
    "    lower_t = lower_t.unsqueeze(0)\n",
    "\n",
    "    # Return shape should be (1, size, size)\n",
    "\n",
    "    return lower_t\n",
    "#| export\n",
    "\n",
    "class Batch:\n",
    "\n",
    "    def __init__(self, src, tgt=None, pad=2):\n",
    "\n",
    "        self.src = src\n",
    "\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "\n",
    "        if tgt is not None:\n",
    "\n",
    "                self.tgt = tgt[:, :-1]\n",
    "\n",
    "                self.tgt_y = tgt[:,1:]\n",
    "\n",
    "                self.pad_tgt_mask = (self.tgt!=pad).unsqueeze(-2)\n",
    "\n",
    "                self.subseq_tgt_mask = subsequent_mask(self.tgt.size(1))\n",
    "\n",
    "                self.tgt_mask = self.pad_tgt_mask & self.subseq_tgt_mask\n",
    "\n",
    "                self.ntokens = (self.tgt_y!=pad).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#| export\n",
    "\n",
    "def data_gen(V, batch_size, nbatches):\n",
    "\n",
    "    \"Generate random data for a src-tgt copy task\"\n",
    "\n",
    "    for i in range(nbatches):\n",
    "\n",
    "        # What do you need to create here?\n",
    "\n",
    "        # How do you generate random sequences?\n",
    "\n",
    "        # How do you make src and tgt?\n",
    "\n",
    "        # What do you yield?\n",
    "\n",
    "        random_int = np.random.randint(1, V, size=[batch_size, 10])\n",
    "\n",
    "        random_int[:,0] = 1\n",
    "\n",
    "        random_int = torch.tensor(random_int)\n",
    "\n",
    "        src = random_int\n",
    "\n",
    "        tgt = random_int\n",
    "\n",
    "        yield Batch(src, tgt, 0)\n",
    "\n",
    "\n",
    "\n",
    "#| export\n",
    "\n",
    "class SimpleLossCompute:\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "\n",
    "        # What do you store?\n",
    "\n",
    "        self.generator = generator\n",
    "\n",
    "        self.criterion = criterion\n",
    "\n",
    "        \n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "\n",
    "        # What operations do you do?\n",
    "\n",
    "        pred = self.generator(x)\n",
    "\n",
    "        vocab = pred.size(-1)\n",
    "\n",
    "        pred_flat = pred.reshape(-1, vocab)\n",
    "\n",
    "        y_flat = y.reshape(-1)\n",
    "\n",
    "        loss = self.criterion(pred_flat, y_flat) / norm\n",
    "\n",
    "        return loss.data * norm, loss\n",
    "#| export\n",
    "\n",
    "def run_epoch(data_iter, model, loss_compute, optimizer, scheduler, mode=\"train\"):\n",
    "\n",
    "    total_tokens = 0\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    \n",
    "\n",
    "    for i, batch in enumerate(data_iter):\n",
    "\n",
    "        # What goes here?\n",
    "\n",
    "        pred = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n",
    "\n",
    "        num_loss, tensor_loss = loss_compute(pred, batch.tgt_y, batch.ntokens)\n",
    "\n",
    "        if mode == \"train\":\n",
    "\n",
    "            tensor_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += num_loss \n",
    "\n",
    "        total_tokens += batch.ntokens\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "\n",
    "#| export\n",
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "\n",
    "    ys = torch.LongTensor([[start_symbol]])\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "\n",
    "        tgt_mask = subsequent_mask(ys.size(1))\n",
    "\n",
    "        x = model.decode(memory, src_mask, ys, tgt_mask)\n",
    "\n",
    "        pred = model.generator(x[:, -1])\n",
    "\n",
    "        _, max_indices = torch.max(pred, dim=-1)\n",
    "\n",
    "        max_indices = max_indices.data[0]\n",
    "\n",
    "        ys = torch.cat([ys, torch.ones(1,1).type_as(src.data).fill_(max_indices)], dim=-1)\n",
    "\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython.display import HTML, display\n",
    "import html\n",
    "import math\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be doing an implementation of the seminal paper \"Attention Is All You Need.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "![pasted_image_815012f1-e052-4514-a18c-db31f235d612.png](attachment:815012f1-e052-4514-a18c-db31f235d612)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Details\n",
    "\n",
    "\n",
    "![pasted_image_670cb52c-fedb-4185-bb81-749fd2bffd33.png](attachment:670cb52c-fedb-4185-bb81-749fd2bffd33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the scaled dot-product attention mechanism. Remember the formula: Attention(Q,K,V) = softmax(QK^T / âˆšd_k)V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def attention(Q, K, V, dropout, mask):\n",
    "    \"\"\"Scaled Dot-Product Attention\"\"\"\n",
    "    sqrt_d_k = math.sqrt(K.size(-1))\n",
    "    scores = Q @ K.transpose(-2,-1) / sqrt_d_k\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "    \n",
    "    attention_weights = torch.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "    \n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result shape: torch.Size([2, 1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Test attention\n",
    "Q = torch.randn(2, 1, 4, 8)\n",
    "K = torch.randn(2, 1, 4, 8)\n",
    "V = torch.randn(2, 1, 4, 8)\n",
    "result = attention(Q, K, V, None, None)\n",
    "print(\"Result shape:\", result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadedAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement multi-head attention. Split d_model into h heads, apply attention to each, then concatenate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultiHeadedAttention(torch.nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.attn = None\n",
    "        \n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(d_model, d_model) for x in range(4)])\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        Q = self.linear_layers[0](query)\n",
    "        K = self.linear_layers[1](key)\n",
    "        V = self.linear_layers[2](value)\n",
    "        \n",
    "        Q = Q.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\n",
    "        K = K.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\n",
    "        V = V.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        \n",
    "        x = attention(Q, K, V, self.dropout, mask)\n",
    "        \n",
    "        x = x.transpose(1,2).reshape(batch_size, -1, self.h * self.d_k)\n",
    "        \n",
    "        output = self.linear_layers[3](x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512]) Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test MultiHeadedAttention\n",
    "mha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n",
    "x = torch.randn(2, 10, 512)\n",
    "output = mha(x, x, x)\n",
    "print(\"Input shape:\", x.shape, \"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PositionwiseFeedForward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the position-wise feed-forward network: FFN(x) = max(0, xW1 + b1)W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PositionwiseFeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear_layer = torch.nn.Linear(d_model, d_ff)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.output_layer = torch.nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512]) Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test PositionwiseFeedForward\n",
    "ffn = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n",
    "x = torch.randn(2, 10, 512)\n",
    "output = ffn(x)\n",
    "print(\"Input shape:\", x.shape, \"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since attention has no notion of position, we add positional encodings using sin/cos functions of different frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        even_indices = torch.arange(0, d_model, 2)\n",
    "        \n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(-even_indices * (torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        \n",
    "        pe[:, ::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:,:seq_len,:].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512]) Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test PositionalEncoding\n",
    "pe = PositionalEncoding(d_model=512, dropout=0.1)\n",
    "x = torch.zeros(2, 10, 512)\n",
    "output = pe(x)\n",
    "print(\"Input shape:\", x.shape, \"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each encoder layer has two sub-layers: multi-head self-attention and feed-forward network, each with residual connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sublayer Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SublayerConnection(torch.nn.Module):\n",
    "    \"A residual connection followed by layer norm\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.layer_norm = torch.nn.LayerNorm(size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        y = self.layer_norm(x)\n",
    "        y = sublayer(y)\n",
    "        y = self.dropout(y)\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final clones function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers\"\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final EncoderLayer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = torch.nn.ModuleList([SublayerConnection(size, dropout) for _ in range(2)])\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.feed_forward(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512]) Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test EncoderLayer\n",
    "attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n",
    "ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n",
    "encoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n",
    "x = torch.randn(2, 10, 512)\n",
    "output = encoder_layer(x, None)\n",
    "print(\"Input shape:\", x.shape, \"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack N encoder layers together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(torch.nn.Module):\n",
    "    \"Stack of N encoder layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.encoders = clones(layer, N)\n",
    "        self.layer_norm = torch.nn.LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, mask)\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512]) Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test Encoder\n",
    "encoder = Encoder(encoder_layer, N=6)\n",
    "x = torch.randn(2, 10, 512)\n",
    "output = encoder(x, None)\n",
    "print(\"Input shape:\", x.shape, \"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each decoder layer has three sub-layers: masked self-attention, cross-attention to encoder output, and feed-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayers = clones(SublayerConnection(size, dropout), 3)\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n",
    "        x = self.sublayers[2](x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512]) Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test DecoderLayer\n",
    "attn1 = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n",
    "attn2 = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n",
    "ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n",
    "decoder_layer = DecoderLayer(size=512, self_attn=attn1, src_attn=attn2, feed_forward=ff, dropout=0.1)\n",
    "x = torch.randn(2, 10, 512)\n",
    "memory = torch.randn(2, 10, 512)\n",
    "output = decoder_layer(x, memory, None, None)\n",
    "print(\"Input shape:\", x.shape, \"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack N decoder layers together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.layer_norm = torch.nn.LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512]) Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test Decoder\n",
    "decoder = Decoder(decoder_layer, N=6)\n",
    "x = torch.randn(2, 10, 512)\n",
    "memory = torch.randn(2, 10, 512)\n",
    "output = decoder(x, memory, None, None)\n",
    "print(\"Input shape:\", x.shape, \"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert token IDs to dense vectors, scaled by sqrt(d_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Embeddings(torch.nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = torch.nn.Embedding(vocab, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10]) Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test Embeddings\n",
    "emb = Embeddings(d_model=512, vocab=1000)\n",
    "x = torch.randint(0, 1000, (2, 10))\n",
    "output = emb(x)\n",
    "print(\"Input shape:\", x.shape, \"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final linear layer + log softmax to convert decoder output to token probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(d_model, vocab)\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.logsoftmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512]) Output shape: torch.Size([2, 10, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Test Generator\n",
    "gen = Generator(d_model=512, vocab=1000)\n",
    "x = torch.randn(2, 10, 512)\n",
    "output = gen(x)\n",
    "print(\"Input shape:\", x.shape, \"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Encoder Decoder Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EncoderDecoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        x = self.src_embed(src)\n",
    "        return self.encoder(x, src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        x = self.tgt_embed(tgt)\n",
    "        return self.decoder(x, memory, src_mask, tgt_mask)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        memory = self.encode(src, src_mask)\n",
    "        return self.decode(memory, src_mask, tgt, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to construct the full transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Construct transformer model from hyperparameters\"\n",
    "    multi_head_attentions = [copy.deepcopy(MultiHeadedAttention(h, d_model, dropout)) for _ in range(3)]\n",
    "    feedforward_layers = [copy.deepcopy(PositionwiseFeedForward(d_model, d_ff, dropout)) for _ in range(2)]\n",
    "    positional_encoding = [PositionalEncoding(d_model, dropout) for _ in range(2)]\n",
    "    \n",
    "    src_embedding_layer = Embeddings(d_model, src_vocab)\n",
    "    tgt_embeddings_layer = Embeddings(d_model, tgt_vocab)\n",
    "    src_embed = torch.nn.Sequential(src_embedding_layer, positional_encoding[0])\n",
    "    tgt_embed = torch.nn.Sequential(tgt_embeddings_layer, positional_encoding[1])\n",
    "    \n",
    "    encoder_layer = EncoderLayer(d_model, multi_head_attentions[0], feedforward_layers[0], dropout)\n",
    "    decoder_layer = DecoderLayer(d_model, multi_head_attentions[1], multi_head_attentions[2], feedforward_layers[1], dropout)\n",
    "    \n",
    "    encoder = Encoder(encoder_layer, N)\n",
    "    decoder = Decoder(decoder_layer, N)\n",
    "    generator = Generator(d_model, tgt_vocab)\n",
    "    \n",
    "    encoder_decoder = EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)\n",
    "    \n",
    "    for p in encoder_decoder.parameters():\n",
    "        if p.dim() > 1:\n",
    "            torch.nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return encoder_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Code Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training utilities for the copy task (provided complete)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final subsequent mask function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def subsequent_mask(size):\n",
    "    \"Create mask to prevent attention to future positions\"\n",
    "    lower_t = torch.ones([size, size]).tril().bool()\n",
    "    lower_t = lower_t.unsqueeze(0)\n",
    "    return lower_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Batch Class for copy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Batch:\n",
    "    def __init__(self, src, tgt=None, pad=2):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]\n",
    "            self.tgt_y = tgt[:,1:]\n",
    "            self.pad_tgt_mask = (self.tgt!=pad).unsqueeze(-2)\n",
    "            self.subseq_tgt_mask = subsequent_mask(self.tgt.size(1))\n",
    "            self.tgt_mask = self.pad_tgt_mask & self.subseq_tgt_mask\n",
    "            self.ntokens = (self.tgt_y!=pad).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Data Gen Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def data_gen(V, batch_size, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task\"\n",
    "    for i in range(nbatches):\n",
    "        random_int = np.random.randint(1, V, size=[batch_size, 10])\n",
    "        random_int[:,0] = 1\n",
    "        random_int = torch.tensor(random_int)\n",
    "        src = random_int\n",
    "        tgt = random_int\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Simple Loss Compute Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SimpleLossCompute:\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        pred = self.generator(x)\n",
    "        vocab = pred.size(-1)\n",
    "        pred_flat = pred.reshape(-1, vocab)\n",
    "        y_flat = y.reshape(-1)\n",
    "        loss = self.criterion(pred_flat, y_flat) / norm\n",
    "        return loss.data * norm, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Run Epoch Function for Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_epoch(data_iter, model, loss_compute, optimizer, scheduler, mode=\"train\"):\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_iter):\n",
    "        pred = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n",
    "        num_loss, tensor_loss = loss_compute(pred, batch.tgt_y, batch.ntokens)\n",
    "        if mode == \"train\":\n",
    "            tensor_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        total_loss += num_loss \n",
    "        total_tokens += batch.ntokens\n",
    "    \n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.LongTensor([[start_symbol]])\n",
    "    for _ in range(max_len - 1):\n",
    "        tgt_mask = subsequent_mask(ys.size(1))\n",
    "        x = model.decode(memory, src_mask, ys, tgt_mask)\n",
    "        pred = model.generator(x[:, -1])\n",
    "        _, max_indices = torch.max(pred, dim=-1)\n",
    "        max_indices = max_indices.data[0]\n",
    "        ys = torch.cat([ys, torch.ones(1,1).type_as(src.data).fill_(max_indices)], dim=-1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 0 Loss: 0.0109\n",
      "Epoch 1 Loss: 0.0094\n",
      "Epoch 2 Loss: 0.0085\n",
      "Epoch 3 Loss: 0.0079\n",
      "Epoch 4 Loss: 0.0073\n",
      "Epoch 5 Loss: 0.0070\n",
      "Epoch 6 Loss: 0.0066\n",
      "Epoch 7 Loss: 0.0060\n",
      "Epoch 8 Loss: 0.0054\n",
      "Epoch 9 Loss: 0.0050\n",
      "Epoch 10 Loss: 0.0044\n",
      "Epoch 11 Loss: 0.0038\n",
      "Epoch 12 Loss: 0.0033\n",
      "Epoch 13 Loss: 0.0028\n",
      "Epoch 14 Loss: 0.0024\n",
      "Epoch 15 Loss: 0.0022\n",
      "Epoch 16 Loss: 0.0017\n",
      "Epoch 17 Loss: 0.0015\n",
      "Epoch 18 Loss: 0.0013\n",
      "Epoch 19 Loss: 0.0012\n",
      "Epoch 20 Loss: 0.0010\n",
      "Epoch 21 Loss: 0.0008\n",
      "Epoch 22 Loss: 0.0008\n",
      "Epoch 23 Loss: 0.0007\n",
      "Epoch 24 Loss: 0.0007\n",
      "Epoch 25 Loss: 0.0006\n",
      "Epoch 26 Loss: 0.0007\n",
      "Epoch 27 Loss: 0.0005\n",
      "Epoch 28 Loss: 0.0004\n",
      "Epoch 29 Loss: 0.0005\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Create small model for testing\n",
    "V = 11\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = make_model(V, V, N=2, d_model=64, d_ff=128, h=4, dropout=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "def rate(step, model_size=64, factor=1.0, warmup=400):\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda step: rate(step))\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    loss_compute = SimpleLossCompute(model.generator, criterion)\n",
    "    loss = run_epoch(data_gen(V, batch_size=30, nbatches=20), model, loss_compute, optimizer, scheduler, mode=\"train\")\n",
    "    print(f\"Epoch {epoch} Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n",
      "Generated: tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n",
      "\n",
      "Success! True\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "src_mask = torch.ones(1, 1, 10)\n",
    "\n",
    "print(\"Source:\", src)\n",
    "result = greedy_decode(model, src, src_mask, max_len=10, start_symbol=1)\n",
    "print(\"Generated:\", result)\n",
    "print(\"\\nSuccess!\", torch.equal(src, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've implemented a Transformer from scratch and trained it successfully!\n",
    "\n",
    "### Resources:\n",
    "- [Original Paper](https://arxiv.org/pdf/1706.03762)\n",
    "- [Harvard NLP Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annotated-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
