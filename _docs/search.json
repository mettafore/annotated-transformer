[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "annotated-transformer",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "annotated-transformer"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "annotated-transformer",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall annotated_transformer in Development mode\n# make sure annotated_transformer package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to annotated_transformer\n$ nbdev_prepare",
    "crumbs": [
      "annotated-transformer"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "annotated-transformer",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/mettafore/annotated-transformer.git\nor from conda\n$ conda install -c mettafore annotated_transformer\nor from pypi\n$ pip install annotated_transformer\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repositoryâ€™s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "annotated-transformer"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "annotated-transformer",
    "section": "How to use",
    "text": "How to use\nFill me in please! Donâ€™t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "annotated-transformer"
    ]
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html",
    "href": "attention_is_all_you_need_dup1.html",
    "title": "Simple 2x2 example",
    "section": "",
    "text": "https://arxiv.org/pdf/1706.03762\nI will be doing an implementation of the seminal paper â€œAttention Is All You Need.â€"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#imports",
    "href": "attention_is_all_you_need_dup1.html#imports",
    "title": "Simple 2x2 example",
    "section": "",
    "text": "https://arxiv.org/pdf/1706.03762\nI will be doing an implementation of the seminal paper â€œAttention Is All You Need.â€"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#transformer-architecture",
    "href": "attention_is_all_you_need_dup1.html#transformer-architecture",
    "title": "Simple 2x2 example",
    "section": "Transformer Architecture",
    "text": "Transformer Architecture\n\n\n\npasted_image_815012f1-e052-4514-a18c-db31f235d612.png\n\n\n\nArchitecture Details\n\n\n\npasted_image_670cb52c-fedb-4185-bb81-749fd2bffd33.png"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#attention-1",
    "href": "attention_is_all_you_need_dup1.html#attention-1",
    "title": "Simple 2x2 example",
    "section": "Attention",
    "text": "Attention\nI have given you enough context to implement a transformer from scratch. I want to learn how to implement transformers without cheating. What should I start from first? I also included the code from Annotated Transformer by Harvard NLP. I want to follow this as the benchmark tutorial, but I am not going to see any of the code. I want to do it from scratch.\n\nğŸ¤–ReplyğŸ¤–\nThatâ€™s an excellent learning goal! Implementing a transformer from scratch is one of the best ways to deeply understand how they work. Let me help you get started with a structured approach.\nFirst, let me understand your background better:\n\nHow comfortable are you with PyTorch (or your preferred deep learning framework)?\nDo you understand the core concepts from the â€œAttention is All You Needâ€ paper - like self-attention, multi-head attention, and positional encoding?\nWould you prefer to build this incrementally (starting with the smallest components) or tackle one complete module at a time?\n\nFor a from-scratch implementation, Iâ€™d suggest starting with the attention mechanism itself, since itâ€™s the heart of the transformer. Specifically, you could begin by implementing:\nScaled Dot-Product Attention - the most fundamental building block.\nThis involves: - Taking query (Q), key (K), and value (V) matrices as input - Computing attention scores - Applying the scaling factor - Using softmax to get attention weights - Returning the weighted sum of values\nWould you like me to explain what this function should do conceptually (inputs, outputs, and the mathematical operations), so you can try implementing it yourself? Or would you prefer to start with an even simpler piece, like understanding the data structures youâ€™ll need?\nQ * K^T / sqrt(D_K)* V\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Youâ€™ve got the formula down. Thatâ€™s the core of scaled dot-product attention.\nLet me make sure you understand each part:\n\nQ Â· K^T: This gives you attention scores (how much each query should â€œattend toâ€ each key)\n/ sqrt(d_k): The scaling factor prevents the dot products from getting too large\nThen softmax (you didnâ€™t write it, but it goes here!) converts scores to probabilities\nÂ· V: Finally, use those probabilities as weights for the values\n\nA couple of questions to check your understanding:\n\nWhat are the dimensions/shapes of Q, K, and V?\nWhere does the softmax operation fit in that formula you wrote?\n\nOnce youâ€™re clear on these, you can start coding the attention function. Want to clarify these points first, or are you ready to start implementing?\nIt makes sure that the outputs add up to one.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly! Softmax normalizes the scores so they sum to 1, turning them into a proper probability distribution over the values.\nSo the complete formula is:\nAttention(Q, K, V) = softmax(Q Â· K^T / sqrt(d_k)) Â· V\nNow, before you start coding, letâ€™s think about one more important detail: masking.\nIn the decoder, you need to prevent positions from â€œlooking aheadâ€ at future tokens. This is done by setting certain attention scores to negative infinity before the softmax.\nDo you understand why masking is needed and where it fits into the formula? Or should I clarify that first?\nOnce youâ€™re comfortable with the concept, you can start implementing the attention() function!\nWell, it makes sure that the outputs add up to one.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly! The softmax converts the raw similarity scores into a probability distribution - all values between 0 and 1 that sum to 1.\nThis is important because it tells us â€œhow much attentionâ€ to pay to each value. If one key is very similar to the query, it gets a high weight (close to 1), and we focus mostly on its corresponding value.\nNow youâ€™re ready to implement it! Try coding the attention() function. It should: - Take query, key, value as inputs (and optionally mask and dropout) - Return the attention output (and optionally the attention weights for visualization)\nDonâ€™t worry about getting it perfect - just give it a try and let me know what you come up with or if you get stuck on any part!\ndef attention(Q, K, V, dropout, mask): Q@K.T Am I doing the correct thing? I am not sure what I am doing.\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re on the right track! Youâ€™ve started with the Q @ K.T part, which is correct.\nLet me help clarify what you need to do step by step:\n\nQ @ K.T âœ“ (you have this!)\nDivide by sqrt(d_k) - you need to scale the scores. What is d_k? (Hint: itâ€™s related to the dimension of your key vectors)\nApply mask (if provided) - set certain positions to a very negative number\nApply softmax - convert scores to probabilities\nMultiply by V - get the weighted sum\nApply dropout (if provided) - for regularization\n\nTry adding step 2 next - how would you get d_k from your inputs, and how would you scale the scores?\nis it K.shape(0)? also give me code written till now\n\n\nğŸ¤–ReplyğŸ¤–\nNot quite! d_k is the dimension of each key vector, which is the last dimension of K.\nSo it would be: K.shape[-1] (or K.size(-1) in PyTorch)\nFor the code so far, I want to make sure youâ€™re building this yourself to really learn it. Hereâ€™s what you have:\ndef attention(Q, K, V, dropout, mask):\n    Q @ K.T\nNow try adding: 1. Store that result in a variable (maybe call it scores) 2. Get d_k from the key dimension 3. Divide scores by sqrt(d_k) (youâ€™ll need to import math for math.sqrt)\nGive it a try and show me what you get!\n\ndef attention(Q, K, V, dropout, mask):\n    sqrt_d_k = math.sqrt(K.size(-1))\n    attention = torch.softmax(Q @ K.T / sqrt_d_k)*V\n    return attention\n\nHow am I doing now?\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re doing great! Youâ€™ve got most of the core pieces in place. A few things to adjust:\n\nTranspose operation: In PyTorch, use .transpose(-2, -1) instead of .T to handle batched matrices correctly\nSoftmax dimension: You need to specify which dimension to apply softmax over - it should be the last dimension: torch.softmax(..., dim=-1)\nMissing pieces: You havenâ€™t used mask or dropout yet. The mask needs to be applied before the softmax (by setting masked positions to a very negative number like -1e9)\nParentheses: The multiplication by V should happen after the softmax, so make sure your parentheses are right\n\nTry fixing the transpose and softmax dimension first. Does that make sense?\nWhat is transpose - 2 - 1? I donâ€™t understand what heâ€™s saying. What do you mean applying softmax to the last dimension? I donâ€™t understand. I think Iâ€™m weak at all this dimension kind of stuff. Please help me.\n\n\nğŸ¤–ReplyğŸ¤–\nNo worries! Letâ€™s break this down - dimensions can be tricky at first.\nUnderstanding dimensions/axes:\nWhen you have a tensor (multi-dimensional array), each â€œdirectionâ€ is called a dimension or axis. For example: - A vector has 1 dimension: [1, 2, 3] has shape (3,) - A matrix has 2 dimensions: rows and columns, shape (rows, cols) - A batch of matrices has 3 dimensions: (batch_size, rows, cols)\nWhat is .transpose(-2, -1)?\nThis swaps the last two dimensions. The -1 means â€œlast dimensionâ€ and -2 means â€œsecond-to-lastâ€.\nFor a matrix with shape (rows, cols), transposing swaps them to (cols, rows).\nFor example, if K has shape (batch, seq_len, d_k), then K.transpose(-2, -1) gives you (batch, d_k, seq_len).\nWhat does dim=-1 in softmax mean?\nIt tells PyTorch which dimension to apply softmax across. dim=-1 means â€œapply softmax across the last dimensionâ€ - so each row will sum to 1.\nDoes this help clarify things? Want to try updating your code with these changes, or do you need more explanation on any part?\nOkay, Iâ€™ll update the code. But after I update the code and the code is proper, letâ€™s pass to an example through it so that I understand better. Give me the code so far.\n\n\nğŸ¤–ReplyğŸ¤–\nI want you to try updating it yourself first - thatâ€™s how youâ€™ll learn best!\nTake your current code:\ndef attention(Q, K, V, dropout, mask):\n    sqrt_d_k = math.sqrt(K.size(-1))\n    attention = torch.softmax(Q @ K.T / sqrt_d_k)*V\n    return attention\nAnd make these changes: 1. Change K.T to K.transpose(-2, -1) 2. Add dim=-1 to the softmax\nGive it a try! Once youâ€™ve updated it, weâ€™ll absolutely run an example through it to see how it works.\n\ndef attention(Q, K, V, dropout, mask):\n    sqrt_d_k = math.sqrt(K.size(-1))\n    dropout = torch.nn.Dropout(p=dropout)\n    attention = torch.softmax(Q @ K.transpose(-2,-1) / sqrt_d_k, dim=-1)@V\n    return dropout(attention)\n\nOkay, am I done?\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nAlmost there! Two things to fix:\n\nMask is not used: You need to apply the mask to the scores before the softmax. The typical approach is:\nscores = scores.masked_fill(mask == 0, -1e9)\nThis sets masked positions to a very negative number, so after softmax they become ~0.\nDropout placement: Dropout should be applied to the attention weights (after softmax, before multiplying by V), not to the final output.\n\nCan you restructure your code to: - Compute scores first (Q @ K.transposeâ€¦) - Apply mask to scores (if mask is provided) - Apply softmax to get attention weights - Apply dropout to attention weights (if dropout is provided) - Multiply by V - Return the result\nTry breaking it into separate lines - itâ€™ll be clearer and easier to debug!\nI donâ€™t understand why we need masks for laser clean.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me explain why masking is needed.\nWhy do we need masks?\nThere are two main reasons:\n\nPadding mask: When you batch sentences together, theyâ€™re often different lengths. You pad shorter sentences with special tokens (like &lt;pad&gt;). You donâ€™t want the model to pay attention to these padding tokens because theyâ€™re meaningless. So you mask them out.\nLook-ahead mask (for decoder): In the decoder during training, when predicting word 3, the model should only see words 1 and 2, not words 4, 5, 6 (the future). This prevents â€œcheatingâ€ - the model canâ€™t look at future words itâ€™s supposed to predict. This is called a â€œcausal maskâ€ or â€œsubsequent maskâ€.\n\nWhat does masking do?\nBy setting masked positions to -1e9 (very negative) before softmax, those positions become ~0 after softmax. This means â€œdonâ€™t pay attention to these positions at all.â€\nDoes this make sense? Do you want to see a simple example of how masking works with numbers?\nYes, give me a simple example. I donâ€™t understand what this command is doing. And where exactly is the mask applied?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Let me show you a simple numerical example.\nSimple masking example:\nLetâ€™s say you have attention scores (before softmax):\nscores = [2.0, 3.0, 1.0, 4.0]\nAnd a mask that says â€œignore positions 2 and 3â€:\nmask = [1, 1, 0, 0]  # 1 = keep, 0 = ignore\nStep 1: Apply mask (this is where scores.masked_fill(mask == 0, -1e9) happens)\nscores = [2.0, 3.0, -1e9, -1e9]\nStep 2: Apply softmax\nAfter softmax: [0.27, 0.73, ~0.0, ~0.0]\nSee? The masked positions become essentially 0 after softmax!\nWhere is the mask applied?\nIn your code, it should go right after you compute the scores and BEFORE softmax:\nscores = Q @ K.transpose(-2, -1) / sqrt_d_k\nif mask is not None:\n    scores = scores.masked_fill(mask == 0, -1e9)\nattention_weights = torch.softmax(scores, dim=-1)\nDoes this clarify it?\n\n\nFinal Attention Function\n\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n\n\n\nattention\n\n attention (Q, K, V, dropout, mask)\n\nCool, is this correct?\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Your attention function looks correct now! ğŸ‰\nYouâ€™ve got all the key pieces: - âœ“ Compute scores with proper transpose - âœ“ Scale by sqrt(d_k) - âœ“ Apply mask before softmax - âœ“ Apply softmax on the right dimension - âœ“ Apply dropout to attention weights - âœ“ Multiply by V\nOne small improvement: itâ€™s common to also return the attention_weights for visualization purposes (optional, but helpful for debugging). But for now, this works!\nWant to test it with a simple example to see it in action? Or are you ready to move on to the next component (Multi-Head Attention)?\nnot that simple. too many 1 and 0 lol\n\n\nğŸ¤–ReplyğŸ¤–\nHaha, fair enough! Let me make it even simpler with different numbers:\n# Super simple example - just 1 query and 2 keys/values\nQ = torch.tensor([[2.0, 2.0]])  # 1 query, d_k=2\n\nK = torch.tensor([[1.0, 1.0],   # key 1\n                  [1.0, 0.0]])  # key 2\n\nV = torch.tensor([[10.0],       # value 1\n                  [20.0]])      # value 2\n\nresult = attention(Q, K, V, dropout=0.0, mask=None)\nprint(result)\nNow try by hand: 1. Q @ K.T = ? (just dot products: [2,2]Â·[1,1] and [2,2]Â·[1,0]) 2. Divide by sqrt(2) â‰ˆ 1.41 3. Apply softmax (which one is bigger?) 4. Weighted sum of values [10] and [20]\nGive it a shot!\nQ ( 1x2) K_T (2x2) Q@K_t = [4 2] div by sqrt2 = [2sqr(2) sqrt(2)] softmax approx [1 0] result = [10 0[\n\n\nğŸ¤–ReplyğŸ¤–\nGreat work! Youâ€™re very close! Let me check your steps:\n\nQ @ K.T: âœ“ Correct! [4, 2]\nDivide by sqrt(2): âœ“ Correct! [2âˆš2, âˆš2] which is approximately [2.83, 1.41]\nSoftmax: Almost! It wonâ€™t be exactly [1, 0]. Since 2.83 is bigger than 1.41, the first position will have higher weight, but not exactly 1. Itâ€™ll be something like [0.8, 0.2] (the exact values depend on the exponentials)\nResult: Because of softmax, itâ€™s a weighted average: 0.8 * 10 + 0.2 * 20 = 8 + 4 = 12\n\nSo the result should be around [[12]] (a weighted average closer to 10 than 20).\nWant to run the code now to see the actual numbers?\nya gimme code\n\n\nğŸ¤–ReplyğŸ¤–\n# Super simple example - just 1 query and 2 keys/values\nQ = torch.tensor([[2.0, 2.0]])  # 1 query, d_k=2\n\nK = torch.tensor([[1.0, 1.0],   # key 1\n                  [1.0, 0.0]])  # key 2\n\nV = torch.tensor([[10.0],       # value 1\n                  [20.0]])      # value 2\n\nresult = attention(Q, K, V, dropout=0.0, mask=None)\nprint(\"Result:\", result)\nprint(\"Shape:\", result.shape)\nRun this and see what you get! Does it match your hand calculation?\n\n# Super simple example - just 1 query and 2 keys/values\nQ = torch.tensor([[2.0, 2.0]])  # 1 query, d_k=2\n\nK = torch.tensor([[1.0, 1.0],   # key 1\n                  [1.0, 0.0]])  # key 2\n\nV = torch.tensor([[10.0],       # value 1\n                  [20.0]])      # value 2\n\nresult = attention(Q, K, V, dropout=0.0, mask=None)\nprint(\"Result:\", result)\nprint(\"Shape:\", result.shape)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[1], line 10\n      4 K = torch.tensor([[1.0, 1.0],   # key 1\n      5                   [1.0, 0.0]])  # key 2\n      7 V = torch.tensor([[10.0],       # value 1\n      8                   [20.0]])      # value 2\n---&gt; 10 result = attention(Q, K, V, dropout=0.0, mask=None)\n     11 print(\"Result:\", result)\n     12 print(\"Shape:\", result.shape)\n\nCell In[1], line 7, in attention(Q, K, V, dropout, mask)\n      5     scores = scores.masked_fill(mask==0, -1e9)\n      6 attention_weights = torch.softmax(scores, dim=-1)\n----&gt; 7 attention_weights = dropout(attention_weights)\n      8 output = attention_weights @ V\n      9 return output\n\nTypeError: 'float' object is not callable\n\n\n\nConvert this into a unit as. Assume Iâ€™m writing an NB Dev Notebook.\n\n\nğŸ¤–ReplyğŸ¤–\nIâ€™ll help you convert your attention function into a unit test for an nbdev notebook. Hereâ€™s how you can structure it:\n#| export\ndef attention(Q, K, V, dropout=None, mask=None):\n    \"Compute Scaled Dot-Product Attention\"\n    sqrt_d_k = math.sqrt(K.size(-1))\n    scores = Q @ K.transpose(-2,-1) / sqrt_d_k\n    if mask is not None:\n        scores = scores.masked_fill(mask==0, -1e9)\n    attention_weights = torch.softmax(scores, dim=-1)\n    if dropout is not None:\n        attention_weights = dropout(attention_weights)\n    output = attention_weights @ V\n    return output\n#| test\ndef test_attention_basic():\n    \"Test attention with simple hand-calculable example\"\n    Q = torch.tensor([[2.0, 2.0]])  # 1 query, d_k=2\n    K = torch.tensor([[1.0, 1.0],   # key 1\n                      [1.0, 0.0]])  # key 2\n    V = torch.tensor([[10.0],       # value 1\n                      [20.0]])      # value 2\n    \n    result = attention(Q, K, V, dropout=None, mask=None)\n    \n    # Result should be weighted average closer to 10 than 20\n    assert result.shape == torch.Size([1, 1])\n    assert 11.0 &lt; result.item() &lt; 13.0  # approximately 11.96\nA couple of notes: 1. I made dropout and mask optional parameters with default None 2. The #| export directive marks the function for export to your module 3. The #| test directive marks the test function\nWould you like me to add more test cases (e.g., testing with masks, or with batched inputs)?\n\n\nUnit Test\n\ndef test_attention_basic():\n    \"Test attention with simple hand-calculable example\"\n    Q = torch.tensor([[2.0, 2.0]])  # 1 query, d_k=2\n    K = torch.tensor([[1.0, 1.0],   # key 1\n                      [1.0, 0.0]])  # key 2\n    V = torch.tensor([[10.0],       # value 1\n                      [20.0]])      # value 2\n    \n    result = attention(Q, K, V, dropout=None, mask=None)\n    \n    # Result should be weighted average closer to 10 than 20\n    assert result.shape == torch.Size([1, 1])\n    assert 11.0 &lt; result.item() &lt; 13.0  # approximately 11.96\n\nnoice But are the dimensions of q, k, v you have given realistic? What are the real ones?\n\nğŸ¤–ReplyğŸ¤–\nGreat question! The dimensions I gave you were simplified for hand calculation. Let me show you realistic dimensions:\nIn a real transformer:\n\nBatch size (B): typically 8, 16, 32, etc.\nSequence length (S): how many tokens (words/subwords) - could be 10, 50, 512, etc.\nd_model: the model dimension - typically 512 (base model) or 1024 (large model)\nd_k (key/query dimension): typically d_model / num_heads. For 8 heads and d_model=512, d_k = 64\n\nRealistic shapes:\nQ: (batch_size, seq_len, d_k)     e.g., (32, 50, 64)\nK: (batch_size, seq_len, d_k)     e.g., (32, 50, 64)\nV: (batch_size, seq_len, d_v)     e.g., (32, 50, 64)\nNote: usually d_k = d_v = 64 when using 8 attention heads.\nWant to test your attention function with more realistic dimensions to make sure it handles batches correctly?\nya\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Try this:\n# Realistic dimensions\nbatch_size = 2\nseq_len = 4\nd_k = 8\n\nQ = torch.randn(batch_size, seq_len, d_k)\nK = torch.randn(batch_size, seq_len, d_k)\nV = torch.randn(batch_size, seq_len, d_k)\n\nresult = attention(Q, K, V, dropout=0.0, mask=None)\nprint(\"Result shape:\", result.shape)\nprint(\"Expected shape:\", (batch_size, seq_len, d_k))\nRun this and check if the output shape matches what you expect. Does it work correctly?\n\n# Realistic dimensions\nbatch_size = 2\nseq_len = 4\nd_k = 8\n\nQ = torch.randn(batch_size, seq_len, d_k)\nK = torch.randn(batch_size, seq_len, d_k)\nV = torch.randn(batch_size, seq_len, d_k)\n\nresult = attention(Q, K, V, dropout=0.0, mask=None)\nprint(\"Result shape:\", result.shape)\nprint(\"Expected shape:\", (batch_size, seq_len, d_k))\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 K = torch.randn(batch_size, seq_len, d_k)\n      8 V = torch.randn(batch_size, seq_len, d_k)\n---&gt; 10 result = attention(Q, K, V, dropout=0.0, mask=None)\n     11 print(\"Result shape:\", result.shape)\n     12 print(\"Expected shape:\", (batch_size, seq_len, d_k))\n\nCell In[1], line 7, in attention(Q, K, V, dropout, mask)\n      5     scores = scores.masked_fill(mask==0, -1e9)\n      6 attention_weights = torch.softmax(scores, dim=-1)\n----&gt; 7 attention_weights = dropout(attention_weights)\n      8 output = attention_weights @ V\n      9 return output\n\nTypeError: 'float' object is not callable\n\n\n\nWhat is the philosophy behind attention? What is Q, what is K, what is V? Give a proper analogy.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me give you an intuitive analogy.\nThe Library Analogy:\nImagine youâ€™re in a library looking for information about â€œtransformers in NLPâ€:\n\nQuery (Q): Your question/search term - â€œWhat are transformers?â€\nKeys (K): The titles/keywords of all books on the shelves\nValues (V): The actual content of those books\n\nHow attention works:\n\nYou compare your query against all the book titles (Q Â· K^T) - which books are most relevant?\nYou get similarity scores - â€œDeep Learningâ€ book scores high, â€œCooking 101â€ scores low\nSoftmax turns these into weights - how much to focus on each book\nYou read a weighted combination of the books (weights Â· V) - paying more attention to relevant ones\n\nIn transformers:\n\nEach word creates its own Query: â€œWhat information do I need?â€\nEach word provides a Key: â€œI have this type of informationâ€\nEach word provides a Value: â€œHereâ€™s my actual informationâ€\n\nThe word â€œbankâ€ (query) might attend strongly to â€œriverâ€ (high attention weight) vs â€œmoneyâ€ depending on context!\nDoes this help clarify the philosophy?\nOkay, so I get that, but how does query, and the key, and the value relate to attention overall? Isnâ€™t that just multiplying inputs with WQ and WK? How is that converting to query and key?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent question! Youâ€™re getting to the heart of how attention actually works.\nYouâ€™re absolutely right! Q, K, and V are created by multiplying the input with learned weight matrices:\nQ = Input @ W_Q\nK = Input @ W_K  \nV = Input @ W_V\nWhy do we do this?\nThe input embeddings are just generic word representations. By multiplying with W_Q, W_K, W_V, weâ€™re learning to transform the input into three different â€œviewsâ€:\n\nW_Q learns: â€œHow should I ask questions?â€ (what to look for)\nW_K learns: â€œHow should I advertise what I have?â€ (what I can provide)\nW_V learns: â€œWhat information should I actually pass along?â€ (the content to share)\n\nExample: The word â€œbankâ€ has one embedding. But: - As a Query (W_Q): it might learn to ask â€œwhatâ€™s nearby?â€ - As a Key (W_K): it might learn to signal â€œIâ€™m a nounâ€ - As a Value (W_V): it might learn to pass along semantic meaning\nThe magic is that W_Q, W_K, W_V are learned during training to make attention useful!\nDoes this connection make sense?\nExcellent. Letâ€™s move on.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Youâ€™ve successfully implemented the core attention mechanism.\nNow the next step is Multi-Head Attention. This is where we run multiple attention operations in parallel (multiple â€œheadsâ€), each learning different patterns.\nThe key idea: instead of one attention, we have h heads (typically 8), each with its own W_Q, W_K, W_V matrices. Then we concatenate all the outputs and project them back.\nBefore we start coding, do you understand conceptually: 1. Why weâ€™d want multiple attention heads instead of just one? 2. How the dimensions change when we split into multiple heads?\nOr should I explain the intuition first?\nWe need multiple attention heads because we want to see: - One word can have a correlation with other words - Another attention head can give another perspective\nHow the dimension changes when we split into multiple heads? Maybe the input dimensions is divided by the number of tangent heads. Thatâ€™s how it splits.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right on both points! ğŸ‰\nWhy multiple heads: Each head can learn different relationships - one might focus on syntactic patterns (subject-verb), another on semantic similarity, another on positional relationships, etc.\nDimension splitting: Yes! If d_model = 512 and we have 8 heads, each head works with d_k = 512/8 = 64 dimensions. This keeps the total computation roughly the same as single-head attention.\nThe process: 1. Start with input of shape (batch, seq_len, d_model=512) 2. Project to Q, K, V using W_Q, W_K, W_V 3. Reshape/split into h heads: (batch, seq_len, h=8, d_k=64) 4. Run attention on each head in parallel 5. Concatenate heads back together: (batch, seq_len, d_model=512) 6. Final linear projection with W_O\nReady to start implementing? What should we build first - the reshaping logic or the overall MultiHeadAttention class structure?\nI donâ€™t know whatâ€™s easier to start with.\n\n\nğŸ¤–ReplyğŸ¤–\nLetâ€™s start with the overall class structure first - itâ€™s easier to see the big picture, then fill in the details.\nA MultiHeadedAttention class needs: - __init__: store hyperparameters (h, d_model) and create the weight matrices - forward: take input, split into heads, run attention, concatenate, project\nTry writing just the skeleton:\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        # What do we need to store here?\n        # What layers/parameters do we need?\n        \n    def forward(self, query, key, value, mask=None):\n        # What steps go here?\nThink about: what parameters does this class need to store? (Hint: you need 4 linear layers - why 4?)\nGive it a try!\ni havent a clu where to start\n\n\nğŸ¤–ReplyğŸ¤–\nNo worries! Let me break it down into smaller pieces.\nWhat does MultiHeadedAttention need to do?\nRemember: we need to project Q, K, V, then split into heads, run attention, concatenate, and project again.\nStart with the __init__ method. You need:\n\nStore h (number of heads) and d_k (dimension per head)\nCreate 4 linear layers:\n\nOne for projecting to Q\nOne for projecting to K\n\nOne for projecting to V\nOne for the final output projection (after concatenating heads)\n\n\nTry writing just the __init__ method:\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        self.h = h\n        self.d_k = d_model // h  # dimension per head\n        \n        # Create 4 linear layers here\n        # Each should be nn.Linear(d_model, d_model)\nCan you add the 4 linear layers? Maybe store them in a list or as separate attributes?\nSo, bro, instead of doing all this guesswork, should I read the paper first and then come back to this notebook?\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nThatâ€™s a totally fair question! Thereâ€™s no single right answer - it depends on how you learn best.\nOption A: Read the paper first - Pros: Youâ€™ll understand the â€œwhyâ€ behind each component, the math notation, and the full architecture before coding - Cons: The paper can be dense; some parts might not click until you actually implement them\nOption B: Keep coding, refer to paper as needed - Pros: Hands-on learning, immediate feedback, you discover the â€œwhyâ€ as you hit roadblocks - Cons: Can feel like guesswork (which is what youâ€™re feeling now!)\nOption C: Hybrid approach (my suggestion) - Read just the Section 3.2 (Attention) of the paper - itâ€™s only ~1.5 pages - It explains Multi-Head Attention with diagrams and equations - Then come back and implement with that understanding\nThe paper is here: https://arxiv.org/pdf/1706.03762 - Section 3.2 starts around page 3.\nWhat sounds best to you? Would you like to take a break and read that section, or would you prefer I explain the Multi-Head Attention concept more thoroughly here first?\nill read the architecture part first i guess. all the misc experiments i can see later\n\n\nğŸ¤–ReplyğŸ¤–\nGreat plan! Reading the architecture section first will give you the foundation you need.\nWhat to focus on:\n\nSection 3: Model Architecture (pages 3-5)\n\n3.1: Overall encoder-decoder structure\n3.2: Attention mechanisms (especially Multi-Head Attention)\n3.3: Position-wise Feed-Forward Networks\n3.4: Embeddings and Positional Encoding\n\n\nYou can skip sections 4 (training details) and 5-6 (experiments/results) for now.\nTake your time reading, and when you come back, let me know: 1. What clicked for you? 2. Whatâ€™s still confusing? 3. Whether youâ€™re ready to continue implementing Multi-Head Attention!\nSee you when youâ€™re back! ğŸ“š"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#reading-paper",
    "href": "attention_is_all_you_need_dup1.html#reading-paper",
    "title": "Simple 2x2 example",
    "section": "Reading Paper",
    "text": "Reading Paper\n\n!pip install pypdf2\nfrom PyPDF2 import PdfReader\nreader = PdfReader('../Attention.pdf')\ntext = '\\n'.join([page.extract_text() for page in reader.pages])\n\nRequirement already satisfied: pypdf2 in /app/data/.local/lib/python3.12/site-packages (3.0.1)\n\n\nAttention Paper Text $text\nis the attention paper in context now?\n\nğŸ¤–ReplyğŸ¤–\nYes! I can see youâ€™ve loaded the Attention paper PDF and extracted the text into the text variable. The paper content is now available in your context.\nWould you like me to help you: 1. Find specific sections (like Section 3.2 on Multi-Head Attention)? 2. Explain parts that are confusing as you read? 3. Answer questions about what youâ€™ve read so far?\nOr are you still reading through it on your own first?\nWhat does transduction mean?\n\n\nğŸ¤–ReplyğŸ¤–\nGood question! Transduction in the context of this paper means transforming one sequence into another sequence.\nExamples of transduction tasks: - Machine translation: English sentence â†’ French sentence - Text summarization: Long article â†’ Short summary\n- Speech recognition: Audio sequence â†’ Text sequence\nItâ€™s basically any task where you take an input sequence and produce an output sequence, which might be in a different â€œformâ€ or language.\nThe paper calls the Transformer a â€œtransduction modelâ€ because it takes a source sequence (e.g., English) and produces a target sequence (e.g., German).\nDoes that make sense? What section are you reading now?\nThe encoder transforms the sequence X to sequence Z. What are they referring to?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me clarify what X and Z represent:\nX (input sequence): The original input to the encoder - Example: The English sentence â€œHello worldâ€ - Represented as token embeddings: (xâ‚, xâ‚‚, â€¦, xâ‚™) - These are discrete tokens converted to vectors\nZ (continuous representations): The encoderâ€™s output - A sequence of continuous vector representations: (zâ‚, zâ‚‚, â€¦, zâ‚™) - Same length as input, but now â€œenrichedâ€ with context from the whole sentence - Each záµ¢ contains information about xáµ¢ AND its relationship to all other tokens\nThe transformation: X â†’ Z - X is the â€œrawâ€ embedded input - Z is the contextualized representation after all the encoder layers process it - Z captures meaning, relationships, and context\nThen the decoder uses Z (along with previous outputs) to generate the target sequence Y.\nDoes that clarify it? What part of the architecture are you reading about now?\nIâ€™m reading Section 3.1.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Section 3.1 covers the overall encoder-decoder structure.\nKey points in 3.1:\n\nEncoder: Takes input sequence, produces contextualized representations (the Z we just discussed)\nDecoder: Takes Z + previously generated outputs, produces next token (auto-regressive)\nBoth encoder and decoder are stacks of N=6 identical layers\nEach layer has sub-layers with residual connections and layer normalization\n\nAs you read through, are the concepts of: 1. Stacked layers (N=6 identical layers on top of each other) clear? 2. Auto-regressive generation (decoder generates one token at a time, using previous outputs) making sense?\nLet me know what questions come up or when youâ€™re ready to move to Section 3.2 (the attention mechanisms)!\nWhy do we divide by square root of dk?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent question! This is one of the key insights in the attention mechanism.\nThe problem without scaling:\nWhen d_k (the dimension) is large, the dot products QÂ·K^T can get very large in magnitude.\nWhy is this bad?\nLarge dot products â†’ softmax gets pushed into regions with extremely small gradients â†’ model canâ€™t learn well (vanishing gradients)\nThe math intuition:\nIf Q and K have components that are random variables with mean 0 and variance 1, then their dot product has: - Mean: 0 - Variance: d_k (grows with dimension!)\nBy dividing by âˆšd_k, we scale the variance back down to ~1, keeping the values in a reasonable range for softmax.\nIn simple terms: It prevents the attention scores from getting too extreme, which keeps the softmax working properly and gradients flowing during training.\nDoes this make sense? Have you gotten to Section 3.2.1 yet where they explain this?\nGive me an actual numerical example.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Let me show you with actual numbers.\nWithout scaling (d_k = 64):\nSay Q and K are random vectors with values around -1 to 1:\nQ = [0.5, -0.3, 0.8, ..., 0.2]  (64 dimensions)\nK = [0.4, 0.6, -0.5, ..., 0.7]  (64 dimensions)\n\nQÂ·K = 0.5*0.4 + (-0.3)*0.6 + ... (64 terms)\n    â‰ˆ 25.3  (large number!)\nAfter softmax on large scores:\nscores = [25.3, 18.7, 30.1, 12.4]\nsoftmax = [0.82, 0.001, 0.999, 0.0001]  â† Very extreme! Almost one-hot\nWith scaling (divide by âˆš64 = 8):\nscaled_scores = [3.16, 2.34, 3.76, 1.55]\nsoftmax = [0.35, 0.15, 0.45, 0.05]  â† Much smoother distribution\nSee the difference? Without scaling, softmax becomes too â€œconfidentâ€ (almost all weight on one item). With scaling, you get a more balanced attention distribution.\nDoes this numerical example clarify why we need the âˆšd_k scaling?\nSo, these scores are what for each sequence length?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me clarify what these scores represent.\nThe scores are for each pair of positions in the sequence.\nIf you have a sequence of length 4 (e.g., 4 words), you get a 4Ã—4 matrix of scores:\n        word1  word2  word3  word4\nword1   [25.3  18.7  30.1  12.4]  â† how much word1 attends to each word\nword2   [20.1  22.5  15.3  28.7]  â† how much word2 attends to each word\nword3   [19.2  31.4  21.8  16.5]  â† how much word3 attends to each word\nword4   [27.6  14.2  23.1  19.8]  â† how much word4 attends to each word\nEach row is one query asking â€œhow much should I attend to each position?â€\nEach column is one key saying â€œhereâ€™s how relevant I am to each queryâ€\nSo for a sequence of length n, you get an nÃ—n attention score matrix.\nDoes that clarify what the scores represent?\nThe paper we mentioned, â€œAdditive Attentionâ€, what the heck is that? Give some examples with numbers as well.\n\n\nğŸ¤–ReplyğŸ¤–\nGood question! Additive attention is an older attention mechanism that came before the dot-product attention used in Transformers.\nAdditive Attention (Bahdanau attention):\nInstead of QÂ·K^T, it uses a small neural network:\nscore = v^T Â· tanh(Wâ‚Â·Q + Wâ‚‚Â·K)\nWhere v, Wâ‚, Wâ‚‚ are learned parameters.\nNumerical example:\nSay d_k = 3, and we have:\nQ = [1, 2, 1]\nK = [2, 0, 1]\n\nStep 1: Linear transformations\nWâ‚Â·Q = [0.5, 1.5]  (project to hidden dim=2)\nWâ‚‚Â·K = [1.0, 0.5]\n\nStep 2: Add and apply tanh\ntanh([0.5+1.0, 1.5+0.5]) = tanh([1.5, 2.0]) = [0.91, 0.96]\n\nStep 3: Project to scalar\nv^T Â· [0.91, 0.96] = [0.5, 0.6] Â· [0.91, 0.96] = 1.03\nKey difference: Additive uses a feedforward network, dot-product just multiplies. Dot-product is faster (just matrix multiplication) which is why Transformers use it.\nDoes this comparison make sense?\nThe paper says it works better for larger values of decay. So do we use additive attention for larger values of decay?\n\n\nğŸ¤–ReplyğŸ¤–\nGood catch reading carefully! Let me clarify what the paper actually says:\nThe paper says additive attention outperforms dot-product attention WITHOUT scaling for larger values of d_k.\nBut then they add the scaling factor (dividing by âˆšd_k), which fixes the problem. So with scaling, dot-product attention works well even for large d_k.\nIn practice: - Transformers use scaled dot-product attention (what you implemented) - Additive attention is rarely used anymore because scaled dot-product is: - Faster (just matrix multiplication, highly optimized) - More space-efficient - Works just as well with the scaling fix\nSo no, we donâ€™t switch to additive attention for large d_k. We just use the âˆšd_k scaling to keep dot-product working well!\nDoes that clear up the confusion?\nSo, in practise, we are never using additive attention, am I correct?\n\n\nğŸ¤–ReplyğŸ¤–\nExactly correct!\nIn modern practice, we donâ€™t use additive attention. The Transformer paper mentions it mainly for historical context and to explain why they chose scaled dot-product attention.\nWhat we actually use: - Scaled dot-product attention (what youâ€™ve already implemented!) - Itâ€™s faster, simpler, and works great with the âˆšd_k scaling\nThe paper just wanted to acknowledge that additive attention exists and explain why dot-product + scaling is the better choice.\nAre you ready to move on to Section 3.2.2 about Multi-Head Attention, or do you have more questions about the attention mechanism itself?\nOkay, Iâ€™m reading this. Iâ€™m kind of lost. We take q, k, and v as inputs and project them using wiq, wiv, and wik. Am I correct?\n\n\nğŸ¤–ReplyğŸ¤–\nYes, youâ€™re on the right track! Let me clarify the flow:\nIn Multi-Head Attention:\n\nYou start with the same input (letâ€™s call it X)\nYou project it to get Q, K, V:\n\nQ = X @ W^Q\nK = X @ W^K\n\nV = X @ W^V\n\nThen for multi-head, each head has its own projection matrices:\n\nHead i uses: W^Q_i, W^K_i, W^V_i\nSo you get Q_i, K_i, V_i for each head\n\nRun attention on each head separately\nConcatenate all head outputs\nFinal projection with W^O\n\nKey point: Youâ€™re projecting the input X multiple times (once per head), not projecting Q, K, V again.\nDoes this clarify the flow? Or are you still confused about which projections happen when?\nOh, so the outputs are D_K. Which is basically the input length divided by the number of heads. Am I correct?\n\n\nğŸ¤–ReplyğŸ¤–\nNot quite! Let me clarify the dimensions:\nd_k is NOT the input length. Itâ€™s the dimension per head.\nHereâ€™s the breakdown:\n\nd_model: The full model dimension (e.g., 512)\nh: Number of heads (e.g., 8)\nd_k: Dimension per head = d_model / h (e.g., 512/8 = 64)\n\nThe shapes:\nInput X: (batch, seq_len, d_model)\n         e.g., (32, 50, 512)\n\nAfter projecting and splitting into heads:\nQ, K, V: (batch, h, seq_len, d_k)\n         e.g., (32, 8, 50, 64)\nSo seq_len (input length) stays the same. Weâ€™re splitting the feature dimension (d_model) across heads, not the sequence length!\nDoes this distinction make sense?\nBut we are basically multiplying the whole d_model input with the W to make projections of size d_k. Am I correct?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! Let me clarify the projection step:\nWhat actually happens:\nYou multiply the input (d_model) with W to get an output thatâ€™s still d_model size, then you reshape/split it into h heads of size d_k each.\nInput: (batch, seq_len, d_model=512)\n       â†“\nW^Q projection: (d_model=512, d_model=512)\n       â†“\nQ: (batch, seq_len, d_model=512)\n       â†“\nReshape/split into heads: (batch, h=8, seq_len, d_k=64)\nSo the projection matrix W^Q is 512Ã—512, not 512Ã—64. You get the full d_model output, then split it into heads.\nAlternative approach (mathematically equivalent): Some implementations do use h separate smaller matrices (d_model â†’ d_k), but conceptually itâ€™s the same.\nDoes this clarify the projection dimensions?\nOkay, Iâ€™m pretty much saying the same thing because the paper talks about WI. Which means the weight for each head.\n\n\nğŸ¤–ReplyğŸ¤–\nAh yes, youâ€™re absolutely right! I apologize for the confusion.\nYouâ€™re correct - the paper uses W^Q_i, W^K_i, W^V_i for each head i, which are the individual projection matrices for that specific head.\nSo yes, each head has its own: - W^Q_i: projects from d_model to d_k - W^K_i: projects from d_model to d_k\n- W^V_i: projects from d_model to d_k\nThen you run attention separately on each headâ€™s Q_i, K_i, V_i.\nYou had it right! The paper does describe it with separate weight matrices per head (the subscript i indicates which head).\nSo to confirm: you multiply the d_model input with W_i to get projections of size d_k for each head. âœ“\nReady to keep reading, or want to clarify anything else about multi-head attention?\nWell, I understand your point too that in application, you just have a consolidated W. Am I correct?\n\n\nğŸ¤–ReplyğŸ¤–\nExactly! Youâ€™ve got it!\nIn practice (implementation): - You often have one big consolidated W^Q matrix (d_model â†’ d_model) - Then you reshape/split the output into h heads of d_k each - This is computationally more efficient (one big matrix multiply vs.Â h small ones)\nConceptually (in the paper): - They describe it as h separate W^Q_i matrices (d_model â†’ d_k) - This makes the math clearer to understand\nBoth views are equivalent - just different ways of thinking about/implementing the same operation!\nHave you finished reading Section 3.2.2 on Multi-Head Attention? Ready to start implementing it, or do you want to read more of the paper first?"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#multiheadedattention",
    "href": "attention_is_all_you_need_dup1.html#multiheadedattention",
    "title": "Simple 2x2 example",
    "section": "MultiHeadedAttention",
    "text": "MultiHeadedAttention\nyes lets implement now\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s implement Multi-Head Attention.\nWeâ€™ll start with the __init__ method. You need to store: 1. Number of heads (h) and dimension per head (d_k) 2. The projection matrices (weâ€™ll use the consolidated approach - 4 linear layers) 3. A dropout layer\nTry writing this:\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0  # d_model must be divisible by h\n        \n        self.d_k = d_model // h\n        self.h = h\n        \n        # Create 4 linear layers (Q, K, V projections + final output)\n        # Hint: you can use a list or create them individually\n        \n        self.dropout = nn.Dropout(p=dropout)\nCan you add the 4 linear layers? Each should be nn.Linear(d_model, d_model).\n\nclass MultiHeadedAttention(torch.nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0  # d_model must be divisible by h\n        \n        self.d_k = d_model // h\n        self.h = h\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.attn=None\n        \n        # Create 4 linear layers (Q, K, V projections + final output)\n        # Hint: you can use a list or create them individually\n        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(d_model, d_model) for x in range(4)])\n\nall good?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Your __init__ is perfect! ğŸ‰\nYouâ€™ve got: - âœ“ d_k calculation - âœ“ All 4 linear layers in a ModuleList - âœ“ Dropout layer - âœ“ self.attn for storing attention weights (good thinking!)\nNow letâ€™s implement the forward method. The steps are:\n\nGet batch size\nProject input to Q, K, V using first 3 linear layers\nReshape from (batch, seq_len, d_model) to (batch, h, seq_len, d_k)\nApply attention function\nReshape back and concatenate heads\nApply final linear layer\n\nTry starting with steps 1-2. Can you write the code to: - Get the batch size from the query input - Apply the first 3 linear layers to query, key, value?\n\ndef forward(self, query, key, value, mask=None):\n    # Step 1: Project query, key, value using the first 3 linear layers\n    Q = self.linear_layers[0](query)\n    K = self.linear_layers[1](key)\n    V = self.linear_layers[2](value)\n    # Step 2: Reshape to split into h heads\n    batch_size = Q.size(0)\n    seq_len = Q.size(1)\n    Q = Q.view(batch_size, self.h, seq_len, self.d_k)\n    K = K.view(batch_size, self.h, seq_len, self.d_k)\n    V = V.view(batch_size, self.h, seq_len, self.d_k)\n    # Step 3: Apply attention\n\n    # Step 4: Concatenate heads back together\n    # Step 5: Apply final linear projection\n\nSo far, so good?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat progress! Your projection and reshaping look good. One small issue with the reshape:\nThe order should be (batch_size, seq_len, h, d_k) first, then use .transpose(1, 2) to get (batch_size, h, seq_len, d_k).\nTry changing:\nQ = Q.view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\nThis makes the reshaping cleaner. Does that make sense?\nOnce you fix that, try Step 3 - call your attention() function with Q, K, V. What do you get back, and what shape will it be?\nWhy is that cleaner? I donâ€™t understand.\n\n\nğŸ¤–ReplyğŸ¤–\nGood question! Let me explain why.\nYour approach:\nQ.view(batch_size, self.h, seq_len, self.d_k)\nThis directly reshapes the flat d_model dimension into (h, seq_len, d_k), but the data in memory isnâ€™t organized that way yet.\nThe cleaner approach:\nQ.view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\nStep 1: view(batch_size, seq_len, self.h, self.d_k) - splits d_model into (h, d_k) while keeping seq_len intact Step 2: .transpose(1, 2) - swaps seq_len and h dimensions to get (batch_size, h, seq_len, d_k)\nWhy is it cleaner? The view operation splits the last dimension naturally (d_model â†’ h Ã— d_k), which matches how the data is laid out in memory. Then transpose just reorders the dimensions.\nYour way might work, but the viewâ†’transpose pattern is the standard approach and less likely to cause weird reshaping bugs.\nWant to try updating your code with this pattern?\nOh, so I understand now. We are first trying to split DM into H pieces. Then we swap the dimension edge and sequence length. So that we can pass it into H attention heads.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly! Youâ€™ve got it! ğŸ‰\nThe logic: 1. Split d_model into h pieces of size d_k: (batch, seq_len, d_model) â†’ (batch, seq_len, h, d_k) 2. Swap so heads come before seq_len: (batch, seq_len, h, d_k) â†’ (batch, h, seq_len, d_k) 3. Now each head can process its own (seq_len, d_k) slice independently in parallel!\nThis organization makes it easy to run attention on all h heads at once.\nNow try updating your reshape code with this pattern, then move to Step 3 - calling the attention() function. What will you pass to it?\n\ndef forward(self, query, key, value, mask=None):\n    # Step 1: Project query, key, value using the first 3 linear layers\n    Q = self.linear_layers[0](query)\n    K = self.linear_layers[1](key)\n    V = self.linear_layers[2](value)\n    # Step 2: Reshape to split into h heads\n    batch_size = Q.size(0)\n    seq_len = Q.size(1)\n    Q = Q.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    K = K.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    V = V.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    # Step 3: Apply attention\n\n    # Step 4: Concatenate heads back together\n    # Step 5: Apply final linear projection\n\nbetter now?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your reshaping is now correct. âœ“\nNow for Step 3 - applying attention. You need to call your attention() function.\nWhat arguments will you pass to it? Remember the signature:\nattention(Q, K, V, dropout, mask)\nWhat should each argument be? Give it a try!\ngood?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! A few small issues:\n\nself.mask doesnâ€™t exist - you should pass the mask parameter from the function arguments\nYouâ€™re passing mask twice - check your attention function signature\nStore the result - you need to capture what attention returns\n\nYour attention function signature is:\ndef attention(Q, K, V, dropout, mask):\nSo it should be:\nx = attention(Q, K, V, self.dropout, mask)\nAlso, one more thing: if mask is provided, you need to unsqueeze it to match the head dimension. Try:\nif mask is not None:\n    mask = mask.unsqueeze(1)  # Add head dimension\nCan you fix these issues?\nWhat is mask unsqueeze 1? I donâ€™t understand.\n\n\nğŸ¤–ReplyğŸ¤–\nGood question! Let me explain unsqueeze.\nWhat is unsqueeze?\nIt adds a new dimension to a tensor at the specified position.\nExample:\nmask = torch.tensor([[1, 1, 0, 0]])  # shape: (1, 4)\n\nmask.unsqueeze(0)  # shape: (1, 1, 4) - adds dim at position 0\nmask.unsqueeze(1)  # shape: (1, 1, 4) - adds dim at position 1\nmask.unsqueeze(2)  # shape: (1, 4, 1) - adds dim at position 2\nWhy do we need it here?\nYour mask comes in with shape: (batch_size, seq_len, seq_len)\nBut after splitting into heads, Q, K, V have shape: (batch_size, h, seq_len, d_k)\nSo the mask needs to be: (batch_size, 1, seq_len, seq_len) - the 1 broadcasts across all h heads.\nmask.unsqueeze(1) adds that dimension at position 1 (after batch_size).\nDoes this make sense? Want to see a numerical example?\nI still donâ€™t understand how the one broadcast across all H heads.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me explain broadcasting.\nWhat is broadcasting?\nWhen you do operations between tensors of different shapes, PyTorch automatically â€œstretchesâ€ dimensions of size 1 to match the other tensor.\nExample:\nA = torch.tensor([[[1, 2, 3]]])  # shape: (1, 1, 3)\nB = torch.tensor([[[10], [20]]])  # shape: (1, 2, 1)\n\nA + B  # Result shape: (1, 2, 3)\n# The 1s get \"broadcast\" (repeated) to match\nIn our case:\nmask shape:      (batch, 1,     seq_len, seq_len)\nattention shape: (batch, h=8,   seq_len, seq_len)\n                          â†‘\n                    This 1 broadcasts to 8\nThe mask with dimension 1 automatically gets copied/repeated 8 times (once for each head) when you apply it.\nSo the same mask applies to all heads, but PyTorch handles the copying automatically!\nDoes this clarify how broadcasting works?\n\ndef forward(self, query, key, value, mask=None):\n    # Step 1: Project query, key, value using the first 3 linear layers\n    Q = self.linear_layers[0](query)\n    K = self.linear_layers[1](key)\n    V = self.linear_layers[2](value)\n    # Step 2: Reshape to split into h heads\n    batch_size = Q.size(0)\n    seq_len = Q.size(1)\n    Q = Q.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    K = K.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    V = V.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    # Step 3: Apply attention\n    if mask is not None:\n        mask = mask.unsqueeze(1)\n    x = attention(Q,K,V, self.dropout, mask)\n\n    # Step 4: Concatenate heads back together\n    # Step 5: Apply final linear projection\n\nIs this better now?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Step 3 is now correct! âœ“\nYouâ€™ve got: - âœ“ Mask unsqueeze for broadcasting across heads - âœ“ Calling attention with the right arguments - âœ“ Storing result in x\nNow for Step 4: Concatenate heads back together\nAfter attention, x has shape: (batch_size, h, seq_len, d_k)\nYou need to get back to: (batch_size, seq_len, d_model)\nThis is the reverse of what you did earlier: 1. Transpose to swap h and seq_len back 2. Reshape/view to merge h and d_k into d_model\nTry writing the code for step 4. What operations do you need?\nWait, Iâ€™m seriously confused. How is reshaping the whole thing led to multiple attention heads? this is the attn fn def attention(Q, K, V, dropout, mask): sqrt_d_k = math.sqrt(K.size(-1)) dropout_layer = torch.nn.Dropout(p=dropout) scores = Q @ K.transpose(-2,-1) / sqrt_d_k if mask is not None: scores = scores.masked_fill(mask==0, -1e9) attention_weights = torch.softmax(scores, dim=-1) attention_weights = dropout_layer(attention_weights) output = attention_weights @ V return output\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent question! This is a key insight about how multi-head attention works.\nThe magic is in the tensor dimensions!\nWhen you pass Q, K, V with shape (batch_size, h, seq_len, d_k) to the attention function, PyTorchâ€™s matrix operations work independently on each head automatically.\nLetâ€™s trace through it:\nQ shape: (batch=2, h=8, seq_len=10, d_k=64)\nK shape: (batch=2, h=8, seq_len=10, d_k=64)\n\nQ @ K.transpose(-2,-1)\nThe @ operator does matrix multiplication on the last two dimensions only. The first dimensions (batch, h) are treated as â€œbatch dimensionsâ€ - so it runs 2Ã—8=16 separate attention operations in parallel!\nIn other words: - You have 8 heads - Each head processes its slice: Q[:, 0, :, :], Q[:, 1, :, :], â€¦ Q[:, 7, :, :] - The matrix operations happen independently for each head - All in one efficient operation!\nDoes this click now? The reshaping sets up the dimensions so that one function call runs all heads in parallel!\nOkay, this makes sense and clicks. No wonder attention (incl multiheaded attention) is so much faster than previous methods like R and N and LSTM.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly! Youâ€™ve hit on one of the key innovations! ğŸ‰\nWhy Transformers are faster:\n\nRNNs/LSTMs: Process sequentially (word 1 â†’ word 2 â†’ word 3â€¦). Canâ€™t parallelize across the sequence.\nTransformers: All positions processed at once! The entire attention matrix is computed in parallel.\n\nPlus, with multi-head attention, all 8 heads also run in parallel thanks to the clever tensor reshaping you just understood.\nThis parallelization is what makes Transformers scale so well with modern GPUs!\nNow, ready to finish Step 4? You need to concatenate the heads back together. Remember: - Current shape: (batch_size, h, seq_len, d_k) - Target shape: (batch_size, seq_len, d_model)\nWhat operations do you need? (Hint: reverse of what you did earlier)\n\ndef forward(self, query, key, value, mask=None):\n    # Step 1: Project query, key, value using the first 3 linear layers\n    Q = self.linear_layers[0](query)\n    K = self.linear_layers[1](key)\n    V = self.linear_layers[2](value)\n    # Step 2: Reshape to split into h heads\n    batch_size = Q.size(0)\n    seq_len = Q.size(1)\n    Q = Q.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    K = K.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    V = V.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    # Step 3: Apply attention\n    if mask is not None:\n        mask = mask.unsqueeze(1)\n    x = attention(Q,K,V, self.dropout, mask)\n\n    # Step 4: Concatenate heads back together\n    x = x.transpose(1,2).view(batch_size,seq_len, -1)\n    # Step 5: Apply final linear projection\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Step 4 is correct! âœ“\nYouâ€™ve done: - .transpose(1,2) to swap h and seq_len back: (batch, h, seq_len, d_k) â†’ (batch, seq_len, h, d_k) - .view(batch_size, seq_len, -1) to merge the last two dims: (batch, seq_len, h, d_k) â†’ (batch, seq_len, d_model)\nThe -1 automatically calculates h Ã— d_k = d_model. Nice!\nNow for Step 5: Apply final linear projection\nYou need to pass x through the 4th linear layer (the output projection W^O). This mixes information across all the concatenated heads.\nCan you add that final step?\n\ndef forward(self, query, key, value, mask=None):\n    # Step 1: Project query, key, value using the first 3 linear layers\n    Q = self.linear_layers[0](query)\n    K = self.linear_layers[1](key)\n    V = self.linear_layers[2](value)\n    # Step 2: Reshape to split into h heads\n    batch_size = Q.size(0)\n    seq_len = Q.size(1)\n    Q = Q.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    K = K.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    V = V.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    # Step 3: Apply attention\n    if mask is not None:\n        mask = mask.unsqueeze(1)\n    x = attention(Q,K,V, self.dropout, mask)\n    # Step 4: Concatenate heads back together\n    x = x.transpose(1,2).view(batch_size,seq_len, -1)\n    # Step 5: Apply final linear projection\n    output = self.linear_layers[3](x)\n    return output\n\nAm I successful now?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent work! Your Multi-Head Attention implementation is complete! ğŸ‰\nYouâ€™ve successfully implemented: - âœ“ Projecting to Q, K, V - âœ“ Splitting into multiple heads - âœ“ Running attention on all heads in parallel - âœ“ Concatenating heads back - âœ“ Final output projection\nNow letâ€™s test it to make sure it works! Try this:\n# Test with realistic dimensions\nmha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = mha.forward(x, x, x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this and see if it works! What do you get?\nWait a minute, shouldnâ€™t we put the class and the function together?\n\n\nğŸ¤–ReplyğŸ¤–\nGood catch! Yes, you need to add the forward method inside the MultiHeadedAttention class!\nRight now you have the class definition with __init__, but the forward method is separate. You need to indent the forward method so itâ€™s part of the class.\nIt should look like:\nclass MultiHeadedAttention(torch.nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        # ... your init code ...\n    \n    def forward(self, query, key, value, mask=None):\n        # ... your forward code ...\nCan you combine them properly?\n\nclass MultiHeadedAttention(torch.nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0  # d_model must be divisible by h\n        \n        self.d_k = d_model // h\n        self.h = h\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.attn=None\n        \n        # Create 4 linear layers (Q, K, V projections + final output)\n        # Hint: you can use a list or create them individually\n        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(d_model, d_model) for x in range(4)])\n    \n    def forward(self, query, key, value, mask=None):\n        # Step 1: Project query, key, value using the first 3 linear layers\n        Q = self.linear_layers[0](query)\n        K = self.linear_layers[1](key)\n        V = self.linear_layers[2](value)\n        # Step 2: Reshape to split into h heads\n        batch_size = Q.size(0)\n        seq_len = Q.size(1)\n        Q = Q.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n        K = K.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n        V = V.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n        # Step 3: Apply attention\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        x = attention(Q,K,V, self.dropout, mask)\n        # Step 4: Concatenate heads back together\n        x = x.transpose(1,2).view(batch_size,seq_len, -1)\n        # Step 5: Apply final linear projection\n        output = self.linear_layers[3](x)\n        return output\n\nall good?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your MultiHeadedAttention class is now complete and properly structured! âœ“\nNow letâ€™s test it to make sure everything works:\n# Test with realistic dimensions\nmha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = mha.forward(x, x, x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this and let me know if it works! Does the output shape match what you expect?\n\n# Test with realistic dimensions\nmha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = mha.forward(x, x, x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[1], line 5\n      2 mha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n      3 x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n----&gt; 5 output = mha.forward(x, x, x, mask=None)\n      6 print(\"Output shape:\", output.shape)\n      7 print(\"Expected:\", (2, 10, 512))\n\nCell In[1], line 31, in MultiHeadedAttention.forward(self, query, key, value, mask)\n     29 x = attention(Q,K,V, self.dropout, mask)\n     30 # Step 4: Concatenate heads back together\n---&gt; 31 x = x.transpose(1,2).view(batch_size,seq_len, -1)\n     32 # Step 5: Apply final linear projection\n     33 output = self.linear_layers[3](x)\n\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n\n\n\nno, whyyyy?\n\n\nğŸ¤–ReplyğŸ¤–\nDonâ€™t worry, this is a common issue! The error is telling you exactly what to do.\nThe problem: After .transpose(), the tensor is no longer â€œcontiguousâ€ in memory, so .view() doesnâ€™t work.\nThe fix: Use .contiguous() before .view(), or just use .reshape() instead:\nChange this line:\nx = x.transpose(1,2).view(batch_size, seq_len, -1)\nTo either:\nx = x.transpose(1,2).contiguous().view(batch_size, seq_len, -1)\nOr simply:\nx = x.transpose(1,2).reshape(batch_size, seq_len, -1)\n.reshape() handles the contiguity automatically! Try updating your code and run the test again.\nI donâ€™t understand, what do you mean by â€œcontiguous?â€ What exactly is the issue?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me explain contiguous memory.\nWhat is contiguous?\nContiguous means the tensorâ€™s elements are stored in a single, continuous block of memory in the order youâ€™d expect.\nExample:\nOriginal tensor: [[1, 2], [3, 4]] stored in memory as: [1, 2, 3, 4] âœ“ contiguous\nAfter transpose: [[1, 3], [2, 4]] but memory is still: [1, 2, 3, 4]\nPyTorch just changes how it interprets the memory (using strides), but doesnâ€™t actually move the data. So now itâ€™s not contiguous anymore!\nWhy does .view() care?\n.view() requires contiguous memory because itâ€™s a â€œzero-copyâ€ operation - it just reinterprets the existing memory layout. If the memory isnâ€™t laid out right, it canâ€™t do that.\nSolutions: - .contiguous() - actually rearranges memory to be contiguous (makes a copy) - .reshape() - automatically calls .contiguous() if needed\nDoes this make sense? Try using .reshape() instead of .view() and it should work!\n\n\nFinal MultiHeaded Attention Class\n\n\n\nMultiHeadedAttention\n\n MultiHeadedAttention (h, d_model, dropout=0.1)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nIs this better?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Now letâ€™s test it again:\n# Test with realistic dimensions\nmha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = mha.forward(x, x, x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this and see if it works now!\n\n# Test with realistic dimensions\nmha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = mha.forward(x, x, x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nAwesome. Convert this into unit test. Assume Iâ€™m using NB dev.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Hereâ€™s your Multi-Head Attention as a unit test for nbdev:\n#| export\nclass MultiHeadedAttention(torch.nn.Module):\n    \"Multi-Head Attention mechanism\"\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0  # d_model must be divisible by h\n        \n        self.d_k = d_model // h\n        self.h = h\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.attn = None\n        \n        # Create 4 linear layers (Q, K, V projections + final output)\n        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(d_model, d_model) for _ in range(4)])\n    \n    def forward(self, query, key, value, mask=None):\n        # Step 1: Project query, key, value using the first 3 linear layers\n        Q = self.linear_layers[0](query)\n        K = self.linear_layers[1](key)\n        V = self.linear_layers[2](value)\n        # Step 2: Reshape to split into h heads\n        batch_size = Q.size(0)\n        seq_len = Q.size(1)\n        Q = Q.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n        K = K.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n        V = V.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n        # Step 3: Apply attention\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        x = attention(Q, K, V, self.dropout, mask)\n        # Step 4: Concatenate heads back together\n        x = x.transpose(1,2).reshape(batch_size, seq_len, -1)\n        # Step 5: Apply final linear projection\n        output = self.linear_layers[3](x)\n        return output\n#| test\ndef test_multihead_attention():\n    \"Test Multi-Head Attention with realistic dimensions\"\n    mha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n    \n    output = mha.forward(x, x, x, mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nGreat work! Youâ€™ve successfully implemented Multi-Head Attention from scratch! ğŸ‰\nWhat would you like to tackle next? Position-wise Feed-Forward Networks, or something else from the architecture?\n\n\nUnit Test\n\ndef test_multihead_attention():\n    \"Test Multi-Head Attention with realistic dimensions\"\n    mha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n    \n    output = mha.forward(x, x, x, mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n\n\ntest_multihead_attention()\n\nGreat, Iâ€™ve exported using NB Dev, now letâ€™s move on.\n\nğŸ¤–ReplyğŸ¤–\nAwesome! ğŸ‰ Your Multi-Head Attention is working perfectly!\nYouâ€™ve now implemented two major components: 1. âœ“ Scaled Dot-Product Attention 2. âœ“ Multi-Head Attention\nWhatâ€™s next?\nLooking at the Transformer architecture, the main components left are:\n\nPosition-wise Feed-Forward Network - a simple 2-layer MLP applied to each position\nPositional Encoding - adds position information to embeddings\nLayer Normalization & Residual Connections - for stable training\nEncoder Layer - combines attention + feed-forward\nDecoder Layer - similar but with masked attention + cross-attention\nFull Encoder/Decoder stacks\n\nWhich would you like to tackle next? Iâ€™d suggest the Feed-Forward Network since itâ€™s simpler and will give you a quick win!\nI just have one question. So, all these linear layers we are using During multi-headed attention, why arenâ€™t there any non-linearities like ReLU at the end of these linear layers?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent question! This is a really important design choice.\nWhy no non-linearity in the projections?\nThe Q, K, V projections are meant to be linear transformations of the input into different subspaces. Adding non-linearities would change the nature of the dot-product attention mechanism.\nThink about it: attention computes similarity via dot products (QÂ·K^T). If you applied ReLU before this, youâ€™d be computing similarity between â€œrectifiedâ€ representations, which doesnâ€™t have the same mathematical properties.\nWhere DO we use non-linearities?\nThe non-linearity (ReLU) comes in the Feed-Forward Network that follows the attention layer! Thatâ€™s where the model gets its non-linear transformation capacity.\nThe pattern in each layer: 1. Multi-Head Attention (linear projections only) 2. Add & Norm (residual connection) 3. Feed-Forward Network (has ReLU!) â† non-linearity here 4. Add & Norm\nSo the model does have non-linearities, just not inside the attention projections themselves.\nDoes this distinction make sense?"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#positionwisefeedforward",
    "href": "attention_is_all_you_need_dup1.html#positionwisefeedforward",
    "title": "Simple 2x2 example",
    "section": "PositionwiseFeedForward",
    "text": "PositionwiseFeedForward\nYes, this makes sense. Letâ€™s move on to the next topic. Make sure weâ€™re sticking to the code which is given in the beginning.\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s move on to the Position-wise Feed-Forward Network.\nLooking at the paper (Section 3.3), this is a simple component:\nFFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚\nItâ€™s just: 1. A linear layer that expands dimensions (d_model â†’ d_ff) 2. ReLU activation 3. Another linear layer that projects back (d_ff â†’ d_model)\nTypically d_ff = 2048 when d_model = 512 (4x expansion).\nTry implementing the class:\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        # What do you need here?\n        \n    def forward(self, x):\n        # What's the sequence of operations?\nWhat layers do you need in __init__? Give it a try!\n\nclass PositionwiseFeedForward(torch.nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        # What do you need here?\n        self.linear_layer = torch.nn.Linear(d_model, d_ff)\n        self.relu = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.output_layer = torch.nn.Linear(d_ff, d_model)\n        \n    def forward(self, x):\n        # What's the sequence of operations?\n        pass\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Your __init__ is perfect! âœ“\nYouâ€™ve got all the pieces: - âœ“ First linear layer (d_model â†’ d_ff) - âœ“ ReLU activation - âœ“ Dropout - âœ“ Second linear layer (d_ff â†’ d_model)\nNow for the forward method - whatâ€™s the sequence of operations?\nApply them in order: linear â†’ relu â†’ dropout â†’ linear\nGive it a try!\n\n\nFinal PositionwiseFeedForward Class\n\n\n\nPositionwiseFeedForward\n\n PositionwiseFeedForward (d_model, d_ff, dropout=0.1)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nShould be fine, right?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Feed-Forward Network is complete and correct! âœ“\nThe sequence is exactly right: - âœ“ Expand with first linear layer - âœ“ Apply ReLU non-linearity - âœ“ Apply dropout for regularization - âœ“ Project back with second linear layer\nLetâ€™s test it:\nffn = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = ffn(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this to verify it works, then we can move on to the next component!\n\nffn = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = ffn(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nAssume we are using NB dev. Convert this into a unit test.\n\n\nğŸ¤–ReplyğŸ¤–\nHereâ€™s your Position-wise Feed-Forward Network as a unit test for nbdev:\n#| export\nclass PositionwiseFeedForward(torch.nn.Module):\n    \"Position-wise Feed-Forward Network\"\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear_layer = torch.nn.Linear(d_model, d_ff)\n        self.relu = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.output_layer = torch.nn.Linear(d_ff, d_model)\n        \n    def forward(self, x):\n        x = self.linear_layer(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.output_layer(x)\n        return x\n#| test\ndef test_positionwise_feedforward():\n    \"Test Position-wise Feed-Forward Network\"\n    ffn = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n    \n    output = ffn(x)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nReady to move on to the next component?\n\n\nUnit Test\n\ndef test_positionwise_feedforward():\n    \"Test Position-wise Feed-Forward Network\"\n    ffn = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n    \n    output = ffn(x)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n\n\ntest_positionwise_feedforward()\n\nWhy is it called position-wise feedforward?\n\nğŸ¤–ReplyğŸ¤–\nGreat question!\nâ€œPosition-wiseâ€ means the same feed-forward network is applied to each position independently.\nThink of it this way: - You have a sequence: [word1, word2, word3, ...] - Each word has a vector of size d_model=512 - The FFN processes each wordâ€™s vector separately - word1â€™s vector doesnâ€™t interact with word2â€™s vector\nItâ€™s like applying the same 2-layer neural network to each position in the sequence independently. The same weights are shared across all positions, but each position is processed on its own.\nContrast with attention: Attention mixes information across positions (word1 looks at word2, word3, etc.). The FFN does not mix across positions - it just transforms each positionâ€™s representation individually.\nDoes this clarify why itâ€™s called â€œposition-wiseâ€?\nOkay, that makes it clear. Letâ€™s move on now.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Youâ€™ve now completed: 1. âœ“ Scaled Dot-Product Attention 2. âœ“ Multi-Head Attention 3. âœ“ Position-wise Feed-Forward Network\nWhatâ€™s next?\nThe remaining key components are:\n\nEmbeddings - convert tokens to vectors\nPositional Encoding - add position information (since attention has no inherent notion of order)\nLayer Normalization - for stable training\nResidual Connections - skip connections around sub-layers\nEncoder Layer - combines attention + FFN with residuals and norms\nDecoder Layer - similar but with masked self-attention + cross-attention\n\nIâ€™d suggest Positional Encoding next - itâ€™s a unique and interesting part of the Transformer. It explains how the model knows the order of words.\nWant to tackle that? Or would you prefer a different component?"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#positional-encoding-1",
    "href": "attention_is_all_you_need_dup1.html#positional-encoding-1",
    "title": "Simple 2x2 example",
    "section": "Positional Encoding",
    "text": "Positional Encoding\nOkay, Iâ€™m down for positional encoding. Letâ€™s do it.\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s tackle Positional Encoding.\nWhy do we need it?\nAttention has no inherent sense of order - it treats the sequence like a set. â€œI love pizzaâ€ and â€œpizza love Iâ€ would look the same! Positional encoding adds information about where each word is in the sequence.\nHow does it work?\nThe paper uses sine and cosine functions of different frequencies:\n\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n\nWhere: - pos = position in sequence (0, 1, 2, â€¦) - i = dimension index (0, 1, 2, â€¦, d_model/2)\nThe class structure:\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        # Create the positional encoding matrix\n        # Store it as a buffer (not a parameter)\n        \n    def forward(self, x):\n        # Add positional encoding to input x\nBefore coding, do you understand why they use sin/cos functions? Want me to explain the intuition?\nI think they use sine and cos because sine and cos have different peaks. For example, sin is 1 while cos is 0.\n\n\nğŸ¤–ReplyğŸ¤–\nThatâ€™s part of it! But thereâ€™s more to why sin/cos are special here.\nKey properties they wanted:\n\nUnique encoding for each position - different positions get different patterns\nRelative position information - the model can learn â€œword A is 3 positions before word Bâ€\nExtrapolation - can handle sequences longer than seen in training\n\nWhy sin/cos specifically?\nFor any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos). This is because of the trigonometric identity:\nsin(a + b) = sin(a)cos(b) + cos(a)sin(b)\nThis means the model can easily learn to attend by relative positions!\nDifferent frequencies: Each dimension uses a different frequency (from 2Ï€ to 10000Â·2Ï€), so different dimensions capture patterns at different scales - some for nearby words, some for distant words.\nDoes this deeper intuition make sense? Ready to start implementing it?\nNo, Iâ€™m still not getting it. What Iâ€™m seeing from the formula is that for even dim weâ€™re using sine, and for odd dim weâ€™re using cosine. Secondly, what I am getting is the frequency changes as the dimension index changes. It goes from lower frequency to higher frequency. Is what Iâ€™m saying making sense? Am I getting the correct intuition?\n\n\nğŸ¤–ReplyğŸ¤–\nYes! Youâ€™re getting it exactly right! ğŸ‰\nWhat you understand correctly:\n\nâœ“ Even dimensions use sine, odd dimensions use cosine - this gives you pairs of (sin, cos) at the same frequency\nâœ“ Frequency changes with dimension - but actually it goes from higher to lower frequency as i increases (because we divide by larger numbers: 10000^(2i/d_model) gets bigger)\n\nWhy does frequency matter?\n\nHigh frequency (early dimensions): Changes quickly, good for capturing nearby positions (position 1 vs 2 vs 3)\nLow frequency (later dimensions): Changes slowly, good for capturing distant positions (position 10 vs 50 vs 100)\n\nSo different dimensions encode position information at different scales!\nThe sin/cos pairing: Using both at the same frequency helps the model learn relative positions through those trig identities I mentioned.\nDoes this click now? Ready to implement it?\nHow are early dimensions high frequency and later low frequency? Give a numerical example.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me show you with numbers.\nThe formula for frequency:\ndiv_term = 10000^(2i/d_model)\nLetâ€™s say d_model = 512:\nEarly dimensions (i=0):\n10000^(2*0/512) = 10000^0 = 1\nfrequency = 1/1 = 1  (HIGH frequency - changes fast!)\nMiddle dimensions (i=64):\n10000^(2*64/512) = 10000^0.25 â‰ˆ 10\nfrequency = 1/10 = 0.1  (medium frequency)\nLater dimensions (i=255):\n10000^(2*255/512) = 10000^0.996 â‰ˆ 6310\nfrequency = 1/6310 â‰ˆ 0.00016  (LOW frequency - changes slowly!)\nWhat this means: - Dimension 0: sin(pos/1) changes every position - Dimension 128: sin(pos/10) changes every ~10 positions\n- Dimension 510: sin(pos/6310) barely changes across the sequence\nDoes this numerical example clarify how frequency decreases as i increases?\nOkay, before jumping into the implementation, letâ€™s do some visualisation. Give me some visualisation code without thinking.\n\n\nğŸ¤–ReplyğŸ¤–\nimport matplotlib.pyplot as plt\n\n# Create positional encoding for visualization\nd_model = 64\nmax_len = 100\n\n# Compute positional encodings\npe = torch.zeros(max_len, d_model)\nposition = torch.arange(0, max_len).unsqueeze(1)\ndiv_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n\npe[:, 0::2] = torch.sin(position * div_term)\npe[:, 1::2] = torch.cos(position * div_term)\n\n# Plot\nplt.figure(figsize=(15, 5))\nplt.imshow(pe.T, cmap='RdBu', aspect='auto')\nplt.xlabel('Position')\nplt.ylabel('Dimension')\nplt.colorbar()\nplt.title('Positional Encoding Heatmap')\nplt.show()\n\n# Plot specific dimensions\nplt.figure(figsize=(15, 4))\nfor dim in [0, 2, 10, 30]:\n    plt.plot(pe[:, dim], label=f'dim {dim}')\nplt.xlabel('Position')\nplt.ylabel('Encoding Value')\nplt.legend()\nplt.title('Positional Encoding for Different Dimensions')\nplt.show()\nRun this to see how the encodings look!\nOk, letâ€™s implement now and much clearer.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s implement Positional Encoding.\nStep 1: Create the positional encoding matrix in __init__\nYou need to: 1. Create a matrix pe of shape (max_len, d_model) filled with zeros 2. Create a position vector: [0, 1, 2, â€¦, max_len-1] 3. Calculate div_term using the formula 4. Fill in the even dimensions with sine 5. Fill in the odd dimensions with cosine\nTry starting with the first 3 steps:\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n        \n        # Create position vector - try this yourself!\n        # Calculate div_term - try this yourself!\nCan you create the position vector and calculate div_term? Remember the formulas we discussed!\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n\n        half_range = torch.range(d_model//2)\n        \n        # Create position vector - try this yourself!\n        cosine = torch.cos(pos/ (1000**(2*half_range/d_model)) )\n        sin = torch.sin(pos/ (1000**(2*half_range/d_model)) )\n        pe[::2] = cosine\n        pe[1::2] = sin\n        # Calculate div_term - try this yourself!\n\nAm I doing well so far?\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re on the right track with the concept, but there are a few issues to fix:\n\npos is not defined - you need to create a position vector first: torch.arange(0, max_len)\nhalf_range - use torch.arange(0, d_model, 2) to get even indices\nThe indexing pe[::2] - this wonâ€™t work correctly. You need to fill columns (dimensions), not rows. Use pe[:, 0::2] for even dims and pe[:, 1::2] for odd dims\nBroadcasting - pos needs to be reshaped to broadcast correctly with div_term\n\nTry fixing step 1 first - create the position vector:\nposition = torch.arange(0, max_len).unsqueeze(1)  # shape: (max_len, 1)\nDoes this make sense? Want to try the next step?\nWhatâ€™s the difference between A-range and range?\n\n\nğŸ¤–ReplyğŸ¤–\nGood question!\ntorch.range() (deprecated): - Older function, not recommended - Includes the end value - torch.range(0, 5) gives [0, 1, 2, 3, 4, 5] (6 elements!)\ntorch.arange() (use this!): - Modern version, works like Pythonâ€™s range() - Excludes the end value - torch.arange(0, 5) gives [0, 1, 2, 3, 4] (5 elements)\nSo always use torch.arange() - itâ€™s more predictable and matches Python conventions!\nReady to continue with the implementation?\nWhy are we unsqueezing?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question!\nWhy unsqueeze?\nWe need the shapes to broadcast correctly for the calculation.\nThe shapes: - position: We want shape (max_len, 1) - each row is a position - div_term: Will have shape (d_model/2,) - different frequencies\nWhen we multiply them:\nposition * div_term\nBroadcasting gives us shape (max_len, d_model/2) - every position combined with every frequency!\nWithout unsqueeze: - position would be shape (max_len,) - Multiplication wouldnâ€™t broadcast the way we want\nVisual example:\nposition (100, 1):        div_term (256,):\n[[0],                     [1.0, 0.5, 0.25, ...]\n [1],\n [2],                     Result (100, 256):\n ...]                     Each pos Ã— each frequency\nDoes this clarify why we need that extra dimension?\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n\n        half_range = torch.range(d_model//2)\n        \n        # Create position vector - try this yourself!\n        position = torch.arange(0, max_len).unsqueeze(1)\n        # Calculate div_term - try this yourself!\n        div_term = 10000**(half_range/d_model)\n\nAre we doing better?\n\n\nğŸ¤–ReplyğŸ¤–\nGetting closer! A few more fixes needed:\n\nhalf_range: Should be torch.arange(0, d_model, 2) not torch.range(d_model//2). You want the even indices: [0, 2, 4, 6, â€¦]\ndiv_term formula: You have it backwards! It should be dividing, not multiplying. Remember: we want 10000^(2i/d_model), which means:\ndiv_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\nThis computes the exponential in a numerically stable way.\n\nTry fixing these two things. Does the div_term formula make sense, or do you want me to explain why we use exp and log?\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n\n        even_indices = torch.arange(0, d_model, 2)\n        \n        # Create position vector - try this yourself!\n        position = torch.arange(0, max_len).unsqueeze(1)\n        # Calculate div_term - try this yourself!\n        div_term = torch.exp(-even_indices*(torch.log(10000))/d_model)\n\nAll good now?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your position vector and div_term are now correct! âœ“\nNow you need to fill in the pe matrix with sine and cosine values:\npe[:, 0::2] = torch.sin(position * div_term)\npe[:, 1::2] = torch.cos(position * div_term)\nThis fills: - Even columns (0, 2, 4, â€¦) with sine - Odd columns (1, 3, 5, â€¦) with cosine\nThen you need to: 1. Add a batch dimension: pe = pe.unsqueeze(0) 2. Register it as a buffer (not a trainable parameter): self.register_buffer('pe', pe)\nCan you add these steps to complete the __init__ method?\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n\n        even_indices = torch.arange(0, d_model, 2)\n        \n        # Create position vector - try this yourself!\n        position = torch.arange(0, max_len).unsqueeze(1)\n        # Calculate div_term - try this yourself!\n        div_term = torch.exp(-even_indices*(torch.log(10000))/d_model)\n\n        pe[:, ::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        return pe\n\nAwesome, thatâ€™s great.\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost there! But you canâ€™t return from __init__ - itâ€™s a constructor method.\nInstead, you need to: 1. Add a batch dimension to pe 2. Register it as a buffer so itâ€™s saved with the model but not trained\nAdd these two lines at the end of __init__:\npe = pe.unsqueeze(0)  # Add batch dimension: (1, max_len, d_model)\nself.register_buffer('pe', pe)  # Save as buffer, not parameter\nThen youâ€™ll need to implement the forward method. Can you add these lines first?\nLet me guess; we are unsqueezing to accommodate batch size, arenâ€™t we?\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! ğŸ‰\nThe pe matrix is currently shape (max_len, d_model), but your input will have shape (batch_size, seq_len, d_model).\nBy unsqueezing to (1, max_len, d_model), the batch dimension of size 1 will broadcast across whatever batch_size you have in your input.\nYouâ€™re really getting the hang of these dimension manipulations!\nNow add those two lines to your code, then we can implement the forward method. Ready?\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n\n        even_indices = torch.arange(0, d_model, 2)\n        \n        # Create position vector - try this yourself!\n        position = torch.arange(0, max_len).unsqueeze(1)\n        # Calculate div_term - try this yourself!\n        div_term = torch.exp(-even_indices*(torch.log(10000))/d_model)\n\n        pe[:, ::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\nAlso, we are saving PE as a buffer, as that does not change at all.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly! ğŸ’¯\nWhy use register_buffer instead of a parameter?\n\nParameters (created with nn.Parameter): Are trained/updated during backpropagation\nBuffers (created with register_buffer): Are part of the model state but NOT trained\n\nSince positional encodings are fixed (computed once using sin/cos, never updated), they should be a buffer, not a parameter!\nThis also ensures pe gets moved to GPU when you call model.to('cuda') and gets saved/loaded with the model.\nNow letâ€™s implement the forward method. What should it do? 1. Take input x of shape (batch, seq_len, d_model) 2. Add the positional encoding for the first seq_len positions 3. Apply dropout 4. Return the result\nCan you try writing the forward method?\n\n\nFinal PositionalEncoding\n\n\n\nPositionalEncoding\n\n PositionalEncoding (d_model, dropout, max_len=5000)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nIs this good?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Positional Encoding implementation is complete and correct! âœ“\nYouâ€™ve got: - âœ“ Extract seq_len from input - âœ“ Slice the positional encoding to match: self.pe[:, :seq_len, :] - âœ“ Set requires_grad_(False) to ensure itâ€™s not trained - âœ“ Add it to the input - âœ“ Apply dropout\nLetâ€™s test it:\npe = PositionalEncoding(d_model=512, dropout=0.1, max_len=100)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = pe(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this to verify it works!\n\npe = PositionalEncoding(d_model=512, dropout=0.1, max_len=100)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = pe(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nAssume we are using NB Dev. Convert this into a unit test.\n\n\nğŸ¤–ReplyğŸ¤–\nHereâ€™s your Positional Encoding as a unit test for nbdev:\n#| export\nclass PositionalEncoding(torch.nn.Module):\n    \"Positional Encoding using sine and cosine functions\"\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n        even_indices = torch.arange(0, d_model, 2)\n        \n        # Create position vector\n        position = torch.arange(0, max_len).unsqueeze(1)\n        # Calculate div_term\n        div_term = torch.exp(-even_indices*(torch.log(torch.tensor(10000.0))/d_model))\n        \n        pe[:, ::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        seq_len = x.size(1)\n        x = x + self.pe[:, :seq_len, :].requires_grad_(False)\n        return self.dropout(x)\n#| test\ndef test_positional_encoding():\n    \"Test Positional Encoding\"\n    pe = PositionalEncoding(d_model=512, dropout=0.1, max_len=100)\n    x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n    \n    output = pe(x)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nGreat progress! Ready to move on to the next component?\n\n\nUnit Test\n\ndef test_positional_encoding():\n    \"Test Positional Encoding\"\n    pe = PositionalEncoding(d_model=512, dropout=0.1, max_len=100)\n    x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n    \n    output = pe(x)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n\n\ntest_positional_encoding()\n\nThatâ€™s amazing, letâ€™s go on to the next block.\n\nğŸ¤–ReplyğŸ¤–\nExcellent work! Youâ€™ve now completed the core building blocks: 1. âœ“ Scaled Dot-Product Attention 2. âœ“ Multi-Head Attention 3. âœ“ Position-wise Feed-Forward Network 4. âœ“ Positional Encoding\nWhatâ€™s next?\nNow we need to build the Encoder Layer - this combines the components youâ€™ve built into a complete encoder block.\nAn Encoder Layer contains: 1. Multi-Head Self-Attention 2. Add & Norm (residual connection + layer normalization) 3. Feed-Forward Network 4. Add & Norm again\nBefore we code, do you understand what residual connections and layer normalization are? Should I explain those concepts first, or do you want to dive into the implementation?"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#encoder-layer",
    "href": "attention_is_all_you_need_dup1.html#encoder-layer",
    "title": "Simple 2x2 example",
    "section": "Encoder Layer",
    "text": "Encoder Layer\nYeah, you can test me on these concepts. I somewhat know them.\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s test your understanding:\nQuestion 1: Residual Connections\nWhat is a residual connection, and why do we use them in deep networks? Can you write the simple equation for a residual connection?\nQuestion 2: Layer Normalization\nWhat does layer normalization do? How is it different from batch normalization?\nGive it a shot - explain in your own words!\nq1 Basically a residual connection is you after you do an operation on an input, you also bring the input forward bypassing the operation. so, output = x + sublayer(x)\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Thatâ€™s exactly right! âœ“\nResidual connection: output = x + sublayer(x)\nYouâ€™ve got the key insight - the input â€œskipsâ€ around the operation and gets added back. This helps with: - Gradient flow during backpropagation (prevents vanishing gradients) - Allows the network to learn identity mappings easily - Makes training very deep networks possible\nNow for Question 2:\nWhat does layer normalization do, and how does it work? (Hint: think about computing mean and standard deviation)\nSo basically, across all the dimensions, we take the mean and standard deviation, and we normalise the outputs.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! âœ“\nLayer Normalization: - Computes mean and std across all dimensions (features) for each example - Normalizes: (x - mean) / std - Then applies learned scale (Î³) and shift (Î²) parameters\nKey difference from Batch Normalization: - Batch Norm: normalizes across the batch dimension (all examples) - Layer Norm: normalizes across the feature dimension (for each example independently)\nLayer Norm works better for sequences because sequence lengths can vary, making batch statistics unreliable.\nNow you understand both concepts! Ready to implement the Encoder Layer that uses these? It will combine: - Multi-Head Attention - Residual + Layer Norm - Feed-Forward - Residual + Layer Norm again\nWant to start coding?\nlets code!!!\n\n\nğŸ¤–ReplyğŸ¤–\nAwesome! Letâ€™s build the Encoder Layer!\nFirst, letâ€™s think about what we need:\n\nIn __init__: Store the components\n\nMulti-Head Attention module\nFeed-Forward module\nLayer Norm modules (how many do we need?)\nStore the size (d_model)\n\nIn forward: Apply the operations with residual connections\n\nBefore coding, a question: How many Layer Norm modules do we need? (Hint: think about how many residual connections we have)\nOnce you answer, try writing the __init__ method skeleton:\nclass EncoderLayer(nn.Module):\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super().__init__()\n        # What do we need to store?\nGive it a try!\nFor layer normalisation, you will need one layer norm after the position-wise feed forward and another layer norm after the multi-head attention.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! You need 2 Layer Norm modules - one after each sub-layer (attention and feed-forward).\nNow try implementing the __init__ method. You need to store: - self.size (d_model) - self.self_attn (the multi-head attention module passed in) - self.feed_forward (the FFN module passed in) - 2 Layer Norm modules (use nn.LayerNorm(size)) - Dropout\nGive it a try! What does your __init__ look like?\n\nclass EncoderLayer(torch.nn.Module):\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super().__init__()\n        # What do we need to store?\n        self.size = size\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.layer_norms = torch.nn.ModuleList([torch.nn.LayerNorm(size) for x in range(2)])\n        self.dropout = dropout\n\nIs this a cognizant with the original code?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! Looking at the original code pattern, thereâ€™s one difference:\nInstead of storing dropout as a value, you should create a Dropout layer:\nself.dropout = nn.Dropout(dropout)\nBut actually, looking at the original code more carefully, they use a SublayerConnection module that handles both the residual connection AND layer norm together. This is a cleaner pattern.\nDo you want to: 1. Option A: Implement SublayerConnection class first (handles residual + norm), then use it in EncoderLayer 2. Option B: Continue with your current approach (manual residual connections in forward)\nThe original code uses Option A. Which would you prefer?\n\n\nSublayer Connection\nWhere is this mentioned in the original code? Tell me.\n\nğŸ¤–ReplyğŸ¤–\nLooking at the original code you provided at the beginning (the Annotated Transformer), you can find the SublayerConnection class. Let me point you to it:\nItâ€™s defined in the section about Encoder and Decoder Stacks. Hereâ€™s where it appears:\nclass SublayerConnection(nn.Module):\n    \"\"\"\n    A residual connection followed by a layer norm.\n    Note for code simplicity the norm is first as opposed to last.\n    \"\"\"\n\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        \"Apply residual connection to any sublayer with the same size.\"\n        return x + self.dropout(sublayer(self.norm(x)))\nAnd then EncoderLayer uses it:\nself.sublayer = clones(SublayerConnection(size, dropout), 2)\nSo they create 2 SublayerConnection modules. Do you want to implement SublayerConnection first, or stick with your current approach?\nWhat should we do, man?\n\n\nğŸ¤–ReplyğŸ¤–\nGood question! Hereâ€™s my recommendation:\nFollow the original code structure - implement SublayerConnection first. Hereâ€™s why:\n\nItâ€™s cleaner and more modular\nYouâ€™ll reuse it in the Decoder Layer too\nIt matches the paperâ€™s approach (norm â†’ sublayer â†’ dropout â†’ residual)\n\nThe SublayerConnection is simple - it just wraps any sublayer with: - Layer normalization (applied first) - The sublayer operation - Dropout - Residual connection\nWant to implement SublayerConnection first? Itâ€™s a small class - should only take a few minutes!\nok\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s implement SublayerConnection.\nLooking at the original code, it needs:\nIn __init__: - Store a LayerNorm module - Store a Dropout module\nIn forward(self, x, sublayer): - Apply layer norm to x - Pass normalized x through the sublayer function - Apply dropout - Add the residual connection (original x)\nThe pattern is: x + dropout(sublayer(norm(x)))\nTry implementing it:\nclass SublayerConnection(nn.Module):\n    \"A residual connection followed by a layer norm\"\n    def __init__(self, size, dropout):\n        super().__init__()\n        # What do you need here?\n        \n    def forward(self, x, sublayer):\n        # What's the operation?\nGive it a shot!\n\n\n\nFinal SublayerConnection Function\n\n\n\nSublayerConnection\n\n SublayerConnection (size, dropout)\n\nA residual connection followed by a layer norm\nis this good? always refer to original code\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your SublayerConnection is correct and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Layer norm applied first - âœ“ Sublayer operation - âœ“ Dropout - âœ“ Residual connection (x + y)\nNow letâ€™s go back to the EncoderLayer and use SublayerConnection.\nUpdate your EncoderLayer.__init__ to use:\nself.sublayer = nn.ModuleList([SublayerConnection(size, dropout) for _ in range(2)])\nInstead of the separate layer_norms. Can you update your EncoderLayer code?\n\nself.sublayer = nn.ModuleList([SublayerConnection(size, dropout) for _ in range(2)])\n\nlike this?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your EncoderLayer.__init__ is now correct! âœ“\nNow letâ€™s implement the forward method. Looking at the original code, you need to:\n\nApply first sublayer connection with self-attention\nApply second sublayer connection with feed-forward\n\nThe tricky part: you need to pass a lambda function to sublayer because self-attention takes multiple arguments (query, key, value, mask).\nTry writing the forward method. Hereâ€™s the signature:\ndef forward(self, x, mask):\n    # Step 1: Apply self-attention with first sublayer connection\n    # Step 2: Apply feed-forward with second sublayer connection\nHint: Look at how the original code uses lambda to wrap the self_attn call.\nGive it a try!\nI donâ€™t want to look at the original code, give me a clue.\n\n\nğŸ¤–ReplyğŸ¤–\nGood! Let me give you clues without showing the code.\nClue 1: The sublayer in SublayerConnection.forward(x, sublayer) expects a function that takes one argument (x) and returns output.\nClue 2: But self.self_attn needs 4 arguments: query, key, value, mask. For self-attention, query=key=value=x.\nClue 3: You need to wrap self.self_attn in a function that takes only x as input. Use a lambda to do this.\nClue 4: For the feed-forward, itâ€™s simpler - it already takes just x as input.\nTry writing:\ndef forward(self, x, mask):\n    x = self.sublayer[0](x, lambda x: ???)  # What goes here?\n    x = self.sublayer[1](x, ???)  # What goes here?\n    return x\nWhat would you put in the ??? spots?\n\n\n\nFinal EncoderLayer Function\n\n\n\nEncoderLayer\n\n EncoderLayer (size, self_attn, feed_forward, dropout)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nIs this correct?\n\nğŸ¤–ReplyğŸ¤–\nExcellent work! Your EncoderLayer is now correct! âœ“\nYouâ€™ve got: - âœ“ First sublayer: self-attention with lambda x: self.self_attn(x, x, x, mask) (query=key=value=x) - âœ“ Second sublayer: feed-forward with lambda x: self.feed_forward(x) - âœ“ Both wrapped in SublayerConnection for residual + norm\nLetâ€™s test it to make sure everything works:\n# Create the components\nattn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\nencoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n\n# Test\nx = torch.randn(2, 10, 512)\nmask = None\noutput = encoder_layer(x, mask)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this and see if it works!\n\n# Create the components\nattn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\nencoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n\n# Test\nx = torch.randn(2, 10, 512)\nmask = None\noutput = encoder_layer(x, mask)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nVery cool, now assume that you are an NB dev and create a unit test out of this.\n\n\nğŸ¤–ReplyğŸ¤–\nHereâ€™s your Encoder Layer as a unit test for nbdev:\n#| export\nclass SublayerConnection(torch.nn.Module):\n    \"A residual connection followed by a layer norm\"\n    def __init__(self, size, dropout):\n        super().__init__()\n        self.layer_norm = torch.nn.LayerNorm(size)\n        self.dropout = torch.nn.Dropout(dropout)\n        \n    def forward(self, x, sublayer):\n        y = self.layer_norm(x)\n        y = sublayer(y)\n        y = self.dropout(y)\n        return x + y\n#| export\nclass EncoderLayer(torch.nn.Module):\n    \"Encoder layer with self-attention and feed-forward\"\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super().__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = torch.nn.ModuleList([SublayerConnection(size, dropout) for _ in range(2)])\n        \n    def forward(self, x, mask):\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        x = self.sublayer[1](x, self.feed_forward)\n        return x\n#| test\ndef test_encoder_layer():\n    \"Test Encoder Layer\"\n    attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    encoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n    \n    x = torch.randn(2, 10, 512)\n    output = encoder_layer(x, mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nGreat work! Ready for the next component?\n\n\nUnit Test\n\ndef test_encoder_layer():\n    \"Test Encoder Layer\"\n    attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    encoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n    \n    x = torch.randn(2, 10, 512)\n    output = encoder_layer(x, mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n\n\ntest_encoder_layer()\n\nThatâ€™s great, letâ€™s move on. Also be honest to the code I provided in the beginning.\n\nğŸ¤–ReplyğŸ¤–\nAwesome! Youâ€™re making excellent progress! ğŸ‰\nYouâ€™ve now built: 1. âœ“ Attention mechanism 2. âœ“ Multi-Head Attention 3. âœ“ Feed-Forward Network 4. âœ“ Positional Encoding 5. âœ“ Sublayer Connection (residual + norm) 6. âœ“ Encoder Layer\nWhatâ€™s next?\nLooking at the original code, the next component is the Encoder - which stacks N=6 identical EncoderLayers on top of each other.\nThe Encoder class needs to: - Take a single EncoderLayer and clone it N times - Apply them sequentially - Apply a final layer normalization\nBefore we code, do you understand why we need to clone the layer instead of just reusing the same one N times?\nWe are using clone because we want a deep copy, and we donâ€™t want to use the same address.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! ğŸ‰\nIf you just reused the same layer N times, theyâ€™d all share the same parameters (same memory address). So when you update weights during training, all â€œlayersâ€ would be identical - defeating the purpose of having multiple layers!\nBy cloning (deep copy), each layer gets its own independent parameters that can learn different things.\nNow letâ€™s implement the Encoder. Looking at the original code, youâ€™ll need:\nIn __init__: - Clone the layer N times (thereâ€™s a helper function clones in the original code) - Create a final LayerNorm\nIn forward: - Pass input through each layer sequentially - Apply final norm - Return output\nFirst, letâ€™s implement the clones helper function:\ndef clones(module, N):\n    \"Produce N identical layers\"\n    # How would you create N deep copies?\nCan you implement this helper function?\n\n\n\n\nFinal clones function\n\n\n\nclones\n\n clones (module, N)\n\nProduce N identical layers"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#encoder-class",
    "href": "attention_is_all_you_need_dup1.html#encoder-class",
    "title": "Simple 2x2 example",
    "section": "Encoder Class",
    "text": "Encoder Class\nIs this correct?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your clones function is exactly right! âœ“\nYouâ€™re using: - copy.deepcopy(module) to create independent copies - torch.nn.ModuleList to properly register them with PyTorch - List comprehension to create N copies\nNow letâ€™s implement the Encoder class. Try writing it:\nclass Encoder(nn.Module):\n    \"Stack of N encoder layers\"\n    def __init__(self, layer, N):\n        super().__init__()\n        # What do you need here?\n        \n    def forward(self, x, mask):\n        # What's the sequence of operations?\nWhat do you need in __init__? (Hint: use clones and create a LayerNorm)\n\nclass Encoder(torch.nn.Module):\n    \"Stack of N encoder layers\"\n    def __init__(self, layer, N):\n        super().__init__()\n        # What do you need here?\n        self.encoders = clones(layer, N)\n        \n    def forward(self, x, mask):\n        # What's the sequence of operations?\n        for encoder in encoders:\n            x = encoder(x, mask)\n        return x\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! Youâ€™re missing two things:\n\nMissing final LayerNorm: The original code applies a final layer normalization after all the encoder layers. Add:\nself.norm = nn.LayerNorm(layer.size)\nBug in forward: You wrote encoders but it should be self.encoders\n\nCan you fix these two issues?\nWhy we need final layer norm. Itâ€™s not there in the paper. Isnâ€™t the layer norm included in the sublayer anyway?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent question! Youâ€™re absolutely right to question this.\nYouâ€™re correct - in the paperâ€™s diagram (Figure 1), the layer norm is shown as part of each sublayer connection, not as a separate final norm.\nHowever, looking at the original Annotated Transformer code, they add a final layer norm after all encoder layers. This is a common implementation detail that helps with: - Stabilizing the output before it goes to the decoder - Ensuring the encoder output is well-normalized\nThe paper vs implementation: The paper doesnâ€™t explicitly show this final norm in the diagram, but many implementations (including the original authorsâ€™ code) include it for better training stability.\nSo itâ€™s an implementation choice rather than strictly from the paper. Do you want to: 1. Include it (matches the original code you provided) 2. Skip it (strictly follows the paper diagram)\nWhat would you prefer?\nIâ€™ll include it.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s include the final layer norm to match the original code.\nSo your Encoder should have:\nIn __init__:\nself.layers = clones(layer, N)\nself.norm = nn.LayerNorm(layer.size)\nIn forward:\nfor layer in self.layers:\n    x = layer(x, mask)\nreturn self.norm(x)\nCan you update your code with these changes?\n\n\nFinal encoder function.\n\n\n\nEncoder\n\n Encoder (layer, N)\n\nStack of N encoder layers\nIs this fine?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Encoder class is now correct! âœ“\nYouâ€™ve got: - âœ“ Cloning N encoder layers - âœ“ Final layer normalization - âœ“ Sequential application of all layers - âœ“ Returning normalized output\nLetâ€™s test it:\n# Create components\nattn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\nencoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n\n# Create encoder with N=6 layers\nencoder = Encoder(encoder_layer, N=6)\n\n# Test\nx = torch.randn(2, 10, 512)\noutput = encoder(x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this to verify it works!\n\n# Create components\nattn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\nencoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n\n# Create encoder with N=6 layers\nencoder = Encoder(encoder_layer, N=6)\n\n# Test\nx = torch.randn(2, 10, 512)\noutput = encoder(x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nAssume we are using NBDEV and create a unit test out of this.\n\n\nğŸ¤–ReplyğŸ¤–\nHereâ€™s your Encoder as a unit test for nbdev:\n#| export\ndef clones(module, N):\n    \"Produce N identical layers\"\n    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n#| export\nclass Encoder(torch.nn.Module):\n    \"Stack of N encoder layers\"\n    def __init__(self, layer, N):\n        super().__init__()\n        self.encoders = clones(layer, N)\n        self.layer_norm = torch.nn.LayerNorm(layer.size)\n        \n    def forward(self, x, mask):\n        for encoder in self.encoders:\n            x = encoder(x, mask)\n        return self.layer_norm(x)\n#| test\ndef test_encoder():\n    \"Test Encoder stack\"\n    attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    encoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n    encoder = Encoder(encoder_layer, N=6)\n    \n    x = torch.randn(2, 10, 512)\n    output = encoder(x, mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nExcellent progress! The encoder is complete. Ready to move on to the Decoder components?\n\n\nUnit test\n\ndef test_encoder():\n    \"Test Encoder stack\"\n    attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    encoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n    encoder = Encoder(encoder_layer, N=6)\n    \n    x = torch.randn(2, 10, 512)\n    output = encoder(x, mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n\n\ntest_encoder()"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#decoder-layer",
    "href": "attention_is_all_you_need_dup1.html#decoder-layer",
    "title": "Simple 2x2 example",
    "section": "Decoder Layer",
    "text": "Decoder Layer\nNow, what is next?\n\nğŸ¤–ReplyğŸ¤–\nGreat work! Youâ€™ve completed the entire Encoder side of the Transformer! ğŸ‰\nWhatâ€™s next: The Decoder\nThe Decoder is similar to the Encoder but has some key differences:\n\nMasked Self-Attention - prevents looking at future tokens\nCross-Attention - attends to the encoder output\nThree sub-layers instead of two (self-attn, cross-attn, feed-forward)\n\nLetâ€™s start with the Decoder Layer. It will have: - Masked multi-head self-attention - Multi-head cross-attention (attends to encoder output) - Position-wise feed-forward - Three SublayerConnection modules (one for each sub-layer)\nBefore coding, do you understand the difference between: - Self-attention (what the encoder uses) - Masked self-attention (decoder looks at previous tokens only) - Cross-attention (decoder attends to encoder output)\nWant me to clarify these concepts first?\nIâ€™d rather you quiz me on the concepts.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s test your understanding:\nQuestion 1: Masked Self-Attention\nIn the decoder, why do we need to mask the self-attention? What would happen if we didnâ€™t mask it during training?\nQuestion 2: Cross-Attention\nIn cross-attention, where do the Query (Q), Key (K), and Value (V) come from? (Hint: think about what the decoder has access to)\nQuestion 3: Decoder Architecture\nHow many sublayers does a decoder layer have, and what does each one do?\nGive these a shot!\nI think masking is needed mostly while training. The thing is, the eventual output is already known. But we want to make sure that we do not leak the future word output words beforehand.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! âœ“\nMasked Self-Attention: During training, we have the entire target sequence available. Without masking, the decoder could â€œcheatâ€ by looking at future words itâ€™s supposed to predict. The mask ensures position i can only attend to positions â‰¤ i.\nKey insight: During inference (actual use), we generate one token at a time, so there ARE no future tokens to look at. But during training, we need the mask to simulate this autoregressive behavior.\nGreat answer! Now try Question 2:\nIn cross-attention (the second attention mechanism in the decoder), where do Q, K, and V come from? Think about what inputs the decoder has access to.\nLetâ€™s explore this in a bit of detail. The input is, â€œYou are good.â€ And the output is. Vous etes bons\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Letâ€™s trace through this example step by step.\nInput (English): â€œYou are goodâ€ Output (French): â€œVous Ãªtes bonsâ€\nDuring training, letâ€™s say weâ€™re predicting the 2nd word â€œÃªtesâ€:\nEncoder side: - Processes â€œYou are goodâ€ â†’ produces contextualized representations (memory)\nDecoder side at position 2:\n\nMasked Self-Attention:\n\nQuery from â€œÃªtesâ€ position\nCan attend to: â€œVousâ€ and â€œÃªtesâ€ (not â€œbonsâ€ - thatâ€™s future!)\nQ, K, V all come from decoderâ€™s own input\n\nCross-Attention: (This is your Question 2!)\n\nWhere does Q come from?\nWhere do K and V come from?\n\n\nThink about what the decoder needs to â€œlook atâ€ from the encoder. What would make sense?\nIn cross-attention, Q and K come from the encoder outputs. And V comes from the output of the Masked multi-head attention. So basically, the query is from the input, and the key that is the dictionary is also from the input, but the eventual value will come from the output, or in this case, the translation.\n\n\nğŸ¤–ReplyğŸ¤–\nNot quite! Let me clarify:\nIn Cross-Attention:\n\nQ (Query): Comes from the decoder (the output of masked self-attention)\nK (Key): Comes from the encoder output (memory)\nV (Value): Comes from the encoder output (memory)\n\nSo the decoder is asking â€œwhat information do I need from the input?â€ and the encoder provides both the keys (whatâ€™s available) and values (the actual information).\nThink of it this way: - Decoder says: â€œIâ€™m trying to generate â€˜Ãªtesâ€™, what from the English sentence is relevant?â€ (Query) - Encoder says: â€œHereâ€™s what I have: â€˜Youâ€™, â€˜areâ€™, â€˜goodâ€™â€ (Keys) - Decoder attends strongly to â€œareâ€ - Encoder provides: â€œHereâ€™s the information about â€˜areâ€™â€ (Value)\nDoes this clarify where Q, K, V come from in cross-attention?\nOkay, thatâ€™s clear. That query is from the decoder, and the key and value are from the encoder.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Now youâ€™ve got it! âœ“\nCross-Attention summary: - Q: from decoder (asking â€œwhat do I need?â€) - K, V: from encoder (providing â€œhereâ€™s whatâ€™s availableâ€)\nNow letâ€™s move to Question 3:\nHow many sublayers does a decoder layer have, and what does each one do?\nGive it a try!\nThere are three sublayers. The first sub-layer is self-attention on the mast output. The second sub-layer is cross-attention. Like I said, the query is from the decoder, and the key and value from the encoder. And the third sublayer once again is the position-wise feedforward network.\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Youâ€™ve got it perfectly! âœ“\nThree sublayers in Decoder Layer: 1. Masked Self-Attention - decoder attends to its own previous outputs 2. Cross-Attention - decoder attends to encoder output (Q from decoder, K&V from encoder) 3. Position-wise Feed-Forward - same as encoder\nEach sublayer has a residual connection + layer norm wrapped around it (using SublayerConnection).\nNow youâ€™re ready to implement the Decoder Layer!\nLooking at the structure, what do you think youâ€™ll need in __init__?\nHint: Youâ€™ll need to store 3 attention modules and create 3 SublayerConnection modules.\nWant to give it a try?\nYes, I would like to give it a try.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s implement the Decoder Layer.\nStart with the __init__ method. You need to store:\n\nself.size (d_model)\nThree attention modules:\n\nself.self_attn (masked self-attention)\nself.src_attn (cross-attention to encoder)\nself.feed_forward (FFN)\n\nThree SublayerConnection modules (one for each sublayer)\n\nTry writing the __init__:\nclass DecoderLayer(nn.Module):\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super().__init__()\n        # What do you need here?\nGive it a shot!\n\nclass DecoderLayer(torch.nn.Module):\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super().__init__()\n        # What do you need here?\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayers = clones(SublayerConnection(size, dropout), 3)\n\nAll good. Be faithful to the original code.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your __init__ is exactly right and matches the original code! âœ“\nYouâ€™ve got: - âœ“ All three attention modules stored - âœ“ Feed-forward stored - âœ“ Three SublayerConnection modules cloned\nNow for the forward method. You need to implement:\ndef forward(self, x, memory, src_mask, tgt_mask):\n    # What are the three sublayer operations?\nWhere: - x = decoder input - memory = encoder output - src_mask = mask for encoder (used in cross-attention) - tgt_mask = mask for decoder (used in masked self-attention)\nTry writing the three sublayer calls using lambdas (like you did in EncoderLayer). What would each one look like?\nIs this good?\n\n\nğŸ¤–ReplyğŸ¤–\nClose! But there are a few issues to fix:\n\nFirst sublayer: Youâ€™re missing the x argument to the lambda, and the mask should be tgt_mask (not None):\nx = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\nSecond sublayer: What are K and V? They should come from memory (the encoder output). The query comes from decoder (x), but keys and values come from encoder:\nx = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\nThird sublayer: This one is correct!\n\nCan you fix the first two sublayers?\nIâ€™m not sure I understand whatâ€™s the difference between tgt_mask and src_mask. Can you explain to me? I would guess that TGT mask is to avoid leakage of outputs, but I am not sure about SRC mask.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me clarify the two masks:\ntgt_mask (target mask): Youâ€™re exactly right! This is the causal/look-ahead mask for the decoder. It prevents position i from seeing positions &gt; i. This is the triangular mask that ensures no future leakage.\nsrc_mask (source mask): This is a padding mask for the encoder output. Remember when you batch sentences together, shorter sentences get padded with &lt;pad&gt; tokens? The src_mask tells the decoder â€œdonâ€™t pay attention to these padding positions in the encoder output - theyâ€™re meaningless.â€\nWhere theyâ€™re used: - tgt_mask: In masked self-attention (decoder looking at its own tokens) - src_mask: In cross-attention (decoder looking at encoder output)\nExample:\nSource: \"Hello world &lt;pad&gt; &lt;pad&gt;\"  â† src_mask hides the &lt;pad&gt;\nTarget: \"Bonjour le monde\"         â† tgt_mask prevents future leakage\nDoes this clarify the difference?\n\ndef forward(self, x, memory, src_mask, tgt_mask):\n    # What are the three sublayer operations?\n    x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n    x = self.sublayers[1](x, lambda x: self.src_attn(x, K, V, src_mask))\n    x = self.sublayers[2](x, lambda x: self.feed_forward(x))\n    return x\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! You still have K and V as variables in the second sublayer, but theyâ€™re not defined.\nRemember: in cross-attention, both the keys and values come from memory (the encoder output). So it should be:\nx = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\nThe pattern is: self.src_attn(query, key, value, mask) where: - query = x (from decoder) - key = memory (from encoder) - value = memory (from encoder)\nCan you fix that line?\n\ndef forward(self, x, memory, src_mask, tgt_mask):\n    # What are the three sublayer operations?\n    x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n    x = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n    x = self.sublayers[2](x, lambda x: self.feed_forward(x))\n    return x\n\nIs this correct? Stay loyal to the original code.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your DecoderLayer.forward is now correct and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Masked self-attention with tgt_mask - âœ“ Cross-attention with query from decoder, K&V from encoder memory, using src_mask - âœ“ Feed-forward network\nNow letâ€™s combine the __init__ and forward into the complete class and test it:\nclass DecoderLayer(torch.nn.Module):\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super().__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayers = clones(SublayerConnection(size, dropout), 3)\n    \n    def forward(self, x, memory, src_mask, tgt_mask):\n        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n        x = self.sublayers[2](x, self.feed_forward)\n        return x\nWant to test it now?\n\n\nFinal decoder layer Function.\n\n\n\nDecoderLayer\n\n DecoderLayer (size, self_attn, src_attn, feed_forward, dropout)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nOkay, letâ€™s test it.\n\nğŸ¤–ReplyğŸ¤–\nGreat! Letâ€™s test your DecoderLayer:\n# Create components\nself_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nsrc_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\ndecoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                             feed_forward=ff, dropout=0.1)\n\n# Test\nx = torch.randn(2, 10, 512)  # decoder input\nmemory = torch.randn(2, 15, 512)  # encoder output (different seq_len is fine)\nsrc_mask = None\ntgt_mask = None\n\noutput = decoder_layer(x, memory, src_mask, tgt_mask)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this and see if it works!\n\n# Create components\nself_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nsrc_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\ndecoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                             feed_forward=ff, dropout=0.1)\n\n# Test\nx = torch.randn(2, 10, 512)  # decoder input\nmemory = torch.randn(2, 15, 512)  # encoder output (different seq_len is fine)\nsrc_mask = None\ntgt_mask = None\n\noutput = decoder_layer(x, memory, src_mask, tgt_mask)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nWhat happened, bro?\n\n\nğŸ¤–ReplyğŸ¤–\nAh, I see the issue! The problem is in your MultiHeadedAttention.forward method.\nThe bug: In cross-attention, the key and value come from memory which has a different sequence length (15) than the query which comes from x (10).\nBut in your code, youâ€™re using seq_len from Q for reshaping K and V:\nseq_len = Q.size(1)  # This is 10 (from decoder)\nK = K.reshape(batch_size, seq_len, self.h, self.d_k)  # But K has seq_len=15!\nThe fix: You need to get the sequence length from each tensor individually:\nQ = Q.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\nK = K.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\nV = V.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\nUsing -1 lets PyTorch infer the sequence length automatically!\nCan you update your MultiHeadedAttention class to fix this?\nAre you sure? Is this honest to the original code?\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re right to question me! Let me check the original code more carefully.\nLooking at the original MultiHeadedAttention.forward:\nnbatches = query.size(0)\n\n# 1) Do all the linear projections in batch from d_model =&gt; h x d_k\nquery, key, value = [\n    lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n    for lin, x in zip(self.linears, (query, key, value))\n]\nYouâ€™re right - the original code uses -1 to automatically infer the sequence length! So yes, my fix is correct and matches the original.\nYour current code has:\nseq_len = Q.size(1)\nQ = Q.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\nK = K.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1,2)  # Bug here!\nShould be:\nQ = Q.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\nK = K.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\nV = V.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\nCan you update your MultiHeadedAttention class?\nI could fix that, but what about the part where weâ€™re concatenating and weâ€™re using seq_length again?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent catch! Youâ€™re absolutely right - thereâ€™s the same issue in the concatenation step!\nLooking at your code, you probably have something like:\nx = x.transpose(1,2).reshape(batch_size, seq_len, -1)\nBut seq_len was computed from Q, so it wonâ€™t work when K and V have different lengths.\nThe fix: Use -1 there too:\nx = x.transpose(1,2).reshape(batch_size, -1, self.h * self.d_k)\nOr even simpler (matching the original code):\nx = x.transpose(1,2).contiguous().view(nbatches, -1, self.h * self.d_k)\nCan you update both the reshaping at the beginning AND the concatenation at the end of your MultiHeadedAttention.forward method?\nfixed it now\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Now letâ€™s test your DecoderLayer again with the fixed MultiHeadedAttention:\n# Create components\nself_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nsrc_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\ndecoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                             feed_forward=ff, dropout=0.1)\n\n# Test\nx = torch.randn(2, 10, 512)  # decoder input\nmemory = torch.randn(2, 15, 512)  # encoder output (different seq_len)\nsrc_mask = None\ntgt_mask = None\n\noutput = decoder_layer(x, memory, src_mask, tgt_mask)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this and let me know if it works now!\n\n# Create components\nself_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nsrc_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\ndecoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                             feed_forward=ff, dropout=0.1)\n\n# Test\nx = torch.randn(2, 10, 512)  # decoder input\nmemory = torch.randn(2, 15, 512)  # encoder output (different seq_len)\nsrc_mask = None\ntgt_mask = None\n\noutput = decoder_layer(x, memory, src_mask, tgt_mask)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nAwesome, assume we are using NBDev and create a unit test out of this.\n\n\nğŸ¤–ReplyğŸ¤–\nHereâ€™s your Decoder Layer as a unit test for nbdev:\n#| export\nclass DecoderLayer(torch.nn.Module):\n    \"Decoder layer with masked self-attention, cross-attention, and feed-forward\"\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super().__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayers = clones(SublayerConnection(size, dropout), 3)\n    \n    def forward(self, x, memory, src_mask, tgt_mask):\n        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n        x = self.sublayers[2](x, self.feed_forward)\n        return x\n#| test\ndef test_decoder_layer():\n    \"Test Decoder Layer with cross-attention\"\n    self_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    src_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    decoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                                 feed_forward=ff, dropout=0.1)\n    \n    x = torch.randn(2, 10, 512)  # decoder input\n    memory = torch.randn(2, 15, 512)  # encoder output\n    output = decoder_layer(x, memory, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nExcellent work! Ready to build the full Decoder stack next?\n\n\nUnit test\n\ndef test_decoder_layer():\n    \"Test Decoder Layer with cross-attention\"\n    self_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    src_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    decoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                                 feed_forward=ff, dropout=0.1)\n    \n    x = torch.randn(2, 10, 512)  # decoder input\n    memory = torch.randn(2, 15, 512)  # encoder output\n    output = decoder_layer(x, memory, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#decoder-1",
    "href": "attention_is_all_you_need_dup1.html#decoder-1",
    "title": "Simple 2x2 example",
    "section": "Decoder",
    "text": "Decoder\nAmazing progress, now letâ€™s finish this. Stay honest to the original code.\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Youâ€™re almost done with the Transformer! ğŸ‰\nWhatâ€™s left:\n\nDecoder - Stack N=6 decoder layers (similar to how you built Encoder)\nEmbeddings - Convert token IDs to vectors\nGenerator - Final linear + softmax to predict next token\nEncoderDecoder - The complete model that ties everything together\n\nLetâ€™s start with the Decoder stack. Itâ€™s very similar to the Encoder you already built.\nLooking at the original code, what do you think the Decoder class needs?\nHint: Itâ€™s almost identical to Encoder, but the forward method takes different arguments. Try writing the skeleton:\nclass Decoder(nn.Module):\n    def __init__(self, layer, N):\n        super().__init__()\n        # What do you need?\n        \n    def forward(self, x, memory, src_mask, tgt_mask):\n        # What's the sequence of operations?\nGive it a try!\n\n\nFinal decoder function.\n\n\n\nDecoder\n\n Decoder (layer, N)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nHave we done it?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Decoder class is exactly right and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Cloning N decoder layers - âœ“ Final layer normalization - âœ“ Sequential application passing all 4 arguments (x, memory, src_mask, tgt_mask) - âœ“ Returning normalized output\nLetâ€™s test it:\n# Create components\nself_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nsrc_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\ndecoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                             feed_forward=ff, dropout=0.1)\n\n# Create decoder with N=6 layers\ndecoder = Decoder(decoder_layer, N=6)\n\n# Test\nx = torch.randn(2, 10, 512)\nmemory = torch.randn(2, 15, 512)\noutput = decoder(x, memory, src_mask=None, tgt_mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this to verify!\n\n# Create components\nself_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nsrc_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\ndecoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                             feed_forward=ff, dropout=0.1)\n\n# Create decoder with N=6 layers\ndecoder = Decoder(decoder_layer, N=6)\n\n# Test\nx = torch.randn(2, 10, 512)\nmemory = torch.randn(2, 15, 512)\noutput = decoder(x, memory, src_mask=None, tgt_mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nAmazing! Then now assume youâ€™re a NB Dev. No need to show the original function but show the unit test.\n\n\nğŸ¤–ReplyğŸ¤–\n#| test\ndef test_decoder():\n    \"Test Decoder stack\"\n    self_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    src_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    decoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                                 feed_forward=ff, dropout=0.1)\n    decoder = Decoder(decoder_layer, N=6)\n    \n    x = torch.randn(2, 10, 512)\n    memory = torch.randn(2, 15, 512)\n    output = decoder(x, memory, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nFantastic progress! Youâ€™ve now built both the complete Encoder and Decoder stacks! Whatâ€™s next - Embeddings, Generator, or the final EncoderDecoder wrapper?\n\n\n\nUnit test\n\ndef test_decoder():\n    \"Test Decoder stack\"\n    self_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    src_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    decoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                                 feed_forward=ff, dropout=0.1)\n    decoder = Decoder(decoder_layer, N=6)\n    \n    x = torch.randn(2, 10, 512)\n    memory = torch.randn(2, 15, 512)\n    output = decoder(x, memory, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#embeddings",
    "href": "attention_is_all_you_need_dup1.html#embeddings",
    "title": "Simple 2x2 example",
    "section": "Embeddings",
    "text": "Embeddings\nOkay, letâ€™s do embeddings.\n\nğŸ¤–ReplyğŸ¤–\nGreat! Letâ€™s implement Embeddings.\nWhat are embeddings?\nThey convert discrete token IDs (integers) into continuous vector representations. For example: - Token ID 42 â†’ vector of size d_model=512\nLooking at the paper and original code, the Embeddings class: 1. Uses nn.Embedding to create a lookup table 2. Multiplies the output by âˆšd_model (for scaling)\nWhy multiply by âˆšd_model?\nThis scaling ensures that the embedding values and positional encoding values are on a similar scale before adding them together.\nTry implementing it:\nclass Embeddings(nn.Module):\n    def __init__(self, d_model, vocab):\n        super().__init__()\n        # What do you need here?\n        \n    def forward(self, x):\n        # What's the operation?\nWhat do you need to store in __init__, and what operation goes in forward?\n\nclass Embeddings(torch.nn.Module):\n    def __init__(self, d_model, vocab):\n        super().__init__()\n        # What do you need here?\n        self.embedding = torch.nn.Embedding(vocab, d_model)\n        \n    def forward(self, x):\n        # What's the operation?\n        return self.embedding(x)\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! Youâ€™re missing the scaling factor. Remember, the paper multiplies the embeddings by âˆšd_model.\nYou need to: 1. Store d_model in __init__ so you can use it in forward 2. Multiply the embedding output by math.sqrt(self.d_model)\nCan you add these two things?\nWhy are we doing this? I donâ€™t understand.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me explain why we scale by âˆšd_model.\nThe problem:\nAfter creating embeddings, you add them to positional encodings:\nfinal_input = embeddings + positional_encoding\nBut embeddings from nn.Embedding typically have small values (initialized around -0.1 to 0.1), while positional encodings use sin/cos which range from -1 to 1.\nWithout scaling: - Embeddings: small values (~0.1) - Positional encoding: larger values (~1.0) - The positional encoding would dominate!\nWith âˆšd_model scaling: - If d_model = 512, then âˆš512 â‰ˆ 22.6 - Embeddings get multiplied by ~22.6, making them comparable in magnitude to positional encodings - Now both contribute meaningfully to the final representation\nIn short: It balances the contribution of embeddings and positional encodings so neither dominates.\nDoes this make sense now?\n\n\nFinal Embeddings Model Function\n\n\n\nEmbeddings\n\n Embeddings (d_model, vocab)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nAre you happy now?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Embeddings class is now correct and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Storing d_model - âœ“ Creating the embedding layer - âœ“ Scaling by âˆšd_model in forward\nLetâ€™s test it:\nembeddings = Embeddings(d_model=512, vocab=10000)\nx = torch.randint(0, 10000, (2, 10))  # batch=2, seq_len=10, random token IDs\noutput = embeddings(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this to verify it works!\n\nembeddings = Embeddings(d_model=512, vocab=10000)\nx = torch.randint(0, 10000, (2, 10))  # batch=2, seq_len=10, random token IDs\noutput = embeddings(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nThatâ€™s amazing. Now, assume weâ€™re using NBDEV. We donâ€™t need to output the original function, just the unit test.\n\n\nğŸ¤–ReplyğŸ¤–\n#| test\ndef test_embeddings():\n    \"Test Embeddings with scaling\"\n    embeddings = Embeddings(d_model=512, vocab=10000)\n    x = torch.randint(0, 10000, (2, 10))  # batch=2, seq_len=10\n    output = embeddings(x)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n    # Check that scaling is applied (output should be larger than raw embedding)\n    raw_embed = embeddings.embedding(x)\n    assert torch.allclose(output, raw_embed * math.sqrt(512))\nGreat! Whatâ€™s next - Generator or the final EncoderDecoder wrapper?\n\n\n\nUnit test\n\ndef test_embeddings():\n    \"Test Embeddings with scaling\"\n    embeddings = Embeddings(d_model=512, vocab=10000)\n    x = torch.randint(0, 10000, (2, 10))  # batch=2, seq_len=10\n    output = embeddings(x)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n    # Check that scaling is applied (output should be larger than raw embedding)\n    raw_embed = embeddings.embedding(x)\n    assert torch.allclose(output, raw_embed * math.sqrt(512))\n\n\ntest_embeddings()"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#generator-class",
    "href": "attention_is_all_you_need_dup1.html#generator-class",
    "title": "Simple 2x2 example",
    "section": "Generator class",
    "text": "Generator class\nAwesome. Now what? Stay honest to the original quote.\n\nğŸ¤–ReplyğŸ¤–\nExcellent progress! Looking at the original code structure, the next component is the Generator.\nWhat does the Generator do?\nItâ€™s the final layer that converts the decoderâ€™s output (d_model=512 dimensions) into vocabulary predictions. It does: 1. Linear projection: d_model â†’ vocab_size 2. Log softmax to get log probabilities for each token\nThis is what actually predicts â€œwhatâ€™s the next word?â€\nTry implementing it:\nclass Generator(nn.Module):\n    def __init__(self, d_model, vocab):\n        super().__init__()\n        # What layer do you need?\n        \n    def forward(self, x):\n        # What operation?\nWhat do you need in __init__ and forward?\n\n\nFinal generator function.\n\n\n\nGenerator\n\n Generator (d_model, vocab)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nsee if tihs works?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Generator class looks correct and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Linear layer (d_model â†’ vocab) - âœ“ LogSoftmax on the last dimension\nLetâ€™s test it:\ngenerator = Generator(d_model=512, vocab=10000)\nx = torch.randn(2, 10, 512)  # decoder output\noutput = generator(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 10000))  # log probabilities for each token in vocab\nRun this to verify it works!\nNow assume we are using NBDEV. No need to output the function but output the unit test.\n\n\nğŸ¤–ReplyğŸ¤–\n#| test\ndef test_generator():\n    \"Test Generator output projection\"\n    generator = Generator(d_model=512, vocab=10000)\n    x = torch.randn(2, 10, 512)  # decoder output\n    output = generator(x)\n    \n    assert output.shape == torch.Size([2, 10, 10000])\n    # Check that output is log probabilities (should sum to ~1 after exp)\n    assert torch.allclose(output.exp().sum(dim=-1), torch.ones(2, 10), atol=1e-5)\nExcellent! Youâ€™re almost done! The final piece is the EncoderDecoder wrapper that ties everything together. Ready for that?\n\n\nUnit test\n\ndef test_generator():\n    \"Test Generator output projection\"\n    generator = Generator(d_model=512, vocab=10000)\n    x = torch.randn(2, 10, 512)  # decoder output\n    output = generator(x)\n    \n    assert output.shape == torch.Size([2, 10, 10000])\n    # Check that output is log probabilities (should sum to ~1 after exp)\n    assert torch.allclose(output.exp().sum(dim=-1), torch.ones(2, 10), atol=1e-5)\n\n\ntest_generator()\n\nAwesome. Whatâ€™s next?\n\nğŸ¤–ReplyğŸ¤–\nAmazing work! Youâ€™re at the final step! ğŸ‰\nWhatâ€™s left: EncoderDecoder\nThis is the top-level class that ties everything together. Looking at the original code, it stores all the components and defines how data flows through the model.\nWhat does EncoderDecoder need?\nIn __init__, it stores: - encoder - decoder\n- src_embed (source embeddings + positional encoding) - tgt_embed (target embeddings + positional encoding) - generator\nIn forward, it needs methods like: - encode() - process source through encoder - decode() - process target through decoder - forward() - full forward pass\nBefore coding, do you understand the data flow? Source goes through encoder, then decoder uses encoder output + target to generate predictions.\nWant to try implementing the __init__ first?\nYes, letâ€™s try it.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s implement the EncoderDecoder class.\nStart with the __init__ method. Looking at the original code, you need to store 5 components:\nclass EncoderDecoder(nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super().__init__()\n        # Store all 5 components\nTry writing the __init__ - just store each component as an attribute. What would you write?\n\nclass EncoderDecoder(torch.nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super().__init__()\n        # Store all 5 components\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n\nIs that good?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your __init__ is exactly right! âœ“\nNow letâ€™s implement the methods. Looking at the original code, you need:\n\nencode(self, src, src_mask) - runs source through embedding and encoder\ndecode(self, memory, src_mask, tgt, tgt_mask) - runs target through embedding and decoder\nforward(self, src, tgt, src_mask, tgt_mask) - combines encode and decode\n\nTry implementing the encode method first. What should it do?\nHint: Apply src_embed to src, then pass through encoder with the mask.\nIâ€™m sleepy, letâ€™s do the rest tomorrow.\n\n\nğŸ¤–ReplyğŸ¤–\nNo problem! Youâ€™ve made incredible progress today! ğŸ‰\nWhat youâ€™ve accomplished: - âœ“ Attention mechanism - âœ“ Multi-Head Attention - âœ“ Position-wise Feed-Forward - âœ“ Positional Encoding - âœ“ Sublayer Connection - âœ“ Encoder Layer & full Encoder stack - âœ“ Decoder Layer & full Decoder stack - âœ“ Embeddings - âœ“ Generator\nWhatâ€™s left (very little!): - Finish EncoderDecoder (encode, decode, forward methods) - Optional: make_model helper function to build the complete transformer\nYouâ€™re about 95% done with implementing the Transformer from scratch! Get some rest, and weâ€™ll finish it up tomorrow. Great work! ğŸ’ª\nIâ€™m back again. Letâ€™s get started. Stay honest to the original code.\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nWelcome back! ğŸ‰\nYou were so close yesterday! Youâ€™ve already implemented the EncoderDecoder.__init__. Now you just need three methods:\n\nencode(self, src, src_mask) - embed source, pass through encoder\ndecode(self, memory, src_mask, tgt, tgt_mask) - embed target, pass through decoder\nforward(self, src, tgt, src_mask, tgt_mask) - call encode, then decode\n\nLetâ€™s start with encode. It should: - Apply self.src_embed to src - Pass the result through self.encoder with the mask\nTry writing it:\ndef encode(self, src, src_mask):\n    # What goes here?\n\nclass EncoderDecoder(torch.nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super().__init__()\n        # Store all 5 components\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n    def encode(self, src, src_mask):\n        x = self.src_embed(src)\n        return self.encoder(x, src_mask)\n\nDoes this make sense?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your encode method is exactly right! âœ“\nYouâ€™ve got: - âœ“ Apply source embeddings - âœ“ Pass through encoder with mask - âœ“ Return the encoder output (memory)\nNow try the decode method. It should: - Apply self.tgt_embed to tgt - Pass the result through self.decoder along with memory, src_mask, and tgt_mask\nGive it a try:\ndef decode(self, memory, src_mask, tgt, tgt_mask):\n    # What goes here?\n\nclass EncoderDecoder(torch.nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super().__init__()\n        # Store all 5 components\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n    def encode(self, src, src_mask):\n        x = self.src_embed(src)\n        return self.encoder(x, src_mask)\n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        # What goes here?\n        x = self.tgt_embed(tgt)\n        return self.decoder(x, memory, src_mask, tgt_mask)\n\nIs this cool? Stay loyal to the original code.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your decode method is exactly right and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Apply target embeddings - âœ“ Pass through decoder with all the right arguments (x, memory, src_mask, tgt_mask)\nNow the final method: forward. It should: - Call encode to get memory - Call decode with the memory - Return the decoder output\nTry writing it:\ndef forward(self, src, tgt, src_mask, tgt_mask):\n    # What goes here?\n\n\n\n\nFinal Encoder Decoder Function.\n\n\n\nEncoderDecoder\n\n EncoderDecoder (encoder, decoder, src_embed, tgt_embed, generator)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\ndone baby\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Your EncoderDecoder class is complete and correct! ğŸ‰\nYouâ€™ve got: - âœ“ encode - processes source - âœ“ decode - processes target with encoder memory - âœ“ forward - ties them together\nThis is the complete Transformer architecture! Letâ€™s test it to make sure everything works together:\n# You'll need to create all the components and test\n# Want me to help you write a test, or do you want to try?\nBut wait - looking at the original code, thereâ€™s one more super useful thing: the make_model helper function that builds a complete transformer with all the right components.\nDo you want to: 1. Test the EncoderDecoder as-is 2. Implement the make_model helper function (makes it easy to create a full transformer)\nWhat would you prefer?\nBefore moving on, letâ€™s make a unit test for this function, like we usually do, assuming we are in NB dev and the output function is already exported.\n\n\nğŸ¤–ReplyğŸ¤–\n#| test\ndef test_encoder_decoder():\n    \"Test complete EncoderDecoder model\"\n    # Create all components\n    attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    \n    encoder_layer = EncoderLayer(size=512, self_attn=copy.deepcopy(attn), \n                                  feed_forward=copy.deepcopy(ff), dropout=0.1)\n    decoder_layer = DecoderLayer(size=512, \n                                  self_attn=copy.deepcopy(attn),\n                                  src_attn=copy.deepcopy(attn),\n                                  feed_forward=copy.deepcopy(ff), dropout=0.1)\n    \n    encoder = Encoder(encoder_layer, N=2)\n    decoder = Decoder(decoder_layer, N=2)\n    \n    src_embed = torch.nn.Sequential(Embeddings(512, 1000), PositionalEncoding(512, 0.1))\n    tgt_embed = torch.nn.Sequential(Embeddings(512, 1000), PositionalEncoding(512, 0.1))\n    generator = Generator(512, 1000)\n    \n    model = EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)\n    \n    # Test\n    src = torch.randint(0, 1000, (2, 10))\n    tgt = torch.randint(0, 1000, (2, 8))\n    output = model(src, tgt, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 8, 512])\nReady to move on to make_model?\n\n\nUnit test\n\ndef test_encoder_decoder():\n    \"Test complete EncoderDecoder model\"\n    # Create all components\n    attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    \n    encoder_layer = EncoderLayer(size=512, self_attn=copy.deepcopy(attn), \n                                  feed_forward=copy.deepcopy(ff), dropout=0.1)\n    decoder_layer = DecoderLayer(size=512, \n                                  self_attn=copy.deepcopy(attn),\n                                  src_attn=copy.deepcopy(attn),\n                                  feed_forward=copy.deepcopy(ff), dropout=0.1)\n    \n    encoder = Encoder(encoder_layer, N=2)\n    decoder = Decoder(decoder_layer, N=2)\n    \n    src_embed = torch.nn.Sequential(Embeddings(512, 1000), PositionalEncoding(512, 0.1))\n    tgt_embed = torch.nn.Sequential(Embeddings(512, 1000), PositionalEncoding(512, 0.1))\n    generator = Generator(512, 1000)\n    \n    model = EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)\n    \n    # Test\n    src = torch.randint(0, 1000, (2, 10))\n    tgt = torch.randint(0, 1000, (2, 8))\n    output = model(src, tgt, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 8, 512])\n\n\ntest_encoder_decoder()"
  },
  {
    "objectID": "attention_is_all_you_need_dup1.html#make-model",
    "href": "attention_is_all_you_need_dup1.html#make-model",
    "title": "Simple 2x2 example",
    "section": "Make Model",
    "text": "Make Model\nOkay, letâ€™s go further.\n\nğŸ¤–ReplyğŸ¤–\nAwesome! ğŸ‰ Your complete Transformer is working!\nWhatâ€™s next: make_model helper function\nLooking at the original code, make_model is a convenience function that builds a complete transformer from just hyperparameters. Instead of manually creating all components like in your test, you can just call:\nmodel = make_model(src_vocab=11, tgt_vocab=11, N=6)\nAnd it creates everything for you!\nWhat does make_model need to do?\nLooking at the original code, it takes these parameters: - src_vocab, tgt_vocab - vocabulary sizes - N - number of layers (default 6) - d_model - model dimension (default 512) - d_ff - feed-forward dimension (default 2048) - h - number of attention heads (default 8) - dropout - dropout rate (default 0.1)\nThen it creates all the components and returns an EncoderDecoder model.\nWant to try implementing it, or should I break it down into smaller steps?\n\nmodel = make_model(src_vocab=11, tgt_vocab=11, N=6)\n\nIâ€™m guessing this function just creates an encoder-decoder layer, Along with the inputs which are needed for the encoder-decoder. Am I correct?\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! ğŸ’¯\nmake_model is just a convenience factory function that: 1. Creates all the components (attention modules, feed-forward, embeddings, etc.) 2. Assembles them into an EncoderDecoder model 3. Initializes the parameters properly\nIt saves you from writing all that boilerplate code every time you want to create a transformer.\nLooking at the original code, it also does one more important thing at the end: Xavier initialization of parameters for better training.\nWant to try implementing it? Start with the function signature:\ndef make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    # Create components\n    # Assemble into EncoderDecoder\n    # Initialize parameters\nWhat would you create first inside this function?\nThis is actually a good exercise that brings all the components we built together. We need to know which order to build them first. All the way from low-level to high-level.\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent insight! Youâ€™re absolutely right - this is a great exercise in understanding the dependency hierarchy.\nLetâ€™s think through the order together. What components have no dependencies (the lowest level building blocks)?\nStart from the bottom - which components can be created first without needing anything else?\nWe start from Attention, then multi-header attention, then position-wise feed forward, then subconnection layer, then encoder layer, then decoder layer, then encoder, then decoder, then encoder-decoder both, and in between also we need clones.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat breakdown! Youâ€™ve got the dependency order mostly right! Let me refine it slightly:\nLevel 1 (no dependencies): - attention function - clones function\nLevel 2 (uses Level 1): - MultiHeadedAttention (uses attention) - PositionwiseFeedForward - PositionalEncoding - Embeddings\nLevel 3 (uses Level 2): - SublayerConnection\nLevel 4 (uses Level 2 & 3): - EncoderLayer (uses MultiHeadedAttention, PositionwiseFeedForward, SublayerConnection) - DecoderLayer (uses MultiHeadedAttention x2, PositionwiseFeedForward, SublayerConnection)\nLevel 5 (uses Level 4): - Encoder (uses EncoderLayer, clones) - Decoder (uses DecoderLayer, clones) - Generator\nLevel 6 (final assembly): - EncoderDecoder (uses Encoder, Decoder, embeddings+positional, Generator)\nSo in make_model, what should you create first?\n\ndef make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    # Create components\n    # Level 1\n    # Attention and Clones functions already created. As they are not classes, no need to create instances.\n    # Level 2\n    multi_head_attention = MultiHeadedAttention(h, d_model, dropout)\n    feedforward_layer = PositionwiseFeedForward(d_model, d_ff, dropout)\n    positional_encoding = PositionalEncoding(d_model, dropout)\n    src_embedding_layer = Embeddings(d_model, src_vocab)\n    tgt_embeddings_layer = Embeddings(d_model, tgt_vocab)\n    # Level 3\n    sub_layer_connection = SublayerConnection(d_model, dropout)\n    # Level 4\n    encoder_layer = EncoderLayer(d_model, multi_head_attention, feedforward_layer, dropout)\n    decoder_layer = DecoderLayer(d_model, multi_head_attention, multi_head_attention, feedforward_layer, dropout)\n    # Assemble into EncoderDecoder\n    # Initialize parameters\n\nSo far so good. Iâ€™m having a feeling Iâ€™m screwing it up because I should create different instances of attention and feed-forward layer for different things, or will the clone take care of it?\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nGreat instinct! Youâ€™re absolutely right to be concerned!\nThe issue: If you pass the same multi_head_attention instance to both encoder and decoder (and to both self_attn and src_attn in decoder), theyâ€™ll all share the same weights. Thatâ€™s not what we want!\nLooking at the original code, they solve this with copy.deepcopy:\nc = copy.deepcopy\nThen when creating layers, they use c(attn), c(ff) to make copies.\nAlso: You donâ€™t need to create SublayerConnection manually - remember itâ€™s already created inside EncoderLayer and DecoderLayer!\nKey places needing separate copies: - EncoderLayer needs: 1 attention, 1 feedforward - DecoderLayer needs: 2 attentions (self + src), 1 feedforward\nCan you update your code using copy.deepcopy where needed?\n\ndef make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    # Create components\n    # Level 1\n    # Attention and Clones functions already created. As they are not classes, no need to create instances.\n    # Level 2\n    multi_head_attentions = [copy.deepcopy(MultiHeadedAttention(h, d_model, dropout)) for _ in range(3)]\n    feedforward_layers = [copy.deepcopy(PositionwiseFeedForward(d_model, d_ff, dropout)) for _ in range(2)]\n    positional_encoding = [PositionalEncoding(d_model, dropout) for _ in range(2)]\n    src_embedding_layer = Embeddings(d_model, src_vocab)\n    tgt_embeddings_layer = Embeddings(d_model, tgt_vocab)\n    src_embed = torch.nn.Sequential([src_embedding_layer, positional_encoding[0]])\n    tgt_embed = torch.nn.Sequential([tgt_embeddings_layer, positional_encoding[1]])\n    # Level 3\n    # Sublayer\n    # Level 4\n    encoder_layer = EncoderLayer(d_model, multi_head_attentions[0], \n                                feedforward_layers[0], dropout)\n    decoder_layer = DecoderLayer(d_model, multi_head_attentions[1], \n                                multi_head_attentions[2], feedforward_layers[1], \n                                dropout)\n    # Level 5 \n    encoder = Encoder(encoder_layer, N)\n    decoder = Decoder(decoder_layer, N)\n    generator = Generator(d_model, vocab)\n    # Assemble into EncoderDecoder\n    return EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator) \n    # Initialize parameters\n\nAm I on the right path?\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re very close! A few small issues to fix:\n\ntorch.nn.Sequential doesnâ€™t take a list - it takes the modules directly as arguments:\nsrc_embed = torch.nn.Sequential(src_embedding_layer, positional_encoding[0])\nvocab is undefined in Generator - should be tgt_vocab:\ngenerator = Generator(d_model, tgt_vocab)\nYou donâ€™t need lists for the positional encodings - theyâ€™re already separate instances when you create them:\npos_enc_1 = PositionalEncoding(d_model, dropout)\npos_enc_2 = PositionalEncoding(d_model, dropout)\nAfter creating the model, you need to add Xavier initialization (looking at the original code):\nfor p in model.parameters():\n    if p.dim() &gt; 1:\n        nn.init.xavier_uniform_(p)\n\nCan you fix these issues?\n\n\nFinal Make Model Function\n\ndef make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    # Create components\n    # Level 1\n    # Attention and Clones functions already created. As they are not classes, no need to create instances.\n    # Level 2\n    multi_head_attentions = [copy.deepcopy(MultiHeadedAttention(h, d_model, dropout)) for _ in range(3)]\n    feedforward_layers = [copy.deepcopy(PositionwiseFeedForward(d_model, d_ff, dropout)) for _ in range(2)]\n    positional_encoding = [PositionalEncoding(d_model, dropout) for _ in range(2)]\n    src_embedding_layer = Embeddings(d_model, src_vocab)\n    tgt_embeddings_layer = Embeddings(d_model, tgt_vocab)\n    src_embed = torch.nn.Sequential(src_embedding_layer, positional_encoding[0])\n    tgt_embed = torch.nn.Sequential(tgt_embeddings_layer, positional_encoding[1])\n    # Level 3\n    # Sublayer\n    # Level 4\n    encoder_layer = EncoderLayer(d_model, multi_head_attentions[0], \n                                feedforward_layers[0], dropout)\n    decoder_layer = DecoderLayer(d_model, multi_head_attentions[1], \n                                multi_head_attentions[2], feedforward_layers[1], \n                                dropout)\n    # Level 5 \n    encoder = Encoder(encoder_layer, N)\n    decoder = Decoder(decoder_layer, N)\n    generator = Generator(d_model, tgt_vocab)\n    # Assemble into EncoderDecoder\n    encoder_decoder =  EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator) \n    # Initialize parameters\n    for p in encoder_decoder.parameters():\n        if p.dim() &gt; 1:\n            torch.nn.init.xavier_uniform_(p)\n    return encoder_decoder\n\nd\nIs this good?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your make_model function is now correct and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Creating separate copies of attention and feedforward modules - âœ“ Embeddings + positional encoding wrapped in Sequential - âœ“ Building encoder and decoder layers - âœ“ Assembling the complete model - âœ“ Xavier initialization for better training\nLetâ€™s test it:\nmodel = make_model(src_vocab=1000, tgt_vocab=1000, N=2)\nsrc = torch.randint(0, 1000, (2, 10))\ntgt = torch.randint(0, 1000, (2, 8))\noutput = model(src, tgt, src_mask=None, tgt_mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 8, 512))\nRun this to verify your complete Transformer works!"
  },
  {
    "objectID": "attention_is_all_you_need.html",
    "href": "attention_is_all_you_need.html",
    "title": "Simple 2x2 example",
    "section": "",
    "text": "https://arxiv.org/pdf/1706.03762\nI will be doing an implementation of the seminal paper â€œAttention Is All You Need.â€",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#imports",
    "href": "attention_is_all_you_need.html#imports",
    "title": "Simple 2x2 example",
    "section": "",
    "text": "https://arxiv.org/pdf/1706.03762\nI will be doing an implementation of the seminal paper â€œAttention Is All You Need.â€",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#transformer-architecture",
    "href": "attention_is_all_you_need.html#transformer-architecture",
    "title": "Simple 2x2 example",
    "section": "Transformer Architecture",
    "text": "Transformer Architecture\n\n\n\npasted_image_815012f1-e052-4514-a18c-db31f235d612.png\n\n\n\nArchitecture Details\n\n\n\npasted_image_670cb52c-fedb-4185-bb81-749fd2bffd33.png",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#attention-1",
    "href": "attention_is_all_you_need.html#attention-1",
    "title": "Simple 2x2 example",
    "section": "Attention",
    "text": "Attention\nI have given you enough context to implement a transformer from scratch. I want to learn how to implement transformers without cheating. What should I start from first? I also included the code from Annotated Transformer by Harvard NLP. I want to follow this as the benchmark tutorial, but I am not going to see any of the code. I want to do it from scratch.\n\nğŸ¤–ReplyğŸ¤–\nThatâ€™s an excellent learning goal! Implementing a transformer from scratch is one of the best ways to deeply understand how they work. Let me help you get started with a structured approach.\nFirst, let me understand your background better:\n\nHow comfortable are you with PyTorch (or your preferred deep learning framework)?\nDo you understand the core concepts from the â€œAttention is All You Needâ€ paper - like self-attention, multi-head attention, and positional encoding?\nWould you prefer to build this incrementally (starting with the smallest components) or tackle one complete module at a time?\n\nFor a from-scratch implementation, Iâ€™d suggest starting with the attention mechanism itself, since itâ€™s the heart of the transformer. Specifically, you could begin by implementing:\nScaled Dot-Product Attention - the most fundamental building block.\nThis involves: - Taking query (Q), key (K), and value (V) matrices as input - Computing attention scores - Applying the scaling factor - Using softmax to get attention weights - Returning the weighted sum of values\nWould you like me to explain what this function should do conceptually (inputs, outputs, and the mathematical operations), so you can try implementing it yourself? Or would you prefer to start with an even simpler piece, like understanding the data structures youâ€™ll need?\nQ * K^T / sqrt(D_K)* V\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Youâ€™ve got the formula down. Thatâ€™s the core of scaled dot-product attention.\nLet me make sure you understand each part:\n\nQ Â· K^T: This gives you attention scores (how much each query should â€œattend toâ€ each key)\n/ sqrt(d_k): The scaling factor prevents the dot products from getting too large\nThen softmax (you didnâ€™t write it, but it goes here!) converts scores to probabilities\nÂ· V: Finally, use those probabilities as weights for the values\n\nA couple of questions to check your understanding:\n\nWhat are the dimensions/shapes of Q, K, and V?\nWhere does the softmax operation fit in that formula you wrote?\n\nOnce youâ€™re clear on these, you can start coding the attention function. Want to clarify these points first, or are you ready to start implementing?\nIt makes sure that the outputs add up to one.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly! Softmax normalizes the scores so they sum to 1, turning them into a proper probability distribution over the values.\nSo the complete formula is:\nAttention(Q, K, V) = softmax(Q Â· K^T / sqrt(d_k)) Â· V\nNow, before you start coding, letâ€™s think about one more important detail: masking.\nIn the decoder, you need to prevent positions from â€œlooking aheadâ€ at future tokens. This is done by setting certain attention scores to negative infinity before the softmax.\nDo you understand why masking is needed and where it fits into the formula? Or should I clarify that first?\nOnce youâ€™re comfortable with the concept, you can start implementing the attention() function!\nWell, it makes sure that the outputs add up to one.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly! The softmax converts the raw similarity scores into a probability distribution - all values between 0 and 1 that sum to 1.\nThis is important because it tells us â€œhow much attentionâ€ to pay to each value. If one key is very similar to the query, it gets a high weight (close to 1), and we focus mostly on its corresponding value.\nNow youâ€™re ready to implement it! Try coding the attention() function. It should: - Take query, key, value as inputs (and optionally mask and dropout) - Return the attention output (and optionally the attention weights for visualization)\nDonâ€™t worry about getting it perfect - just give it a try and let me know what you come up with or if you get stuck on any part!\ndef attention(Q, K, V, dropout, mask): Q@K.T Am I doing the correct thing? I am not sure what I am doing.\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re on the right track! Youâ€™ve started with the Q @ K.T part, which is correct.\nLet me help clarify what you need to do step by step:\n\nQ @ K.T âœ“ (you have this!)\nDivide by sqrt(d_k) - you need to scale the scores. What is d_k? (Hint: itâ€™s related to the dimension of your key vectors)\nApply mask (if provided) - set certain positions to a very negative number\nApply softmax - convert scores to probabilities\nMultiply by V - get the weighted sum\nApply dropout (if provided) - for regularization\n\nTry adding step 2 next - how would you get d_k from your inputs, and how would you scale the scores?\nis it K.shape(0)? also give me code written till now\n\n\nğŸ¤–ReplyğŸ¤–\nNot quite! d_k is the dimension of each key vector, which is the last dimension of K.\nSo it would be: K.shape[-1] (or K.size(-1) in PyTorch)\nFor the code so far, I want to make sure youâ€™re building this yourself to really learn it. Hereâ€™s what you have:\ndef attention(Q, K, V, dropout, mask):\n    Q @ K.T\nNow try adding: 1. Store that result in a variable (maybe call it scores) 2. Get d_k from the key dimension 3. Divide scores by sqrt(d_k) (youâ€™ll need to import math for math.sqrt)\nGive it a try and show me what you get!\n\ndef attention(Q, K, V, dropout, mask):\n    sqrt_d_k = math.sqrt(K.size(-1))\n    attention = torch.softmax(Q @ K.T / sqrt_d_k)*V\n    return attention\n\nHow am I doing now?\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re doing great! Youâ€™ve got most of the core pieces in place. A few things to adjust:\n\nTranspose operation: In PyTorch, use .transpose(-2, -1) instead of .T to handle batched matrices correctly\nSoftmax dimension: You need to specify which dimension to apply softmax over - it should be the last dimension: torch.softmax(..., dim=-1)\nMissing pieces: You havenâ€™t used mask or dropout yet. The mask needs to be applied before the softmax (by setting masked positions to a very negative number like -1e9)\nParentheses: The multiplication by V should happen after the softmax, so make sure your parentheses are right\n\nTry fixing the transpose and softmax dimension first. Does that make sense?\nWhat is transpose - 2 - 1? I donâ€™t understand what heâ€™s saying. What do you mean applying softmax to the last dimension? I donâ€™t understand. I think Iâ€™m weak at all this dimension kind of stuff. Please help me.\n\n\nğŸ¤–ReplyğŸ¤–\nNo worries! Letâ€™s break this down - dimensions can be tricky at first.\nUnderstanding dimensions/axes:\nWhen you have a tensor (multi-dimensional array), each â€œdirectionâ€ is called a dimension or axis. For example: - A vector has 1 dimension: [1, 2, 3] has shape (3,) - A matrix has 2 dimensions: rows and columns, shape (rows, cols) - A batch of matrices has 3 dimensions: (batch_size, rows, cols)\nWhat is .transpose(-2, -1)?\nThis swaps the last two dimensions. The -1 means â€œlast dimensionâ€ and -2 means â€œsecond-to-lastâ€.\nFor a matrix with shape (rows, cols), transposing swaps them to (cols, rows).\nFor example, if K has shape (batch, seq_len, d_k), then K.transpose(-2, -1) gives you (batch, d_k, seq_len).\nWhat does dim=-1 in softmax mean?\nIt tells PyTorch which dimension to apply softmax across. dim=-1 means â€œapply softmax across the last dimensionâ€ - so each row will sum to 1.\nDoes this help clarify things? Want to try updating your code with these changes, or do you need more explanation on any part?\nOkay, Iâ€™ll update the code. But after I update the code and the code is proper, letâ€™s pass to an example through it so that I understand better. Give me the code so far.\n\n\nğŸ¤–ReplyğŸ¤–\nI want you to try updating it yourself first - thatâ€™s how youâ€™ll learn best!\nTake your current code:\ndef attention(Q, K, V, dropout, mask):\n    sqrt_d_k = math.sqrt(K.size(-1))\n    attention = torch.softmax(Q @ K.T / sqrt_d_k)*V\n    return attention\nAnd make these changes: 1. Change K.T to K.transpose(-2, -1) 2. Add dim=-1 to the softmax\nGive it a try! Once youâ€™ve updated it, weâ€™ll absolutely run an example through it to see how it works.\n\ndef attention(Q, K, V, dropout, mask):\n    sqrt_d_k = math.sqrt(K.size(-1))\n    dropout = torch.nn.Dropout(p=dropout)\n    attention = torch.softmax(Q @ K.transpose(-2,-1) / sqrt_d_k, dim=-1)@V\n    return dropout(attention)\n\nOkay, am I done?\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nAlmost there! Two things to fix:\n\nMask is not used: You need to apply the mask to the scores before the softmax. The typical approach is:\nscores = scores.masked_fill(mask == 0, -1e9)\nThis sets masked positions to a very negative number, so after softmax they become ~0.\nDropout placement: Dropout should be applied to the attention weights (after softmax, before multiplying by V), not to the final output.\n\nCan you restructure your code to: - Compute scores first (Q @ K.transposeâ€¦) - Apply mask to scores (if mask is provided) - Apply softmax to get attention weights - Apply dropout to attention weights (if dropout is provided) - Multiply by V - Return the result\nTry breaking it into separate lines - itâ€™ll be clearer and easier to debug!\nI donâ€™t understand why we need masks for laser clean.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me explain why masking is needed.\nWhy do we need masks?\nThere are two main reasons:\n\nPadding mask: When you batch sentences together, theyâ€™re often different lengths. You pad shorter sentences with special tokens (like &lt;pad&gt;). You donâ€™t want the model to pay attention to these padding tokens because theyâ€™re meaningless. So you mask them out.\nLook-ahead mask (for decoder): In the decoder during training, when predicting word 3, the model should only see words 1 and 2, not words 4, 5, 6 (the future). This prevents â€œcheatingâ€ - the model canâ€™t look at future words itâ€™s supposed to predict. This is called a â€œcausal maskâ€ or â€œsubsequent maskâ€.\n\nWhat does masking do?\nBy setting masked positions to -1e9 (very negative) before softmax, those positions become ~0 after softmax. This means â€œdonâ€™t pay attention to these positions at all.â€\nDoes this make sense? Do you want to see a simple example of how masking works with numbers?\nYes, give me a simple example. I donâ€™t understand what this command is doing. And where exactly is the mask applied?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Let me show you a simple numerical example.\nSimple masking example:\nLetâ€™s say you have attention scores (before softmax):\nscores = [2.0, 3.0, 1.0, 4.0]\nAnd a mask that says â€œignore positions 2 and 3â€:\nmask = [1, 1, 0, 0]  # 1 = keep, 0 = ignore\nStep 1: Apply mask (this is where scores.masked_fill(mask == 0, -1e9) happens)\nscores = [2.0, 3.0, -1e9, -1e9]\nStep 2: Apply softmax\nAfter softmax: [0.27, 0.73, ~0.0, ~0.0]\nSee? The masked positions become essentially 0 after softmax!\nWhere is the mask applied?\nIn your code, it should go right after you compute the scores and BEFORE softmax:\nscores = Q @ K.transpose(-2, -1) / sqrt_d_k\nif mask is not None:\n    scores = scores.masked_fill(mask == 0, -1e9)\nattention_weights = torch.softmax(scores, dim=-1)\nDoes this clarify it?\n\n\nFinal Attention Function\n\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n\n\n\nattention\n\n attention (Q, K, V, dropout, mask)\n\nCool, is this correct?\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Your attention function looks correct now! ğŸ‰\nYouâ€™ve got all the key pieces: - âœ“ Compute scores with proper transpose - âœ“ Scale by sqrt(d_k) - âœ“ Apply mask before softmax - âœ“ Apply softmax on the right dimension - âœ“ Apply dropout to attention weights - âœ“ Multiply by V\nOne small improvement: itâ€™s common to also return the attention_weights for visualization purposes (optional, but helpful for debugging). But for now, this works!\nWant to test it with a simple example to see it in action? Or are you ready to move on to the next component (Multi-Head Attention)?\nnot that simple. too many 1 and 0 lol\n\n\nğŸ¤–ReplyğŸ¤–\nHaha, fair enough! Let me make it even simpler with different numbers:\n# Super simple example - just 1 query and 2 keys/values\nQ = torch.tensor([[2.0, 2.0]])  # 1 query, d_k=2\n\nK = torch.tensor([[1.0, 1.0],   # key 1\n                  [1.0, 0.0]])  # key 2\n\nV = torch.tensor([[10.0],       # value 1\n                  [20.0]])      # value 2\n\nresult = attention(Q, K, V, dropout=0.0, mask=None)\nprint(result)\nNow try by hand: 1. Q @ K.T = ? (just dot products: [2,2]Â·[1,1] and [2,2]Â·[1,0]) 2. Divide by sqrt(2) â‰ˆ 1.41 3. Apply softmax (which one is bigger?) 4. Weighted sum of values [10] and [20]\nGive it a shot!\nQ ( 1x2) K_T (2x2) Q@K_t = [4 2] div by sqrt2 = [2sqr(2) sqrt(2)] softmax approx [1 0] result = [10 0[\n\n\nğŸ¤–ReplyğŸ¤–\nGreat work! Youâ€™re very close! Let me check your steps:\n\nQ @ K.T: âœ“ Correct! [4, 2]\nDivide by sqrt(2): âœ“ Correct! [2âˆš2, âˆš2] which is approximately [2.83, 1.41]\nSoftmax: Almost! It wonâ€™t be exactly [1, 0]. Since 2.83 is bigger than 1.41, the first position will have higher weight, but not exactly 1. Itâ€™ll be something like [0.8, 0.2] (the exact values depend on the exponentials)\nResult: Because of softmax, itâ€™s a weighted average: 0.8 * 10 + 0.2 * 20 = 8 + 4 = 12\n\nSo the result should be around [[12]] (a weighted average closer to 10 than 20).\nWant to run the code now to see the actual numbers?\nya gimme code\n\n\nğŸ¤–ReplyğŸ¤–\n# Super simple example - just 1 query and 2 keys/values\nQ = torch.tensor([[2.0, 2.0]])  # 1 query, d_k=2\n\nK = torch.tensor([[1.0, 1.0],   # key 1\n                  [1.0, 0.0]])  # key 2\n\nV = torch.tensor([[10.0],       # value 1\n                  [20.0]])      # value 2\n\nresult = attention(Q, K, V, dropout=0.0, mask=None)\nprint(\"Result:\", result)\nprint(\"Shape:\", result.shape)\nRun this and see what you get! Does it match your hand calculation?\n\n# Super simple example - just 1 query and 2 keys/values\nQ = torch.tensor([[2.0, 2.0]])  # 1 query, d_k=2\n\nK = torch.tensor([[1.0, 1.0],   # key 1\n                  [1.0, 0.0]])  # key 2\n\nV = torch.tensor([[10.0],       # value 1\n                  [20.0]])      # value 2\n\nresult = attention(Q, K, V, dropout=0.0, mask=None)\nprint(\"Result:\", result)\nprint(\"Shape:\", result.shape)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[1], line 10\n      4 K = torch.tensor([[1.0, 1.0],   # key 1\n      5                   [1.0, 0.0]])  # key 2\n      7 V = torch.tensor([[10.0],       # value 1\n      8                   [20.0]])      # value 2\n---&gt; 10 result = attention(Q, K, V, dropout=0.0, mask=None)\n     11 print(\"Result:\", result)\n     12 print(\"Shape:\", result.shape)\n\nCell In[1], line 7, in attention(Q, K, V, dropout, mask)\n      5     scores = scores.masked_fill(mask==0, -1e9)\n      6 attention_weights = torch.softmax(scores, dim=-1)\n----&gt; 7 attention_weights = dropout(attention_weights)\n      8 output = attention_weights @ V\n      9 return output\n\nTypeError: 'float' object is not callable\n\n\n\nConvert this into a unit as. Assume Iâ€™m writing an NB Dev Notebook.\n\n\nğŸ¤–ReplyğŸ¤–\nIâ€™ll help you convert your attention function into a unit test for an nbdev notebook. Hereâ€™s how you can structure it:\n#| export\ndef attention(Q, K, V, dropout=None, mask=None):\n    \"Compute Scaled Dot-Product Attention\"\n    sqrt_d_k = math.sqrt(K.size(-1))\n    scores = Q @ K.transpose(-2,-1) / sqrt_d_k\n    if mask is not None:\n        scores = scores.masked_fill(mask==0, -1e9)\n    attention_weights = torch.softmax(scores, dim=-1)\n    if dropout is not None:\n        attention_weights = dropout(attention_weights)\n    output = attention_weights @ V\n    return output\n#| test\ndef test_attention_basic():\n    \"Test attention with simple hand-calculable example\"\n    Q = torch.tensor([[2.0, 2.0]])  # 1 query, d_k=2\n    K = torch.tensor([[1.0, 1.0],   # key 1\n                      [1.0, 0.0]])  # key 2\n    V = torch.tensor([[10.0],       # value 1\n                      [20.0]])      # value 2\n    \n    result = attention(Q, K, V, dropout=None, mask=None)\n    \n    # Result should be weighted average closer to 10 than 20\n    assert result.shape == torch.Size([1, 1])\n    assert 11.0 &lt; result.item() &lt; 13.0  # approximately 11.96\nA couple of notes: 1. I made dropout and mask optional parameters with default None 2. The #| export directive marks the function for export to your module 3. The #| test directive marks the test function\nWould you like me to add more test cases (e.g., testing with masks, or with batched inputs)?\n\n\nUnit Test\n\ndef test_attention_basic():\n    \"Test attention with simple hand-calculable example\"\n    Q = torch.tensor([[2.0, 2.0]])  # 1 query, d_k=2\n    K = torch.tensor([[1.0, 1.0],   # key 1\n                      [1.0, 0.0]])  # key 2\n    V = torch.tensor([[10.0],       # value 1\n                      [20.0]])      # value 2\n    \n    result = attention(Q, K, V, dropout=None, mask=None)\n    \n    # Result should be weighted average closer to 10 than 20\n    assert result.shape == torch.Size([1, 1])\n    assert 11.0 &lt; result.item() &lt; 13.0  # approximately 11.96\n\nnoice But are the dimensions of q, k, v you have given realistic? What are the real ones?\n\nğŸ¤–ReplyğŸ¤–\nGreat question! The dimensions I gave you were simplified for hand calculation. Let me show you realistic dimensions:\nIn a real transformer:\n\nBatch size (B): typically 8, 16, 32, etc.\nSequence length (S): how many tokens (words/subwords) - could be 10, 50, 512, etc.\nd_model: the model dimension - typically 512 (base model) or 1024 (large model)\nd_k (key/query dimension): typically d_model / num_heads. For 8 heads and d_model=512, d_k = 64\n\nRealistic shapes:\nQ: (batch_size, seq_len, d_k)     e.g., (32, 50, 64)\nK: (batch_size, seq_len, d_k)     e.g., (32, 50, 64)\nV: (batch_size, seq_len, d_v)     e.g., (32, 50, 64)\nNote: usually d_k = d_v = 64 when using 8 attention heads.\nWant to test your attention function with more realistic dimensions to make sure it handles batches correctly?\nya\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Try this:\n# Realistic dimensions\nbatch_size = 2\nseq_len = 4\nd_k = 8\n\nQ = torch.randn(batch_size, seq_len, d_k)\nK = torch.randn(batch_size, seq_len, d_k)\nV = torch.randn(batch_size, seq_len, d_k)\n\nresult = attention(Q, K, V, dropout=0.0, mask=None)\nprint(\"Result shape:\", result.shape)\nprint(\"Expected shape:\", (batch_size, seq_len, d_k))\nRun this and check if the output shape matches what you expect. Does it work correctly?\n\n# Realistic dimensions\nbatch_size = 2\nseq_len = 4\nd_k = 8\n\nQ = torch.randn(batch_size, seq_len, d_k)\nK = torch.randn(batch_size, seq_len, d_k)\nV = torch.randn(batch_size, seq_len, d_k)\n\nresult = attention(Q, K, V, dropout=0.0, mask=None)\nprint(\"Result shape:\", result.shape)\nprint(\"Expected shape:\", (batch_size, seq_len, d_k))\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 K = torch.randn(batch_size, seq_len, d_k)\n      8 V = torch.randn(batch_size, seq_len, d_k)\n---&gt; 10 result = attention(Q, K, V, dropout=0.0, mask=None)\n     11 print(\"Result shape:\", result.shape)\n     12 print(\"Expected shape:\", (batch_size, seq_len, d_k))\n\nCell In[1], line 7, in attention(Q, K, V, dropout, mask)\n      5     scores = scores.masked_fill(mask==0, -1e9)\n      6 attention_weights = torch.softmax(scores, dim=-1)\n----&gt; 7 attention_weights = dropout(attention_weights)\n      8 output = attention_weights @ V\n      9 return output\n\nTypeError: 'float' object is not callable\n\n\n\nWhat is the philosophy behind attention? What is Q, what is K, what is V? Give a proper analogy.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me give you an intuitive analogy.\nThe Library Analogy:\nImagine youâ€™re in a library looking for information about â€œtransformers in NLPâ€:\n\nQuery (Q): Your question/search term - â€œWhat are transformers?â€\nKeys (K): The titles/keywords of all books on the shelves\nValues (V): The actual content of those books\n\nHow attention works:\n\nYou compare your query against all the book titles (Q Â· K^T) - which books are most relevant?\nYou get similarity scores - â€œDeep Learningâ€ book scores high, â€œCooking 101â€ scores low\nSoftmax turns these into weights - how much to focus on each book\nYou read a weighted combination of the books (weights Â· V) - paying more attention to relevant ones\n\nIn transformers:\n\nEach word creates its own Query: â€œWhat information do I need?â€\nEach word provides a Key: â€œI have this type of informationâ€\nEach word provides a Value: â€œHereâ€™s my actual informationâ€\n\nThe word â€œbankâ€ (query) might attend strongly to â€œriverâ€ (high attention weight) vs â€œmoneyâ€ depending on context!\nDoes this help clarify the philosophy?\nOkay, so I get that, but how does query, and the key, and the value relate to attention overall? Isnâ€™t that just multiplying inputs with WQ and WK? How is that converting to query and key?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent question! Youâ€™re getting to the heart of how attention actually works.\nYouâ€™re absolutely right! Q, K, and V are created by multiplying the input with learned weight matrices:\nQ = Input @ W_Q\nK = Input @ W_K  \nV = Input @ W_V\nWhy do we do this?\nThe input embeddings are just generic word representations. By multiplying with W_Q, W_K, W_V, weâ€™re learning to transform the input into three different â€œviewsâ€:\n\nW_Q learns: â€œHow should I ask questions?â€ (what to look for)\nW_K learns: â€œHow should I advertise what I have?â€ (what I can provide)\nW_V learns: â€œWhat information should I actually pass along?â€ (the content to share)\n\nExample: The word â€œbankâ€ has one embedding. But: - As a Query (W_Q): it might learn to ask â€œwhatâ€™s nearby?â€ - As a Key (W_K): it might learn to signal â€œIâ€™m a nounâ€ - As a Value (W_V): it might learn to pass along semantic meaning\nThe magic is that W_Q, W_K, W_V are learned during training to make attention useful!\nDoes this connection make sense?\nExcellent. Letâ€™s move on.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Youâ€™ve successfully implemented the core attention mechanism.\nNow the next step is Multi-Head Attention. This is where we run multiple attention operations in parallel (multiple â€œheadsâ€), each learning different patterns.\nThe key idea: instead of one attention, we have h heads (typically 8), each with its own W_Q, W_K, W_V matrices. Then we concatenate all the outputs and project them back.\nBefore we start coding, do you understand conceptually: 1. Why weâ€™d want multiple attention heads instead of just one? 2. How the dimensions change when we split into multiple heads?\nOr should I explain the intuition first?\nWe need multiple attention heads because we want to see: - One word can have a correlation with other words - Another attention head can give another perspective\nHow the dimension changes when we split into multiple heads? Maybe the input dimensions is divided by the number of tangent heads. Thatâ€™s how it splits.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right on both points! ğŸ‰\nWhy multiple heads: Each head can learn different relationships - one might focus on syntactic patterns (subject-verb), another on semantic similarity, another on positional relationships, etc.\nDimension splitting: Yes! If d_model = 512 and we have 8 heads, each head works with d_k = 512/8 = 64 dimensions. This keeps the total computation roughly the same as single-head attention.\nThe process: 1. Start with input of shape (batch, seq_len, d_model=512) 2. Project to Q, K, V using W_Q, W_K, W_V 3. Reshape/split into h heads: (batch, seq_len, h=8, d_k=64) 4. Run attention on each head in parallel 5. Concatenate heads back together: (batch, seq_len, d_model=512) 6. Final linear projection with W_O\nReady to start implementing? What should we build first - the reshaping logic or the overall MultiHeadAttention class structure?\nI donâ€™t know whatâ€™s easier to start with.\n\n\nğŸ¤–ReplyğŸ¤–\nLetâ€™s start with the overall class structure first - itâ€™s easier to see the big picture, then fill in the details.\nA MultiHeadedAttention class needs: - __init__: store hyperparameters (h, d_model) and create the weight matrices - forward: take input, split into heads, run attention, concatenate, project\nTry writing just the skeleton:\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        # What do we need to store here?\n        # What layers/parameters do we need?\n        \n    def forward(self, query, key, value, mask=None):\n        # What steps go here?\nThink about: what parameters does this class need to store? (Hint: you need 4 linear layers - why 4?)\nGive it a try!\ni havent a clu where to start\n\n\nğŸ¤–ReplyğŸ¤–\nNo worries! Let me break it down into smaller pieces.\nWhat does MultiHeadedAttention need to do?\nRemember: we need to project Q, K, V, then split into heads, run attention, concatenate, and project again.\nStart with the __init__ method. You need:\n\nStore h (number of heads) and d_k (dimension per head)\nCreate 4 linear layers:\n\nOne for projecting to Q\nOne for projecting to K\n\nOne for projecting to V\nOne for the final output projection (after concatenating heads)\n\n\nTry writing just the __init__ method:\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        self.h = h\n        self.d_k = d_model // h  # dimension per head\n        \n        # Create 4 linear layers here\n        # Each should be nn.Linear(d_model, d_model)\nCan you add the 4 linear layers? Maybe store them in a list or as separate attributes?\nSo, bro, instead of doing all this guesswork, should I read the paper first and then come back to this notebook?\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nThatâ€™s a totally fair question! Thereâ€™s no single right answer - it depends on how you learn best.\nOption A: Read the paper first - Pros: Youâ€™ll understand the â€œwhyâ€ behind each component, the math notation, and the full architecture before coding - Cons: The paper can be dense; some parts might not click until you actually implement them\nOption B: Keep coding, refer to paper as needed - Pros: Hands-on learning, immediate feedback, you discover the â€œwhyâ€ as you hit roadblocks - Cons: Can feel like guesswork (which is what youâ€™re feeling now!)\nOption C: Hybrid approach (my suggestion) - Read just the Section 3.2 (Attention) of the paper - itâ€™s only ~1.5 pages - It explains Multi-Head Attention with diagrams and equations - Then come back and implement with that understanding\nThe paper is here: https://arxiv.org/pdf/1706.03762 - Section 3.2 starts around page 3.\nWhat sounds best to you? Would you like to take a break and read that section, or would you prefer I explain the Multi-Head Attention concept more thoroughly here first?\nill read the architecture part first i guess. all the misc experiments i can see later\n\n\nğŸ¤–ReplyğŸ¤–\nGreat plan! Reading the architecture section first will give you the foundation you need.\nWhat to focus on:\n\nSection 3: Model Architecture (pages 3-5)\n\n3.1: Overall encoder-decoder structure\n3.2: Attention mechanisms (especially Multi-Head Attention)\n3.3: Position-wise Feed-Forward Networks\n3.4: Embeddings and Positional Encoding\n\n\nYou can skip sections 4 (training details) and 5-6 (experiments/results) for now.\nTake your time reading, and when you come back, let me know: 1. What clicked for you? 2. Whatâ€™s still confusing? 3. Whether youâ€™re ready to continue implementing Multi-Head Attention!\nSee you when youâ€™re back! ğŸ“š",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#reading-paper",
    "href": "attention_is_all_you_need.html#reading-paper",
    "title": "Simple 2x2 example",
    "section": "Reading Paper",
    "text": "Reading Paper\n\n!pip install pypdf2\nfrom PyPDF2 import PdfReader\nreader = PdfReader('../Attention.pdf')\ntext = '\\n'.join([page.extract_text() for page in reader.pages])\n\nRequirement already satisfied: pypdf2 in /app/data/.local/lib/python3.12/site-packages (3.0.1)\n\n\nAttention Paper Text $text\nis the attention paper in context now?\n\nğŸ¤–ReplyğŸ¤–\nYes! I can see youâ€™ve loaded the Attention paper PDF and extracted the text into the text variable. The paper content is now available in your context.\nWould you like me to help you: 1. Find specific sections (like Section 3.2 on Multi-Head Attention)? 2. Explain parts that are confusing as you read? 3. Answer questions about what youâ€™ve read so far?\nOr are you still reading through it on your own first?\nWhat does transduction mean?\n\n\nğŸ¤–ReplyğŸ¤–\nGood question! Transduction in the context of this paper means transforming one sequence into another sequence.\nExamples of transduction tasks: - Machine translation: English sentence â†’ French sentence - Text summarization: Long article â†’ Short summary\n- Speech recognition: Audio sequence â†’ Text sequence\nItâ€™s basically any task where you take an input sequence and produce an output sequence, which might be in a different â€œformâ€ or language.\nThe paper calls the Transformer a â€œtransduction modelâ€ because it takes a source sequence (e.g., English) and produces a target sequence (e.g., German).\nDoes that make sense? What section are you reading now?\nThe encoder transforms the sequence X to sequence Z. What are they referring to?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me clarify what X and Z represent:\nX (input sequence): The original input to the encoder - Example: The English sentence â€œHello worldâ€ - Represented as token embeddings: (xâ‚, xâ‚‚, â€¦, xâ‚™) - These are discrete tokens converted to vectors\nZ (continuous representations): The encoderâ€™s output - A sequence of continuous vector representations: (zâ‚, zâ‚‚, â€¦, zâ‚™) - Same length as input, but now â€œenrichedâ€ with context from the whole sentence - Each záµ¢ contains information about xáµ¢ AND its relationship to all other tokens\nThe transformation: X â†’ Z - X is the â€œrawâ€ embedded input - Z is the contextualized representation after all the encoder layers process it - Z captures meaning, relationships, and context\nThen the decoder uses Z (along with previous outputs) to generate the target sequence Y.\nDoes that clarify it? What part of the architecture are you reading about now?\nIâ€™m reading Section 3.1.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Section 3.1 covers the overall encoder-decoder structure.\nKey points in 3.1:\n\nEncoder: Takes input sequence, produces contextualized representations (the Z we just discussed)\nDecoder: Takes Z + previously generated outputs, produces next token (auto-regressive)\nBoth encoder and decoder are stacks of N=6 identical layers\nEach layer has sub-layers with residual connections and layer normalization\n\nAs you read through, are the concepts of: 1. Stacked layers (N=6 identical layers on top of each other) clear? 2. Auto-regressive generation (decoder generates one token at a time, using previous outputs) making sense?\nLet me know what questions come up or when youâ€™re ready to move to Section 3.2 (the attention mechanisms)!\nWhy do we divide by square root of dk?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent question! This is one of the key insights in the attention mechanism.\nThe problem without scaling:\nWhen d_k (the dimension) is large, the dot products QÂ·K^T can get very large in magnitude.\nWhy is this bad?\nLarge dot products â†’ softmax gets pushed into regions with extremely small gradients â†’ model canâ€™t learn well (vanishing gradients)\nThe math intuition:\nIf Q and K have components that are random variables with mean 0 and variance 1, then their dot product has: - Mean: 0 - Variance: d_k (grows with dimension!)\nBy dividing by âˆšd_k, we scale the variance back down to ~1, keeping the values in a reasonable range for softmax.\nIn simple terms: It prevents the attention scores from getting too extreme, which keeps the softmax working properly and gradients flowing during training.\nDoes this make sense? Have you gotten to Section 3.2.1 yet where they explain this?\nGive me an actual numerical example.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Let me show you with actual numbers.\nWithout scaling (d_k = 64):\nSay Q and K are random vectors with values around -1 to 1:\nQ = [0.5, -0.3, 0.8, ..., 0.2]  (64 dimensions)\nK = [0.4, 0.6, -0.5, ..., 0.7]  (64 dimensions)\n\nQÂ·K = 0.5*0.4 + (-0.3)*0.6 + ... (64 terms)\n    â‰ˆ 25.3  (large number!)\nAfter softmax on large scores:\nscores = [25.3, 18.7, 30.1, 12.4]\nsoftmax = [0.82, 0.001, 0.999, 0.0001]  â† Very extreme! Almost one-hot\nWith scaling (divide by âˆš64 = 8):\nscaled_scores = [3.16, 2.34, 3.76, 1.55]\nsoftmax = [0.35, 0.15, 0.45, 0.05]  â† Much smoother distribution\nSee the difference? Without scaling, softmax becomes too â€œconfidentâ€ (almost all weight on one item). With scaling, you get a more balanced attention distribution.\nDoes this numerical example clarify why we need the âˆšd_k scaling?\nSo, these scores are what for each sequence length?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me clarify what these scores represent.\nThe scores are for each pair of positions in the sequence.\nIf you have a sequence of length 4 (e.g., 4 words), you get a 4Ã—4 matrix of scores:\n        word1  word2  word3  word4\nword1   [25.3  18.7  30.1  12.4]  â† how much word1 attends to each word\nword2   [20.1  22.5  15.3  28.7]  â† how much word2 attends to each word\nword3   [19.2  31.4  21.8  16.5]  â† how much word3 attends to each word\nword4   [27.6  14.2  23.1  19.8]  â† how much word4 attends to each word\nEach row is one query asking â€œhow much should I attend to each position?â€\nEach column is one key saying â€œhereâ€™s how relevant I am to each queryâ€\nSo for a sequence of length n, you get an nÃ—n attention score matrix.\nDoes that clarify what the scores represent?\nThe paper we mentioned, â€œAdditive Attentionâ€, what the heck is that? Give some examples with numbers as well.\n\n\nğŸ¤–ReplyğŸ¤–\nGood question! Additive attention is an older attention mechanism that came before the dot-product attention used in Transformers.\nAdditive Attention (Bahdanau attention):\nInstead of QÂ·K^T, it uses a small neural network:\nscore = v^T Â· tanh(Wâ‚Â·Q + Wâ‚‚Â·K)\nWhere v, Wâ‚, Wâ‚‚ are learned parameters.\nNumerical example:\nSay d_k = 3, and we have:\nQ = [1, 2, 1]\nK = [2, 0, 1]\n\nStep 1: Linear transformations\nWâ‚Â·Q = [0.5, 1.5]  (project to hidden dim=2)\nWâ‚‚Â·K = [1.0, 0.5]\n\nStep 2: Add and apply tanh\ntanh([0.5+1.0, 1.5+0.5]) = tanh([1.5, 2.0]) = [0.91, 0.96]\n\nStep 3: Project to scalar\nv^T Â· [0.91, 0.96] = [0.5, 0.6] Â· [0.91, 0.96] = 1.03\nKey difference: Additive uses a feedforward network, dot-product just multiplies. Dot-product is faster (just matrix multiplication) which is why Transformers use it.\nDoes this comparison make sense?\nThe paper says it works better for larger values of decay. So do we use additive attention for larger values of decay?\n\n\nğŸ¤–ReplyğŸ¤–\nGood catch reading carefully! Let me clarify what the paper actually says:\nThe paper says additive attention outperforms dot-product attention WITHOUT scaling for larger values of d_k.\nBut then they add the scaling factor (dividing by âˆšd_k), which fixes the problem. So with scaling, dot-product attention works well even for large d_k.\nIn practice: - Transformers use scaled dot-product attention (what you implemented) - Additive attention is rarely used anymore because scaled dot-product is: - Faster (just matrix multiplication, highly optimized) - More space-efficient - Works just as well with the scaling fix\nSo no, we donâ€™t switch to additive attention for large d_k. We just use the âˆšd_k scaling to keep dot-product working well!\nDoes that clear up the confusion?\nSo, in practise, we are never using additive attention, am I correct?\n\n\nğŸ¤–ReplyğŸ¤–\nExactly correct!\nIn modern practice, we donâ€™t use additive attention. The Transformer paper mentions it mainly for historical context and to explain why they chose scaled dot-product attention.\nWhat we actually use: - Scaled dot-product attention (what youâ€™ve already implemented!) - Itâ€™s faster, simpler, and works great with the âˆšd_k scaling\nThe paper just wanted to acknowledge that additive attention exists and explain why dot-product + scaling is the better choice.\nAre you ready to move on to Section 3.2.2 about Multi-Head Attention, or do you have more questions about the attention mechanism itself?\nOkay, Iâ€™m reading this. Iâ€™m kind of lost. We take q, k, and v as inputs and project them using wiq, wiv, and wik. Am I correct?\n\n\nğŸ¤–ReplyğŸ¤–\nYes, youâ€™re on the right track! Let me clarify the flow:\nIn Multi-Head Attention:\n\nYou start with the same input (letâ€™s call it X)\nYou project it to get Q, K, V:\n\nQ = X @ W^Q\nK = X @ W^K\n\nV = X @ W^V\n\nThen for multi-head, each head has its own projection matrices:\n\nHead i uses: W^Q_i, W^K_i, W^V_i\nSo you get Q_i, K_i, V_i for each head\n\nRun attention on each head separately\nConcatenate all head outputs\nFinal projection with W^O\n\nKey point: Youâ€™re projecting the input X multiple times (once per head), not projecting Q, K, V again.\nDoes this clarify the flow? Or are you still confused about which projections happen when?\nOh, so the outputs are D_K. Which is basically the input length divided by the number of heads. Am I correct?\n\n\nğŸ¤–ReplyğŸ¤–\nNot quite! Let me clarify the dimensions:\nd_k is NOT the input length. Itâ€™s the dimension per head.\nHereâ€™s the breakdown:\n\nd_model: The full model dimension (e.g., 512)\nh: Number of heads (e.g., 8)\nd_k: Dimension per head = d_model / h (e.g., 512/8 = 64)\n\nThe shapes:\nInput X: (batch, seq_len, d_model)\n         e.g., (32, 50, 512)\n\nAfter projecting and splitting into heads:\nQ, K, V: (batch, h, seq_len, d_k)\n         e.g., (32, 8, 50, 64)\nSo seq_len (input length) stays the same. Weâ€™re splitting the feature dimension (d_model) across heads, not the sequence length!\nDoes this distinction make sense?\nBut we are basically multiplying the whole d_model input with the W to make projections of size d_k. Am I correct?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! Let me clarify the projection step:\nWhat actually happens:\nYou multiply the input (d_model) with W to get an output thatâ€™s still d_model size, then you reshape/split it into h heads of size d_k each.\nInput: (batch, seq_len, d_model=512)\n       â†“\nW^Q projection: (d_model=512, d_model=512)\n       â†“\nQ: (batch, seq_len, d_model=512)\n       â†“\nReshape/split into heads: (batch, h=8, seq_len, d_k=64)\nSo the projection matrix W^Q is 512Ã—512, not 512Ã—64. You get the full d_model output, then split it into heads.\nAlternative approach (mathematically equivalent): Some implementations do use h separate smaller matrices (d_model â†’ d_k), but conceptually itâ€™s the same.\nDoes this clarify the projection dimensions?\nOkay, Iâ€™m pretty much saying the same thing because the paper talks about WI. Which means the weight for each head.\n\n\nğŸ¤–ReplyğŸ¤–\nAh yes, youâ€™re absolutely right! I apologize for the confusion.\nYouâ€™re correct - the paper uses W^Q_i, W^K_i, W^V_i for each head i, which are the individual projection matrices for that specific head.\nSo yes, each head has its own: - W^Q_i: projects from d_model to d_k - W^K_i: projects from d_model to d_k\n- W^V_i: projects from d_model to d_k\nThen you run attention separately on each headâ€™s Q_i, K_i, V_i.\nYou had it right! The paper does describe it with separate weight matrices per head (the subscript i indicates which head).\nSo to confirm: you multiply the d_model input with W_i to get projections of size d_k for each head. âœ“\nReady to keep reading, or want to clarify anything else about multi-head attention?\nWell, I understand your point too that in application, you just have a consolidated W. Am I correct?\n\n\nğŸ¤–ReplyğŸ¤–\nExactly! Youâ€™ve got it!\nIn practice (implementation): - You often have one big consolidated W^Q matrix (d_model â†’ d_model) - Then you reshape/split the output into h heads of d_k each - This is computationally more efficient (one big matrix multiply vs.Â h small ones)\nConceptually (in the paper): - They describe it as h separate W^Q_i matrices (d_model â†’ d_k) - This makes the math clearer to understand\nBoth views are equivalent - just different ways of thinking about/implementing the same operation!\nHave you finished reading Section 3.2.2 on Multi-Head Attention? Ready to start implementing it, or do you want to read more of the paper first?",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#multiheadedattention",
    "href": "attention_is_all_you_need.html#multiheadedattention",
    "title": "Simple 2x2 example",
    "section": "MultiHeadedAttention",
    "text": "MultiHeadedAttention\nyes lets implement now\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s implement Multi-Head Attention.\nWeâ€™ll start with the __init__ method. You need to store: 1. Number of heads (h) and dimension per head (d_k) 2. The projection matrices (weâ€™ll use the consolidated approach - 4 linear layers) 3. A dropout layer\nTry writing this:\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0  # d_model must be divisible by h\n        \n        self.d_k = d_model // h\n        self.h = h\n        \n        # Create 4 linear layers (Q, K, V projections + final output)\n        # Hint: you can use a list or create them individually\n        \n        self.dropout = nn.Dropout(p=dropout)\nCan you add the 4 linear layers? Each should be nn.Linear(d_model, d_model).\n\nclass MultiHeadedAttention(torch.nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0  # d_model must be divisible by h\n        \n        self.d_k = d_model // h\n        self.h = h\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.attn=None\n        \n        # Create 4 linear layers (Q, K, V projections + final output)\n        # Hint: you can use a list or create them individually\n        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(d_model, d_model) for x in range(4)])\n\nall good?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Your __init__ is perfect! ğŸ‰\nYouâ€™ve got: - âœ“ d_k calculation - âœ“ All 4 linear layers in a ModuleList - âœ“ Dropout layer - âœ“ self.attn for storing attention weights (good thinking!)\nNow letâ€™s implement the forward method. The steps are:\n\nGet batch size\nProject input to Q, K, V using first 3 linear layers\nReshape from (batch, seq_len, d_model) to (batch, h, seq_len, d_k)\nApply attention function\nReshape back and concatenate heads\nApply final linear layer\n\nTry starting with steps 1-2. Can you write the code to: - Get the batch size from the query input - Apply the first 3 linear layers to query, key, value?\n\ndef forward(self, query, key, value, mask=None):\n    # Step 1: Project query, key, value using the first 3 linear layers\n    Q = self.linear_layers[0](query)\n    K = self.linear_layers[1](key)\n    V = self.linear_layers[2](value)\n    # Step 2: Reshape to split into h heads\n    batch_size = Q.size(0)\n    seq_len = Q.size(1)\n    Q = Q.view(batch_size, self.h, seq_len, self.d_k)\n    K = K.view(batch_size, self.h, seq_len, self.d_k)\n    V = V.view(batch_size, self.h, seq_len, self.d_k)\n    # Step 3: Apply attention\n\n    # Step 4: Concatenate heads back together\n    # Step 5: Apply final linear projection\n\nSo far, so good?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat progress! Your projection and reshaping look good. One small issue with the reshape:\nThe order should be (batch_size, seq_len, h, d_k) first, then use .transpose(1, 2) to get (batch_size, h, seq_len, d_k).\nTry changing:\nQ = Q.view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\nThis makes the reshaping cleaner. Does that make sense?\nOnce you fix that, try Step 3 - call your attention() function with Q, K, V. What do you get back, and what shape will it be?\nWhy is that cleaner? I donâ€™t understand.\n\n\nğŸ¤–ReplyğŸ¤–\nGood question! Let me explain why.\nYour approach:\nQ.view(batch_size, self.h, seq_len, self.d_k)\nThis directly reshapes the flat d_model dimension into (h, seq_len, d_k), but the data in memory isnâ€™t organized that way yet.\nThe cleaner approach:\nQ.view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\nStep 1: view(batch_size, seq_len, self.h, self.d_k) - splits d_model into (h, d_k) while keeping seq_len intact Step 2: .transpose(1, 2) - swaps seq_len and h dimensions to get (batch_size, h, seq_len, d_k)\nWhy is it cleaner? The view operation splits the last dimension naturally (d_model â†’ h Ã— d_k), which matches how the data is laid out in memory. Then transpose just reorders the dimensions.\nYour way might work, but the viewâ†’transpose pattern is the standard approach and less likely to cause weird reshaping bugs.\nWant to try updating your code with this pattern?\nOh, so I understand now. We are first trying to split DM into H pieces. Then we swap the dimension edge and sequence length. So that we can pass it into H attention heads.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly! Youâ€™ve got it! ğŸ‰\nThe logic: 1. Split d_model into h pieces of size d_k: (batch, seq_len, d_model) â†’ (batch, seq_len, h, d_k) 2. Swap so heads come before seq_len: (batch, seq_len, h, d_k) â†’ (batch, h, seq_len, d_k) 3. Now each head can process its own (seq_len, d_k) slice independently in parallel!\nThis organization makes it easy to run attention on all h heads at once.\nNow try updating your reshape code with this pattern, then move to Step 3 - calling the attention() function. What will you pass to it?\n\ndef forward(self, query, key, value, mask=None):\n    # Step 1: Project query, key, value using the first 3 linear layers\n    Q = self.linear_layers[0](query)\n    K = self.linear_layers[1](key)\n    V = self.linear_layers[2](value)\n    # Step 2: Reshape to split into h heads\n    batch_size = Q.size(0)\n    seq_len = Q.size(1)\n    Q = Q.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    K = K.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    V = V.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    # Step 3: Apply attention\n\n    # Step 4: Concatenate heads back together\n    # Step 5: Apply final linear projection\n\nbetter now?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your reshaping is now correct. âœ“\nNow for Step 3 - applying attention. You need to call your attention() function.\nWhat arguments will you pass to it? Remember the signature:\nattention(Q, K, V, dropout, mask)\nWhat should each argument be? Give it a try!\ngood?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! A few small issues:\n\nself.mask doesnâ€™t exist - you should pass the mask parameter from the function arguments\nYouâ€™re passing mask twice - check your attention function signature\nStore the result - you need to capture what attention returns\n\nYour attention function signature is:\ndef attention(Q, K, V, dropout, mask):\nSo it should be:\nx = attention(Q, K, V, self.dropout, mask)\nAlso, one more thing: if mask is provided, you need to unsqueeze it to match the head dimension. Try:\nif mask is not None:\n    mask = mask.unsqueeze(1)  # Add head dimension\nCan you fix these issues?\nWhat is mask unsqueeze 1? I donâ€™t understand.\n\n\nğŸ¤–ReplyğŸ¤–\nGood question! Let me explain unsqueeze.\nWhat is unsqueeze?\nIt adds a new dimension to a tensor at the specified position.\nExample:\nmask = torch.tensor([[1, 1, 0, 0]])  # shape: (1, 4)\n\nmask.unsqueeze(0)  # shape: (1, 1, 4) - adds dim at position 0\nmask.unsqueeze(1)  # shape: (1, 1, 4) - adds dim at position 1\nmask.unsqueeze(2)  # shape: (1, 4, 1) - adds dim at position 2\nWhy do we need it here?\nYour mask comes in with shape: (batch_size, seq_len, seq_len)\nBut after splitting into heads, Q, K, V have shape: (batch_size, h, seq_len, d_k)\nSo the mask needs to be: (batch_size, 1, seq_len, seq_len) - the 1 broadcasts across all h heads.\nmask.unsqueeze(1) adds that dimension at position 1 (after batch_size).\nDoes this make sense? Want to see a numerical example?\nI still donâ€™t understand how the one broadcast across all H heads.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me explain broadcasting.\nWhat is broadcasting?\nWhen you do operations between tensors of different shapes, PyTorch automatically â€œstretchesâ€ dimensions of size 1 to match the other tensor.\nExample:\nA = torch.tensor([[[1, 2, 3]]])  # shape: (1, 1, 3)\nB = torch.tensor([[[10], [20]]])  # shape: (1, 2, 1)\n\nA + B  # Result shape: (1, 2, 3)\n# The 1s get \"broadcast\" (repeated) to match\nIn our case:\nmask shape:      (batch, 1,     seq_len, seq_len)\nattention shape: (batch, h=8,   seq_len, seq_len)\n                          â†‘\n                    This 1 broadcasts to 8\nThe mask with dimension 1 automatically gets copied/repeated 8 times (once for each head) when you apply it.\nSo the same mask applies to all heads, but PyTorch handles the copying automatically!\nDoes this clarify how broadcasting works?\n\ndef forward(self, query, key, value, mask=None):\n    # Step 1: Project query, key, value using the first 3 linear layers\n    Q = self.linear_layers[0](query)\n    K = self.linear_layers[1](key)\n    V = self.linear_layers[2](value)\n    # Step 2: Reshape to split into h heads\n    batch_size = Q.size(0)\n    seq_len = Q.size(1)\n    Q = Q.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    K = K.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    V = V.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    # Step 3: Apply attention\n    if mask is not None:\n        mask = mask.unsqueeze(1)\n    x = attention(Q,K,V, self.dropout, mask)\n\n    # Step 4: Concatenate heads back together\n    # Step 5: Apply final linear projection\n\nIs this better now?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Step 3 is now correct! âœ“\nYouâ€™ve got: - âœ“ Mask unsqueeze for broadcasting across heads - âœ“ Calling attention with the right arguments - âœ“ Storing result in x\nNow for Step 4: Concatenate heads back together\nAfter attention, x has shape: (batch_size, h, seq_len, d_k)\nYou need to get back to: (batch_size, seq_len, d_model)\nThis is the reverse of what you did earlier: 1. Transpose to swap h and seq_len back 2. Reshape/view to merge h and d_k into d_model\nTry writing the code for step 4. What operations do you need?\nWait, Iâ€™m seriously confused. How is reshaping the whole thing led to multiple attention heads? this is the attn fn def attention(Q, K, V, dropout, mask): sqrt_d_k = math.sqrt(K.size(-1)) dropout_layer = torch.nn.Dropout(p=dropout) scores = Q @ K.transpose(-2,-1) / sqrt_d_k if mask is not None: scores = scores.masked_fill(mask==0, -1e9) attention_weights = torch.softmax(scores, dim=-1) attention_weights = dropout_layer(attention_weights) output = attention_weights @ V return output\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent question! This is a key insight about how multi-head attention works.\nThe magic is in the tensor dimensions!\nWhen you pass Q, K, V with shape (batch_size, h, seq_len, d_k) to the attention function, PyTorchâ€™s matrix operations work independently on each head automatically.\nLetâ€™s trace through it:\nQ shape: (batch=2, h=8, seq_len=10, d_k=64)\nK shape: (batch=2, h=8, seq_len=10, d_k=64)\n\nQ @ K.transpose(-2,-1)\nThe @ operator does matrix multiplication on the last two dimensions only. The first dimensions (batch, h) are treated as â€œbatch dimensionsâ€ - so it runs 2Ã—8=16 separate attention operations in parallel!\nIn other words: - You have 8 heads - Each head processes its slice: Q[:, 0, :, :], Q[:, 1, :, :], â€¦ Q[:, 7, :, :] - The matrix operations happen independently for each head - All in one efficient operation!\nDoes this click now? The reshaping sets up the dimensions so that one function call runs all heads in parallel!\nOkay, this makes sense and clicks. No wonder attention (incl multiheaded attention) is so much faster than previous methods like R and N and LSTM.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly! Youâ€™ve hit on one of the key innovations! ğŸ‰\nWhy Transformers are faster:\n\nRNNs/LSTMs: Process sequentially (word 1 â†’ word 2 â†’ word 3â€¦). Canâ€™t parallelize across the sequence.\nTransformers: All positions processed at once! The entire attention matrix is computed in parallel.\n\nPlus, with multi-head attention, all 8 heads also run in parallel thanks to the clever tensor reshaping you just understood.\nThis parallelization is what makes Transformers scale so well with modern GPUs!\nNow, ready to finish Step 4? You need to concatenate the heads back together. Remember: - Current shape: (batch_size, h, seq_len, d_k) - Target shape: (batch_size, seq_len, d_model)\nWhat operations do you need? (Hint: reverse of what you did earlier)\n\ndef forward(self, query, key, value, mask=None):\n    # Step 1: Project query, key, value using the first 3 linear layers\n    Q = self.linear_layers[0](query)\n    K = self.linear_layers[1](key)\n    V = self.linear_layers[2](value)\n    # Step 2: Reshape to split into h heads\n    batch_size = Q.size(0)\n    seq_len = Q.size(1)\n    Q = Q.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    K = K.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    V = V.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    # Step 3: Apply attention\n    if mask is not None:\n        mask = mask.unsqueeze(1)\n    x = attention(Q,K,V, self.dropout, mask)\n\n    # Step 4: Concatenate heads back together\n    x = x.transpose(1,2).view(batch_size,seq_len, -1)\n    # Step 5: Apply final linear projection\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Step 4 is correct! âœ“\nYouâ€™ve done: - .transpose(1,2) to swap h and seq_len back: (batch, h, seq_len, d_k) â†’ (batch, seq_len, h, d_k) - .view(batch_size, seq_len, -1) to merge the last two dims: (batch, seq_len, h, d_k) â†’ (batch, seq_len, d_model)\nThe -1 automatically calculates h Ã— d_k = d_model. Nice!\nNow for Step 5: Apply final linear projection\nYou need to pass x through the 4th linear layer (the output projection W^O). This mixes information across all the concatenated heads.\nCan you add that final step?\n\ndef forward(self, query, key, value, mask=None):\n    # Step 1: Project query, key, value using the first 3 linear layers\n    Q = self.linear_layers[0](query)\n    K = self.linear_layers[1](key)\n    V = self.linear_layers[2](value)\n    # Step 2: Reshape to split into h heads\n    batch_size = Q.size(0)\n    seq_len = Q.size(1)\n    Q = Q.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    K = K.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    V = V.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n    # Step 3: Apply attention\n    if mask is not None:\n        mask = mask.unsqueeze(1)\n    x = attention(Q,K,V, self.dropout, mask)\n    # Step 4: Concatenate heads back together\n    x = x.transpose(1,2).view(batch_size,seq_len, -1)\n    # Step 5: Apply final linear projection\n    output = self.linear_layers[3](x)\n    return output\n\nAm I successful now?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent work! Your Multi-Head Attention implementation is complete! ğŸ‰\nYouâ€™ve successfully implemented: - âœ“ Projecting to Q, K, V - âœ“ Splitting into multiple heads - âœ“ Running attention on all heads in parallel - âœ“ Concatenating heads back - âœ“ Final output projection\nNow letâ€™s test it to make sure it works! Try this:\n# Test with realistic dimensions\nmha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = mha.forward(x, x, x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this and see if it works! What do you get?\nWait a minute, shouldnâ€™t we put the class and the function together?\n\n\nğŸ¤–ReplyğŸ¤–\nGood catch! Yes, you need to add the forward method inside the MultiHeadedAttention class!\nRight now you have the class definition with __init__, but the forward method is separate. You need to indent the forward method so itâ€™s part of the class.\nIt should look like:\nclass MultiHeadedAttention(torch.nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        # ... your init code ...\n    \n    def forward(self, query, key, value, mask=None):\n        # ... your forward code ...\nCan you combine them properly?\n\nclass MultiHeadedAttention(torch.nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0  # d_model must be divisible by h\n        \n        self.d_k = d_model // h\n        self.h = h\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.attn=None\n        \n        # Create 4 linear layers (Q, K, V projections + final output)\n        # Hint: you can use a list or create them individually\n        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(d_model, d_model) for x in range(4)])\n    \n    def forward(self, query, key, value, mask=None):\n        # Step 1: Project query, key, value using the first 3 linear layers\n        Q = self.linear_layers[0](query)\n        K = self.linear_layers[1](key)\n        V = self.linear_layers[2](value)\n        # Step 2: Reshape to split into h heads\n        batch_size = Q.size(0)\n        seq_len = Q.size(1)\n        Q = Q.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n        K = K.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n        V = V.view(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n        # Step 3: Apply attention\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        x = attention(Q,K,V, self.dropout, mask)\n        # Step 4: Concatenate heads back together\n        x = x.transpose(1,2).view(batch_size,seq_len, -1)\n        # Step 5: Apply final linear projection\n        output = self.linear_layers[3](x)\n        return output\n\nall good?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your MultiHeadedAttention class is now complete and properly structured! âœ“\nNow letâ€™s test it to make sure everything works:\n# Test with realistic dimensions\nmha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = mha.forward(x, x, x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this and let me know if it works! Does the output shape match what you expect?\n\n# Test with realistic dimensions\nmha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = mha.forward(x, x, x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[1], line 5\n      2 mha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n      3 x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n----&gt; 5 output = mha.forward(x, x, x, mask=None)\n      6 print(\"Output shape:\", output.shape)\n      7 print(\"Expected:\", (2, 10, 512))\n\nCell In[1], line 31, in MultiHeadedAttention.forward(self, query, key, value, mask)\n     29 x = attention(Q,K,V, self.dropout, mask)\n     30 # Step 4: Concatenate heads back together\n---&gt; 31 x = x.transpose(1,2).view(batch_size,seq_len, -1)\n     32 # Step 5: Apply final linear projection\n     33 output = self.linear_layers[3](x)\n\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n\n\n\nno, whyyyy?\n\n\nğŸ¤–ReplyğŸ¤–\nDonâ€™t worry, this is a common issue! The error is telling you exactly what to do.\nThe problem: After .transpose(), the tensor is no longer â€œcontiguousâ€ in memory, so .view() doesnâ€™t work.\nThe fix: Use .contiguous() before .view(), or just use .reshape() instead:\nChange this line:\nx = x.transpose(1,2).view(batch_size, seq_len, -1)\nTo either:\nx = x.transpose(1,2).contiguous().view(batch_size, seq_len, -1)\nOr simply:\nx = x.transpose(1,2).reshape(batch_size, seq_len, -1)\n.reshape() handles the contiguity automatically! Try updating your code and run the test again.\nI donâ€™t understand, what do you mean by â€œcontiguous?â€ What exactly is the issue?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me explain contiguous memory.\nWhat is contiguous?\nContiguous means the tensorâ€™s elements are stored in a single, continuous block of memory in the order youâ€™d expect.\nExample:\nOriginal tensor: [[1, 2], [3, 4]] stored in memory as: [1, 2, 3, 4] âœ“ contiguous\nAfter transpose: [[1, 3], [2, 4]] but memory is still: [1, 2, 3, 4]\nPyTorch just changes how it interprets the memory (using strides), but doesnâ€™t actually move the data. So now itâ€™s not contiguous anymore!\nWhy does .view() care?\n.view() requires contiguous memory because itâ€™s a â€œzero-copyâ€ operation - it just reinterprets the existing memory layout. If the memory isnâ€™t laid out right, it canâ€™t do that.\nSolutions: - .contiguous() - actually rearranges memory to be contiguous (makes a copy) - .reshape() - automatically calls .contiguous() if needed\nDoes this make sense? Try using .reshape() instead of .view() and it should work!\n\n\nFinal MultiHeaded Attention Class\n\n\n\nMultiHeadedAttention\n\n MultiHeadedAttention (h, d_model, dropout=0.1)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nIs this better?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Now letâ€™s test it again:\n# Test with realistic dimensions\nmha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = mha.forward(x, x, x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this and see if it works now!\n\n# Test with realistic dimensions\nmha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = mha.forward(x, x, x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nAwesome. Convert this into unit test. Assume Iâ€™m using NB dev.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Hereâ€™s your Multi-Head Attention as a unit test for nbdev:\n#| export\nclass MultiHeadedAttention(torch.nn.Module):\n    \"Multi-Head Attention mechanism\"\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0  # d_model must be divisible by h\n        \n        self.d_k = d_model // h\n        self.h = h\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.attn = None\n        \n        # Create 4 linear layers (Q, K, V projections + final output)\n        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(d_model, d_model) for _ in range(4)])\n    \n    def forward(self, query, key, value, mask=None):\n        # Step 1: Project query, key, value using the first 3 linear layers\n        Q = self.linear_layers[0](query)\n        K = self.linear_layers[1](key)\n        V = self.linear_layers[2](value)\n        # Step 2: Reshape to split into h heads\n        batch_size = Q.size(0)\n        seq_len = Q.size(1)\n        Q = Q.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n        K = K.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n        V = V.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\n        # Step 3: Apply attention\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        x = attention(Q, K, V, self.dropout, mask)\n        # Step 4: Concatenate heads back together\n        x = x.transpose(1,2).reshape(batch_size, seq_len, -1)\n        # Step 5: Apply final linear projection\n        output = self.linear_layers[3](x)\n        return output\n#| test\ndef test_multihead_attention():\n    \"Test Multi-Head Attention with realistic dimensions\"\n    mha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n    \n    output = mha.forward(x, x, x, mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nGreat work! Youâ€™ve successfully implemented Multi-Head Attention from scratch! ğŸ‰\nWhat would you like to tackle next? Position-wise Feed-Forward Networks, or something else from the architecture?\n\n\nUnit Test\n\ndef test_multihead_attention():\n    \"Test Multi-Head Attention with realistic dimensions\"\n    mha = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n    \n    output = mha.forward(x, x, x, mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n\n\ntest_multihead_attention()\n\nGreat, Iâ€™ve exported using NB Dev, now letâ€™s move on.\n\nğŸ¤–ReplyğŸ¤–\nAwesome! ğŸ‰ Your Multi-Head Attention is working perfectly!\nYouâ€™ve now implemented two major components: 1. âœ“ Scaled Dot-Product Attention 2. âœ“ Multi-Head Attention\nWhatâ€™s next?\nLooking at the Transformer architecture, the main components left are:\n\nPosition-wise Feed-Forward Network - a simple 2-layer MLP applied to each position\nPositional Encoding - adds position information to embeddings\nLayer Normalization & Residual Connections - for stable training\nEncoder Layer - combines attention + feed-forward\nDecoder Layer - similar but with masked attention + cross-attention\nFull Encoder/Decoder stacks\n\nWhich would you like to tackle next? Iâ€™d suggest the Feed-Forward Network since itâ€™s simpler and will give you a quick win!\nI just have one question. So, all these linear layers we are using During multi-headed attention, why arenâ€™t there any non-linearities like ReLU at the end of these linear layers?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent question! This is a really important design choice.\nWhy no non-linearity in the projections?\nThe Q, K, V projections are meant to be linear transformations of the input into different subspaces. Adding non-linearities would change the nature of the dot-product attention mechanism.\nThink about it: attention computes similarity via dot products (QÂ·K^T). If you applied ReLU before this, youâ€™d be computing similarity between â€œrectifiedâ€ representations, which doesnâ€™t have the same mathematical properties.\nWhere DO we use non-linearities?\nThe non-linearity (ReLU) comes in the Feed-Forward Network that follows the attention layer! Thatâ€™s where the model gets its non-linear transformation capacity.\nThe pattern in each layer: 1. Multi-Head Attention (linear projections only) 2. Add & Norm (residual connection) 3. Feed-Forward Network (has ReLU!) â† non-linearity here 4. Add & Norm\nSo the model does have non-linearities, just not inside the attention projections themselves.\nDoes this distinction make sense?",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#positionwisefeedforward",
    "href": "attention_is_all_you_need.html#positionwisefeedforward",
    "title": "Simple 2x2 example",
    "section": "PositionwiseFeedForward",
    "text": "PositionwiseFeedForward\nYes, this makes sense. Letâ€™s move on to the next topic. Make sure weâ€™re sticking to the code which is given in the beginning.\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s move on to the Position-wise Feed-Forward Network.\nLooking at the paper (Section 3.3), this is a simple component:\nFFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚\nItâ€™s just: 1. A linear layer that expands dimensions (d_model â†’ d_ff) 2. ReLU activation 3. Another linear layer that projects back (d_ff â†’ d_model)\nTypically d_ff = 2048 when d_model = 512 (4x expansion).\nTry implementing the class:\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        # What do you need here?\n        \n    def forward(self, x):\n        # What's the sequence of operations?\nWhat layers do you need in __init__? Give it a try!\n\nclass PositionwiseFeedForward(torch.nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        # What do you need here?\n        self.linear_layer = torch.nn.Linear(d_model, d_ff)\n        self.relu = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.output_layer = torch.nn.Linear(d_ff, d_model)\n        \n    def forward(self, x):\n        # What's the sequence of operations?\n        pass\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Your __init__ is perfect! âœ“\nYouâ€™ve got all the pieces: - âœ“ First linear layer (d_model â†’ d_ff) - âœ“ ReLU activation - âœ“ Dropout - âœ“ Second linear layer (d_ff â†’ d_model)\nNow for the forward method - whatâ€™s the sequence of operations?\nApply them in order: linear â†’ relu â†’ dropout â†’ linear\nGive it a try!\n\n\nFinal PositionwiseFeedForward Class\n\n\n\nPositionwiseFeedForward\n\n PositionwiseFeedForward (d_model, d_ff, dropout=0.1)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nShould be fine, right?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Feed-Forward Network is complete and correct! âœ“\nThe sequence is exactly right: - âœ“ Expand with first linear layer - âœ“ Apply ReLU non-linearity - âœ“ Apply dropout for regularization - âœ“ Project back with second linear layer\nLetâ€™s test it:\nffn = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = ffn(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this to verify it works, then we can move on to the next component!\n\nffn = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = ffn(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nAssume we are using NB dev. Convert this into a unit test.\n\n\nğŸ¤–ReplyğŸ¤–\nHereâ€™s your Position-wise Feed-Forward Network as a unit test for nbdev:\n#| export\nclass PositionwiseFeedForward(torch.nn.Module):\n    \"Position-wise Feed-Forward Network\"\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear_layer = torch.nn.Linear(d_model, d_ff)\n        self.relu = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.output_layer = torch.nn.Linear(d_ff, d_model)\n        \n    def forward(self, x):\n        x = self.linear_layer(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.output_layer(x)\n        return x\n#| test\ndef test_positionwise_feedforward():\n    \"Test Position-wise Feed-Forward Network\"\n    ffn = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n    \n    output = ffn(x)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nReady to move on to the next component?\n\n\nUnit Test\n\ndef test_positionwise_feedforward():\n    \"Test Position-wise Feed-Forward Network\"\n    ffn = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n    \n    output = ffn(x)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n\n\ntest_positionwise_feedforward()\n\nWhy is it called position-wise feedforward?\n\nğŸ¤–ReplyğŸ¤–\nGreat question!\nâ€œPosition-wiseâ€ means the same feed-forward network is applied to each position independently.\nThink of it this way: - You have a sequence: [word1, word2, word3, ...] - Each word has a vector of size d_model=512 - The FFN processes each wordâ€™s vector separately - word1â€™s vector doesnâ€™t interact with word2â€™s vector\nItâ€™s like applying the same 2-layer neural network to each position in the sequence independently. The same weights are shared across all positions, but each position is processed on its own.\nContrast with attention: Attention mixes information across positions (word1 looks at word2, word3, etc.). The FFN does not mix across positions - it just transforms each positionâ€™s representation individually.\nDoes this clarify why itâ€™s called â€œposition-wiseâ€?\nOkay, that makes it clear. Letâ€™s move on now.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Youâ€™ve now completed: 1. âœ“ Scaled Dot-Product Attention 2. âœ“ Multi-Head Attention 3. âœ“ Position-wise Feed-Forward Network\nWhatâ€™s next?\nThe remaining key components are:\n\nEmbeddings - convert tokens to vectors\nPositional Encoding - add position information (since attention has no inherent notion of order)\nLayer Normalization - for stable training\nResidual Connections - skip connections around sub-layers\nEncoder Layer - combines attention + FFN with residuals and norms\nDecoder Layer - similar but with masked self-attention + cross-attention\n\nIâ€™d suggest Positional Encoding next - itâ€™s a unique and interesting part of the Transformer. It explains how the model knows the order of words.\nWant to tackle that? Or would you prefer a different component?",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#positional-encoding-1",
    "href": "attention_is_all_you_need.html#positional-encoding-1",
    "title": "Simple 2x2 example",
    "section": "Positional Encoding",
    "text": "Positional Encoding\nOkay, Iâ€™m down for positional encoding. Letâ€™s do it.\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s tackle Positional Encoding.\nWhy do we need it?\nAttention has no inherent sense of order - it treats the sequence like a set. â€œI love pizzaâ€ and â€œpizza love Iâ€ would look the same! Positional encoding adds information about where each word is in the sequence.\nHow does it work?\nThe paper uses sine and cosine functions of different frequencies:\n\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n\nWhere: - pos = position in sequence (0, 1, 2, â€¦) - i = dimension index (0, 1, 2, â€¦, d_model/2)\nThe class structure:\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        # Create the positional encoding matrix\n        # Store it as a buffer (not a parameter)\n        \n    def forward(self, x):\n        # Add positional encoding to input x\nBefore coding, do you understand why they use sin/cos functions? Want me to explain the intuition?\nI think they use sine and cos because sine and cos have different peaks. For example, sin is 1 while cos is 0.\n\n\nğŸ¤–ReplyğŸ¤–\nThatâ€™s part of it! But thereâ€™s more to why sin/cos are special here.\nKey properties they wanted:\n\nUnique encoding for each position - different positions get different patterns\nRelative position information - the model can learn â€œword A is 3 positions before word Bâ€\nExtrapolation - can handle sequences longer than seen in training\n\nWhy sin/cos specifically?\nFor any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos). This is because of the trigonometric identity:\nsin(a + b) = sin(a)cos(b) + cos(a)sin(b)\nThis means the model can easily learn to attend by relative positions!\nDifferent frequencies: Each dimension uses a different frequency (from 2Ï€ to 10000Â·2Ï€), so different dimensions capture patterns at different scales - some for nearby words, some for distant words.\nDoes this deeper intuition make sense? Ready to start implementing it?\nNo, Iâ€™m still not getting it. What Iâ€™m seeing from the formula is that for even dim weâ€™re using sine, and for odd dim weâ€™re using cosine. Secondly, what I am getting is the frequency changes as the dimension index changes. It goes from lower frequency to higher frequency. Is what Iâ€™m saying making sense? Am I getting the correct intuition?\n\n\nğŸ¤–ReplyğŸ¤–\nYes! Youâ€™re getting it exactly right! ğŸ‰\nWhat you understand correctly:\n\nâœ“ Even dimensions use sine, odd dimensions use cosine - this gives you pairs of (sin, cos) at the same frequency\nâœ“ Frequency changes with dimension - but actually it goes from higher to lower frequency as i increases (because we divide by larger numbers: 10000^(2i/d_model) gets bigger)\n\nWhy does frequency matter?\n\nHigh frequency (early dimensions): Changes quickly, good for capturing nearby positions (position 1 vs 2 vs 3)\nLow frequency (later dimensions): Changes slowly, good for capturing distant positions (position 10 vs 50 vs 100)\n\nSo different dimensions encode position information at different scales!\nThe sin/cos pairing: Using both at the same frequency helps the model learn relative positions through those trig identities I mentioned.\nDoes this click now? Ready to implement it?\nHow are early dimensions high frequency and later low frequency? Give a numerical example.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me show you with numbers.\nThe formula for frequency:\ndiv_term = 10000^(2i/d_model)\nLetâ€™s say d_model = 512:\nEarly dimensions (i=0):\n10000^(2*0/512) = 10000^0 = 1\nfrequency = 1/1 = 1  (HIGH frequency - changes fast!)\nMiddle dimensions (i=64):\n10000^(2*64/512) = 10000^0.25 â‰ˆ 10\nfrequency = 1/10 = 0.1  (medium frequency)\nLater dimensions (i=255):\n10000^(2*255/512) = 10000^0.996 â‰ˆ 6310\nfrequency = 1/6310 â‰ˆ 0.00016  (LOW frequency - changes slowly!)\nWhat this means: - Dimension 0: sin(pos/1) changes every position - Dimension 128: sin(pos/10) changes every ~10 positions\n- Dimension 510: sin(pos/6310) barely changes across the sequence\nDoes this numerical example clarify how frequency decreases as i increases?\nOkay, before jumping into the implementation, letâ€™s do some visualisation. Give me some visualisation code without thinking.\n\n\nğŸ¤–ReplyğŸ¤–\nimport matplotlib.pyplot as plt\n\n# Create positional encoding for visualization\nd_model = 64\nmax_len = 100\n\n# Compute positional encodings\npe = torch.zeros(max_len, d_model)\nposition = torch.arange(0, max_len).unsqueeze(1)\ndiv_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n\npe[:, 0::2] = torch.sin(position * div_term)\npe[:, 1::2] = torch.cos(position * div_term)\n\n# Plot\nplt.figure(figsize=(15, 5))\nplt.imshow(pe.T, cmap='RdBu', aspect='auto')\nplt.xlabel('Position')\nplt.ylabel('Dimension')\nplt.colorbar()\nplt.title('Positional Encoding Heatmap')\nplt.show()\n\n# Plot specific dimensions\nplt.figure(figsize=(15, 4))\nfor dim in [0, 2, 10, 30]:\n    plt.plot(pe[:, dim], label=f'dim {dim}')\nplt.xlabel('Position')\nplt.ylabel('Encoding Value')\nplt.legend()\nplt.title('Positional Encoding for Different Dimensions')\nplt.show()\nRun this to see how the encodings look!\nOk, letâ€™s implement now and much clearer.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s implement Positional Encoding.\nStep 1: Create the positional encoding matrix in __init__\nYou need to: 1. Create a matrix pe of shape (max_len, d_model) filled with zeros 2. Create a position vector: [0, 1, 2, â€¦, max_len-1] 3. Calculate div_term using the formula 4. Fill in the even dimensions with sine 5. Fill in the odd dimensions with cosine\nTry starting with the first 3 steps:\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n        \n        # Create position vector - try this yourself!\n        # Calculate div_term - try this yourself!\nCan you create the position vector and calculate div_term? Remember the formulas we discussed!\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n\n        half_range = torch.range(d_model//2)\n        \n        # Create position vector - try this yourself!\n        cosine = torch.cos(pos/ (1000**(2*half_range/d_model)) )\n        sin = torch.sin(pos/ (1000**(2*half_range/d_model)) )\n        pe[::2] = cosine\n        pe[1::2] = sin\n        # Calculate div_term - try this yourself!\n\nAm I doing well so far?\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re on the right track with the concept, but there are a few issues to fix:\n\npos is not defined - you need to create a position vector first: torch.arange(0, max_len)\nhalf_range - use torch.arange(0, d_model, 2) to get even indices\nThe indexing pe[::2] - this wonâ€™t work correctly. You need to fill columns (dimensions), not rows. Use pe[:, 0::2] for even dims and pe[:, 1::2] for odd dims\nBroadcasting - pos needs to be reshaped to broadcast correctly with div_term\n\nTry fixing step 1 first - create the position vector:\nposition = torch.arange(0, max_len).unsqueeze(1)  # shape: (max_len, 1)\nDoes this make sense? Want to try the next step?\nWhatâ€™s the difference between A-range and range?\n\n\nğŸ¤–ReplyğŸ¤–\nGood question!\ntorch.range() (deprecated): - Older function, not recommended - Includes the end value - torch.range(0, 5) gives [0, 1, 2, 3, 4, 5] (6 elements!)\ntorch.arange() (use this!): - Modern version, works like Pythonâ€™s range() - Excludes the end value - torch.arange(0, 5) gives [0, 1, 2, 3, 4] (5 elements)\nSo always use torch.arange() - itâ€™s more predictable and matches Python conventions!\nReady to continue with the implementation?\nWhy are we unsqueezing?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question!\nWhy unsqueeze?\nWe need the shapes to broadcast correctly for the calculation.\nThe shapes: - position: We want shape (max_len, 1) - each row is a position - div_term: Will have shape (d_model/2,) - different frequencies\nWhen we multiply them:\nposition * div_term\nBroadcasting gives us shape (max_len, d_model/2) - every position combined with every frequency!\nWithout unsqueeze: - position would be shape (max_len,) - Multiplication wouldnâ€™t broadcast the way we want\nVisual example:\nposition (100, 1):        div_term (256,):\n[[0],                     [1.0, 0.5, 0.25, ...]\n [1],\n [2],                     Result (100, 256):\n ...]                     Each pos Ã— each frequency\nDoes this clarify why we need that extra dimension?\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n\n        half_range = torch.range(d_model//2)\n        \n        # Create position vector - try this yourself!\n        position = torch.arange(0, max_len).unsqueeze(1)\n        # Calculate div_term - try this yourself!\n        div_term = 10000**(half_range/d_model)\n\nAre we doing better?\n\n\nğŸ¤–ReplyğŸ¤–\nGetting closer! A few more fixes needed:\n\nhalf_range: Should be torch.arange(0, d_model, 2) not torch.range(d_model//2). You want the even indices: [0, 2, 4, 6, â€¦]\ndiv_term formula: You have it backwards! It should be dividing, not multiplying. Remember: we want 10000^(2i/d_model), which means:\ndiv_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\nThis computes the exponential in a numerically stable way.\n\nTry fixing these two things. Does the div_term formula make sense, or do you want me to explain why we use exp and log?\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n\n        even_indices = torch.arange(0, d_model, 2)\n        \n        # Create position vector - try this yourself!\n        position = torch.arange(0, max_len).unsqueeze(1)\n        # Calculate div_term - try this yourself!\n        div_term = torch.exp(-even_indices*(torch.log(10000))/d_model)\n\nAll good now?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your position vector and div_term are now correct! âœ“\nNow you need to fill in the pe matrix with sine and cosine values:\npe[:, 0::2] = torch.sin(position * div_term)\npe[:, 1::2] = torch.cos(position * div_term)\nThis fills: - Even columns (0, 2, 4, â€¦) with sine - Odd columns (1, 3, 5, â€¦) with cosine\nThen you need to: 1. Add a batch dimension: pe = pe.unsqueeze(0) 2. Register it as a buffer (not a trainable parameter): self.register_buffer('pe', pe)\nCan you add these steps to complete the __init__ method?\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n\n        even_indices = torch.arange(0, d_model, 2)\n        \n        # Create position vector - try this yourself!\n        position = torch.arange(0, max_len).unsqueeze(1)\n        # Calculate div_term - try this yourself!\n        div_term = torch.exp(-even_indices*(torch.log(10000))/d_model)\n\n        pe[:, ::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        return pe\n\nAwesome, thatâ€™s great.\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost there! But you canâ€™t return from __init__ - itâ€™s a constructor method.\nInstead, you need to: 1. Add a batch dimension to pe 2. Register it as a buffer so itâ€™s saved with the model but not trained\nAdd these two lines at the end of __init__:\npe = pe.unsqueeze(0)  # Add batch dimension: (1, max_len, d_model)\nself.register_buffer('pe', pe)  # Save as buffer, not parameter\nThen youâ€™ll need to implement the forward method. Can you add these lines first?\nLet me guess; we are unsqueezing to accommodate batch size, arenâ€™t we?\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! ğŸ‰\nThe pe matrix is currently shape (max_len, d_model), but your input will have shape (batch_size, seq_len, d_model).\nBy unsqueezing to (1, max_len, d_model), the batch dimension of size 1 will broadcast across whatever batch_size you have in your input.\nYouâ€™re really getting the hang of these dimension manipulations!\nNow add those two lines to your code, then we can implement the forward method. Ready?\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n\n        even_indices = torch.arange(0, d_model, 2)\n        \n        # Create position vector - try this yourself!\n        position = torch.arange(0, max_len).unsqueeze(1)\n        # Calculate div_term - try this yourself!\n        div_term = torch.exp(-even_indices*(torch.log(10000))/d_model)\n\n        pe[:, ::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\nAlso, we are saving PE as a buffer, as that does not change at all.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly! ğŸ’¯\nWhy use register_buffer instead of a parameter?\n\nParameters (created with nn.Parameter): Are trained/updated during backpropagation\nBuffers (created with register_buffer): Are part of the model state but NOT trained\n\nSince positional encodings are fixed (computed once using sin/cos, never updated), they should be a buffer, not a parameter!\nThis also ensures pe gets moved to GPU when you call model.to('cuda') and gets saved/loaded with the model.\nNow letâ€™s implement the forward method. What should it do? 1. Take input x of shape (batch, seq_len, d_model) 2. Add the positional encoding for the first seq_len positions 3. Apply dropout 4. Return the result\nCan you try writing the forward method?\n\n\nFinal PositionalEncoding\n\n\n\nPositionalEncoding\n\n PositionalEncoding (d_model, dropout, max_len=5000)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nIs this good?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Positional Encoding implementation is complete and correct! âœ“\nYouâ€™ve got: - âœ“ Extract seq_len from input - âœ“ Slice the positional encoding to match: self.pe[:, :seq_len, :] - âœ“ Set requires_grad_(False) to ensure itâ€™s not trained - âœ“ Add it to the input - âœ“ Apply dropout\nLetâ€™s test it:\npe = PositionalEncoding(d_model=512, dropout=0.1, max_len=100)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = pe(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this to verify it works!\n\npe = PositionalEncoding(d_model=512, dropout=0.1, max_len=100)\nx = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n\noutput = pe(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nAssume we are using NB Dev. Convert this into a unit test.\n\n\nğŸ¤–ReplyğŸ¤–\nHereâ€™s your Positional Encoding as a unit test for nbdev:\n#| export\nclass PositionalEncoding(torch.nn.Module):\n    \"Positional Encoding using sine and cosine functions\"\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(p=dropout)\n        \n        # Create pe matrix\n        pe = torch.zeros(max_len, d_model)\n        even_indices = torch.arange(0, d_model, 2)\n        \n        # Create position vector\n        position = torch.arange(0, max_len).unsqueeze(1)\n        # Calculate div_term\n        div_term = torch.exp(-even_indices*(torch.log(torch.tensor(10000.0))/d_model))\n        \n        pe[:, ::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        seq_len = x.size(1)\n        x = x + self.pe[:, :seq_len, :].requires_grad_(False)\n        return self.dropout(x)\n#| test\ndef test_positional_encoding():\n    \"Test Positional Encoding\"\n    pe = PositionalEncoding(d_model=512, dropout=0.1, max_len=100)\n    x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n    \n    output = pe(x)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nGreat progress! Ready to move on to the next component?\n\n\nUnit Test\n\ndef test_positional_encoding():\n    \"Test Positional Encoding\"\n    pe = PositionalEncoding(d_model=512, dropout=0.1, max_len=100)\n    x = torch.randn(2, 10, 512)  # batch=2, seq_len=10, d_model=512\n    \n    output = pe(x)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n\n\ntest_positional_encoding()\n\nThatâ€™s amazing, letâ€™s go on to the next block.\n\nğŸ¤–ReplyğŸ¤–\nExcellent work! Youâ€™ve now completed the core building blocks: 1. âœ“ Scaled Dot-Product Attention 2. âœ“ Multi-Head Attention 3. âœ“ Position-wise Feed-Forward Network 4. âœ“ Positional Encoding\nWhatâ€™s next?\nNow we need to build the Encoder Layer - this combines the components youâ€™ve built into a complete encoder block.\nAn Encoder Layer contains: 1. Multi-Head Self-Attention 2. Add & Norm (residual connection + layer normalization) 3. Feed-Forward Network 4. Add & Norm again\nBefore we code, do you understand what residual connections and layer normalization are? Should I explain those concepts first, or do you want to dive into the implementation?",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#encoder-layer",
    "href": "attention_is_all_you_need.html#encoder-layer",
    "title": "Simple 2x2 example",
    "section": "Encoder Layer",
    "text": "Encoder Layer\nYeah, you can test me on these concepts. I somewhat know them.\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s test your understanding:\nQuestion 1: Residual Connections\nWhat is a residual connection, and why do we use them in deep networks? Can you write the simple equation for a residual connection?\nQuestion 2: Layer Normalization\nWhat does layer normalization do? How is it different from batch normalization?\nGive it a shot - explain in your own words!\nq1 Basically a residual connection is you after you do an operation on an input, you also bring the input forward bypassing the operation. so, output = x + sublayer(x)\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Thatâ€™s exactly right! âœ“\nResidual connection: output = x + sublayer(x)\nYouâ€™ve got the key insight - the input â€œskipsâ€ around the operation and gets added back. This helps with: - Gradient flow during backpropagation (prevents vanishing gradients) - Allows the network to learn identity mappings easily - Makes training very deep networks possible\nNow for Question 2:\nWhat does layer normalization do, and how does it work? (Hint: think about computing mean and standard deviation)\nSo basically, across all the dimensions, we take the mean and standard deviation, and we normalise the outputs.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! âœ“\nLayer Normalization: - Computes mean and std across all dimensions (features) for each example - Normalizes: (x - mean) / std - Then applies learned scale (Î³) and shift (Î²) parameters\nKey difference from Batch Normalization: - Batch Norm: normalizes across the batch dimension (all examples) - Layer Norm: normalizes across the feature dimension (for each example independently)\nLayer Norm works better for sequences because sequence lengths can vary, making batch statistics unreliable.\nNow you understand both concepts! Ready to implement the Encoder Layer that uses these? It will combine: - Multi-Head Attention - Residual + Layer Norm - Feed-Forward - Residual + Layer Norm again\nWant to start coding?\nlets code!!!\n\n\nğŸ¤–ReplyğŸ¤–\nAwesome! Letâ€™s build the Encoder Layer!\nFirst, letâ€™s think about what we need:\n\nIn __init__: Store the components\n\nMulti-Head Attention module\nFeed-Forward module\nLayer Norm modules (how many do we need?)\nStore the size (d_model)\n\nIn forward: Apply the operations with residual connections\n\nBefore coding, a question: How many Layer Norm modules do we need? (Hint: think about how many residual connections we have)\nOnce you answer, try writing the __init__ method skeleton:\nclass EncoderLayer(nn.Module):\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super().__init__()\n        # What do we need to store?\nGive it a try!\nFor layer normalisation, you will need one layer norm after the position-wise feed forward and another layer norm after the multi-head attention.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! You need 2 Layer Norm modules - one after each sub-layer (attention and feed-forward).\nNow try implementing the __init__ method. You need to store: - self.size (d_model) - self.self_attn (the multi-head attention module passed in) - self.feed_forward (the FFN module passed in) - 2 Layer Norm modules (use nn.LayerNorm(size)) - Dropout\nGive it a try! What does your __init__ look like?\n\nclass EncoderLayer(torch.nn.Module):\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super().__init__()\n        # What do we need to store?\n        self.size = size\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.layer_norms = torch.nn.ModuleList([torch.nn.LayerNorm(size) for x in range(2)])\n        self.dropout = dropout\n\nIs this a cognizant with the original code?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! Looking at the original code pattern, thereâ€™s one difference:\nInstead of storing dropout as a value, you should create a Dropout layer:\nself.dropout = nn.Dropout(dropout)\nBut actually, looking at the original code more carefully, they use a SublayerConnection module that handles both the residual connection AND layer norm together. This is a cleaner pattern.\nDo you want to: 1. Option A: Implement SublayerConnection class first (handles residual + norm), then use it in EncoderLayer 2. Option B: Continue with your current approach (manual residual connections in forward)\nThe original code uses Option A. Which would you prefer?\n\n\nSublayer Connection\nWhere is this mentioned in the original code? Tell me.\n\nğŸ¤–ReplyğŸ¤–\nLooking at the original code you provided at the beginning (the Annotated Transformer), you can find the SublayerConnection class. Let me point you to it:\nItâ€™s defined in the section about Encoder and Decoder Stacks. Hereâ€™s where it appears:\nclass SublayerConnection(nn.Module):\n    \"\"\"\n    A residual connection followed by a layer norm.\n    Note for code simplicity the norm is first as opposed to last.\n    \"\"\"\n\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        \"Apply residual connection to any sublayer with the same size.\"\n        return x + self.dropout(sublayer(self.norm(x)))\nAnd then EncoderLayer uses it:\nself.sublayer = clones(SublayerConnection(size, dropout), 2)\nSo they create 2 SublayerConnection modules. Do you want to implement SublayerConnection first, or stick with your current approach?\nWhat should we do, man?\n\n\nğŸ¤–ReplyğŸ¤–\nGood question! Hereâ€™s my recommendation:\nFollow the original code structure - implement SublayerConnection first. Hereâ€™s why:\n\nItâ€™s cleaner and more modular\nYouâ€™ll reuse it in the Decoder Layer too\nIt matches the paperâ€™s approach (norm â†’ sublayer â†’ dropout â†’ residual)\n\nThe SublayerConnection is simple - it just wraps any sublayer with: - Layer normalization (applied first) - The sublayer operation - Dropout - Residual connection\nWant to implement SublayerConnection first? Itâ€™s a small class - should only take a few minutes!\nok\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s implement SublayerConnection.\nLooking at the original code, it needs:\nIn __init__: - Store a LayerNorm module - Store a Dropout module\nIn forward(self, x, sublayer): - Apply layer norm to x - Pass normalized x through the sublayer function - Apply dropout - Add the residual connection (original x)\nThe pattern is: x + dropout(sublayer(norm(x)))\nTry implementing it:\nclass SublayerConnection(nn.Module):\n    \"A residual connection followed by a layer norm\"\n    def __init__(self, size, dropout):\n        super().__init__()\n        # What do you need here?\n        \n    def forward(self, x, sublayer):\n        # What's the operation?\nGive it a shot!\n\n\n\nFinal SublayerConnection Function\n\n\n\nSublayerConnection\n\n SublayerConnection (size, dropout)\n\nA residual connection followed by a layer norm\nis this good? always refer to original code\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your SublayerConnection is correct and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Layer norm applied first - âœ“ Sublayer operation - âœ“ Dropout - âœ“ Residual connection (x + y)\nNow letâ€™s go back to the EncoderLayer and use SublayerConnection.\nUpdate your EncoderLayer.__init__ to use:\nself.sublayer = nn.ModuleList([SublayerConnection(size, dropout) for _ in range(2)])\nInstead of the separate layer_norms. Can you update your EncoderLayer code?\n\nself.sublayer = nn.ModuleList([SublayerConnection(size, dropout) for _ in range(2)])\n\nlike this?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your EncoderLayer.__init__ is now correct! âœ“\nNow letâ€™s implement the forward method. Looking at the original code, you need to:\n\nApply first sublayer connection with self-attention\nApply second sublayer connection with feed-forward\n\nThe tricky part: you need to pass a lambda function to sublayer because self-attention takes multiple arguments (query, key, value, mask).\nTry writing the forward method. Hereâ€™s the signature:\ndef forward(self, x, mask):\n    # Step 1: Apply self-attention with first sublayer connection\n    # Step 2: Apply feed-forward with second sublayer connection\nHint: Look at how the original code uses lambda to wrap the self_attn call.\nGive it a try!\nI donâ€™t want to look at the original code, give me a clue.\n\n\nğŸ¤–ReplyğŸ¤–\nGood! Let me give you clues without showing the code.\nClue 1: The sublayer in SublayerConnection.forward(x, sublayer) expects a function that takes one argument (x) and returns output.\nClue 2: But self.self_attn needs 4 arguments: query, key, value, mask. For self-attention, query=key=value=x.\nClue 3: You need to wrap self.self_attn in a function that takes only x as input. Use a lambda to do this.\nClue 4: For the feed-forward, itâ€™s simpler - it already takes just x as input.\nTry writing:\ndef forward(self, x, mask):\n    x = self.sublayer[0](x, lambda x: ???)  # What goes here?\n    x = self.sublayer[1](x, ???)  # What goes here?\n    return x\nWhat would you put in the ??? spots?\n\n\n\nFinal EncoderLayer Function\n\n\n\nEncoderLayer\n\n EncoderLayer (size, self_attn, feed_forward, dropout)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nIs this correct?\n\nğŸ¤–ReplyğŸ¤–\nExcellent work! Your EncoderLayer is now correct! âœ“\nYouâ€™ve got: - âœ“ First sublayer: self-attention with lambda x: self.self_attn(x, x, x, mask) (query=key=value=x) - âœ“ Second sublayer: feed-forward with lambda x: self.feed_forward(x) - âœ“ Both wrapped in SublayerConnection for residual + norm\nLetâ€™s test it to make sure everything works:\n# Create the components\nattn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\nencoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n\n# Test\nx = torch.randn(2, 10, 512)\nmask = None\noutput = encoder_layer(x, mask)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this and see if it works!\n\n# Create the components\nattn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\nencoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n\n# Test\nx = torch.randn(2, 10, 512)\nmask = None\noutput = encoder_layer(x, mask)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nVery cool, now assume that you are an NB dev and create a unit test out of this.\n\n\nğŸ¤–ReplyğŸ¤–\nHereâ€™s your Encoder Layer as a unit test for nbdev:\n#| export\nclass SublayerConnection(torch.nn.Module):\n    \"A residual connection followed by a layer norm\"\n    def __init__(self, size, dropout):\n        super().__init__()\n        self.layer_norm = torch.nn.LayerNorm(size)\n        self.dropout = torch.nn.Dropout(dropout)\n        \n    def forward(self, x, sublayer):\n        y = self.layer_norm(x)\n        y = sublayer(y)\n        y = self.dropout(y)\n        return x + y\n#| export\nclass EncoderLayer(torch.nn.Module):\n    \"Encoder layer with self-attention and feed-forward\"\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super().__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = torch.nn.ModuleList([SublayerConnection(size, dropout) for _ in range(2)])\n        \n    def forward(self, x, mask):\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        x = self.sublayer[1](x, self.feed_forward)\n        return x\n#| test\ndef test_encoder_layer():\n    \"Test Encoder Layer\"\n    attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    encoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n    \n    x = torch.randn(2, 10, 512)\n    output = encoder_layer(x, mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nGreat work! Ready for the next component?\n\n\nUnit Test\n\ndef test_encoder_layer():\n    \"Test Encoder Layer\"\n    attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    encoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n    \n    x = torch.randn(2, 10, 512)\n    output = encoder_layer(x, mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n\n\ntest_encoder_layer()\n\nThatâ€™s great, letâ€™s move on. Also be honest to the code I provided in the beginning.\n\nğŸ¤–ReplyğŸ¤–\nAwesome! Youâ€™re making excellent progress! ğŸ‰\nYouâ€™ve now built: 1. âœ“ Attention mechanism 2. âœ“ Multi-Head Attention 3. âœ“ Feed-Forward Network 4. âœ“ Positional Encoding 5. âœ“ Sublayer Connection (residual + norm) 6. âœ“ Encoder Layer\nWhatâ€™s next?\nLooking at the original code, the next component is the Encoder - which stacks N=6 identical EncoderLayers on top of each other.\nThe Encoder class needs to: - Take a single EncoderLayer and clone it N times - Apply them sequentially - Apply a final layer normalization\nBefore we code, do you understand why we need to clone the layer instead of just reusing the same one N times?\nWe are using clone because we want a deep copy, and we donâ€™t want to use the same address.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! ğŸ‰\nIf you just reused the same layer N times, theyâ€™d all share the same parameters (same memory address). So when you update weights during training, all â€œlayersâ€ would be identical - defeating the purpose of having multiple layers!\nBy cloning (deep copy), each layer gets its own independent parameters that can learn different things.\nNow letâ€™s implement the Encoder. Looking at the original code, youâ€™ll need:\nIn __init__: - Clone the layer N times (thereâ€™s a helper function clones in the original code) - Create a final LayerNorm\nIn forward: - Pass input through each layer sequentially - Apply final norm - Return output\nFirst, letâ€™s implement the clones helper function:\ndef clones(module, N):\n    \"Produce N identical layers\"\n    # How would you create N deep copies?\nCan you implement this helper function?\n\n\n\n\nFinal clones function\n\n\n\nclones\n\n clones (module, N)\n\nProduce N identical layers",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#encoder-class",
    "href": "attention_is_all_you_need.html#encoder-class",
    "title": "Simple 2x2 example",
    "section": "Encoder Class",
    "text": "Encoder Class\nIs this correct?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your clones function is exactly right! âœ“\nYouâ€™re using: - copy.deepcopy(module) to create independent copies - torch.nn.ModuleList to properly register them with PyTorch - List comprehension to create N copies\nNow letâ€™s implement the Encoder class. Try writing it:\nclass Encoder(nn.Module):\n    \"Stack of N encoder layers\"\n    def __init__(self, layer, N):\n        super().__init__()\n        # What do you need here?\n        \n    def forward(self, x, mask):\n        # What's the sequence of operations?\nWhat do you need in __init__? (Hint: use clones and create a LayerNorm)\n\nclass Encoder(torch.nn.Module):\n    \"Stack of N encoder layers\"\n    def __init__(self, layer, N):\n        super().__init__()\n        # What do you need here?\n        self.encoders = clones(layer, N)\n        \n    def forward(self, x, mask):\n        # What's the sequence of operations?\n        for encoder in encoders:\n            x = encoder(x, mask)\n        return x\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! Youâ€™re missing two things:\n\nMissing final LayerNorm: The original code applies a final layer normalization after all the encoder layers. Add:\nself.norm = nn.LayerNorm(layer.size)\nBug in forward: You wrote encoders but it should be self.encoders\n\nCan you fix these two issues?\nWhy we need final layer norm. Itâ€™s not there in the paper. Isnâ€™t the layer norm included in the sublayer anyway?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent question! Youâ€™re absolutely right to question this.\nYouâ€™re correct - in the paperâ€™s diagram (Figure 1), the layer norm is shown as part of each sublayer connection, not as a separate final norm.\nHowever, looking at the original Annotated Transformer code, they add a final layer norm after all encoder layers. This is a common implementation detail that helps with: - Stabilizing the output before it goes to the decoder - Ensuring the encoder output is well-normalized\nThe paper vs implementation: The paper doesnâ€™t explicitly show this final norm in the diagram, but many implementations (including the original authorsâ€™ code) include it for better training stability.\nSo itâ€™s an implementation choice rather than strictly from the paper. Do you want to: 1. Include it (matches the original code you provided) 2. Skip it (strictly follows the paper diagram)\nWhat would you prefer?\nIâ€™ll include it.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s include the final layer norm to match the original code.\nSo your Encoder should have:\nIn __init__:\nself.layers = clones(layer, N)\nself.norm = nn.LayerNorm(layer.size)\nIn forward:\nfor layer in self.layers:\n    x = layer(x, mask)\nreturn self.norm(x)\nCan you update your code with these changes?\n\n\nFinal encoder function.\n\n\n\nEncoder\n\n Encoder (layer, N)\n\nStack of N encoder layers\nIs this fine?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Encoder class is now correct! âœ“\nYouâ€™ve got: - âœ“ Cloning N encoder layers - âœ“ Final layer normalization - âœ“ Sequential application of all layers - âœ“ Returning normalized output\nLetâ€™s test it:\n# Create components\nattn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\nencoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n\n# Create encoder with N=6 layers\nencoder = Encoder(encoder_layer, N=6)\n\n# Test\nx = torch.randn(2, 10, 512)\noutput = encoder(x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this to verify it works!\n\n# Create components\nattn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\nencoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n\n# Create encoder with N=6 layers\nencoder = Encoder(encoder_layer, N=6)\n\n# Test\nx = torch.randn(2, 10, 512)\noutput = encoder(x, mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nAssume we are using NBDEV and create a unit test out of this.\n\n\nğŸ¤–ReplyğŸ¤–\nHereâ€™s your Encoder as a unit test for nbdev:\n#| export\ndef clones(module, N):\n    \"Produce N identical layers\"\n    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n#| export\nclass Encoder(torch.nn.Module):\n    \"Stack of N encoder layers\"\n    def __init__(self, layer, N):\n        super().__init__()\n        self.encoders = clones(layer, N)\n        self.layer_norm = torch.nn.LayerNorm(layer.size)\n        \n    def forward(self, x, mask):\n        for encoder in self.encoders:\n            x = encoder(x, mask)\n        return self.layer_norm(x)\n#| test\ndef test_encoder():\n    \"Test Encoder stack\"\n    attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    encoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n    encoder = Encoder(encoder_layer, N=6)\n    \n    x = torch.randn(2, 10, 512)\n    output = encoder(x, mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nExcellent progress! The encoder is complete. Ready to move on to the Decoder components?\n\n\nUnit test\n\ndef test_encoder():\n    \"Test Encoder stack\"\n    attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    encoder_layer = EncoderLayer(size=512, self_attn=attn, feed_forward=ff, dropout=0.1)\n    encoder = Encoder(encoder_layer, N=6)\n    \n    x = torch.randn(2, 10, 512)\n    output = encoder(x, mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n\n\ntest_encoder()",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#decoder-layer",
    "href": "attention_is_all_you_need.html#decoder-layer",
    "title": "Simple 2x2 example",
    "section": "Decoder Layer",
    "text": "Decoder Layer\nNow, what is next?\n\nğŸ¤–ReplyğŸ¤–\nGreat work! Youâ€™ve completed the entire Encoder side of the Transformer! ğŸ‰\nWhatâ€™s next: The Decoder\nThe Decoder is similar to the Encoder but has some key differences:\n\nMasked Self-Attention - prevents looking at future tokens\nCross-Attention - attends to the encoder output\nThree sub-layers instead of two (self-attn, cross-attn, feed-forward)\n\nLetâ€™s start with the Decoder Layer. It will have: - Masked multi-head self-attention - Multi-head cross-attention (attends to encoder output) - Position-wise feed-forward - Three SublayerConnection modules (one for each sub-layer)\nBefore coding, do you understand the difference between: - Self-attention (what the encoder uses) - Masked self-attention (decoder looks at previous tokens only) - Cross-attention (decoder attends to encoder output)\nWant me to clarify these concepts first?\nIâ€™d rather you quiz me on the concepts.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s test your understanding:\nQuestion 1: Masked Self-Attention\nIn the decoder, why do we need to mask the self-attention? What would happen if we didnâ€™t mask it during training?\nQuestion 2: Cross-Attention\nIn cross-attention, where do the Query (Q), Key (K), and Value (V) come from? (Hint: think about what the decoder has access to)\nQuestion 3: Decoder Architecture\nHow many sublayers does a decoder layer have, and what does each one do?\nGive these a shot!\nI think masking is needed mostly while training. The thing is, the eventual output is already known. But we want to make sure that we do not leak the future word output words beforehand.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! âœ“\nMasked Self-Attention: During training, we have the entire target sequence available. Without masking, the decoder could â€œcheatâ€ by looking at future words itâ€™s supposed to predict. The mask ensures position i can only attend to positions â‰¤ i.\nKey insight: During inference (actual use), we generate one token at a time, so there ARE no future tokens to look at. But during training, we need the mask to simulate this autoregressive behavior.\nGreat answer! Now try Question 2:\nIn cross-attention (the second attention mechanism in the decoder), where do Q, K, and V come from? Think about what inputs the decoder has access to.\nLetâ€™s explore this in a bit of detail. The input is, â€œYou are good.â€ And the output is. Vous etes bons\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Letâ€™s trace through this example step by step.\nInput (English): â€œYou are goodâ€ Output (French): â€œVous Ãªtes bonsâ€\nDuring training, letâ€™s say weâ€™re predicting the 2nd word â€œÃªtesâ€:\nEncoder side: - Processes â€œYou are goodâ€ â†’ produces contextualized representations (memory)\nDecoder side at position 2:\n\nMasked Self-Attention:\n\nQuery from â€œÃªtesâ€ position\nCan attend to: â€œVousâ€ and â€œÃªtesâ€ (not â€œbonsâ€ - thatâ€™s future!)\nQ, K, V all come from decoderâ€™s own input\n\nCross-Attention: (This is your Question 2!)\n\nWhere does Q come from?\nWhere do K and V come from?\n\n\nThink about what the decoder needs to â€œlook atâ€ from the encoder. What would make sense?\nIn cross-attention, Q and K come from the encoder outputs. And V comes from the output of the Masked multi-head attention. So basically, the query is from the input, and the key that is the dictionary is also from the input, but the eventual value will come from the output, or in this case, the translation.\n\n\nğŸ¤–ReplyğŸ¤–\nNot quite! Let me clarify:\nIn Cross-Attention:\n\nQ (Query): Comes from the decoder (the output of masked self-attention)\nK (Key): Comes from the encoder output (memory)\nV (Value): Comes from the encoder output (memory)\n\nSo the decoder is asking â€œwhat information do I need from the input?â€ and the encoder provides both the keys (whatâ€™s available) and values (the actual information).\nThink of it this way: - Decoder says: â€œIâ€™m trying to generate â€˜Ãªtesâ€™, what from the English sentence is relevant?â€ (Query) - Encoder says: â€œHereâ€™s what I have: â€˜Youâ€™, â€˜areâ€™, â€˜goodâ€™â€ (Keys) - Decoder attends strongly to â€œareâ€ - Encoder provides: â€œHereâ€™s the information about â€˜areâ€™â€ (Value)\nDoes this clarify where Q, K, V come from in cross-attention?\nOkay, thatâ€™s clear. That query is from the decoder, and the key and value are from the encoder.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Now youâ€™ve got it! âœ“\nCross-Attention summary: - Q: from decoder (asking â€œwhat do I need?â€) - K, V: from encoder (providing â€œhereâ€™s whatâ€™s availableâ€)\nNow letâ€™s move to Question 3:\nHow many sublayers does a decoder layer have, and what does each one do?\nGive it a try!\nThere are three sublayers. The first sub-layer is self-attention on the mast output. The second sub-layer is cross-attention. Like I said, the query is from the decoder, and the key and value from the encoder. And the third sublayer once again is the position-wise feedforward network.\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Youâ€™ve got it perfectly! âœ“\nThree sublayers in Decoder Layer: 1. Masked Self-Attention - decoder attends to its own previous outputs 2. Cross-Attention - decoder attends to encoder output (Q from decoder, K&V from encoder) 3. Position-wise Feed-Forward - same as encoder\nEach sublayer has a residual connection + layer norm wrapped around it (using SublayerConnection).\nNow youâ€™re ready to implement the Decoder Layer!\nLooking at the structure, what do you think youâ€™ll need in __init__?\nHint: Youâ€™ll need to store 3 attention modules and create 3 SublayerConnection modules.\nWant to give it a try?\nYes, I would like to give it a try.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s implement the Decoder Layer.\nStart with the __init__ method. You need to store:\n\nself.size (d_model)\nThree attention modules:\n\nself.self_attn (masked self-attention)\nself.src_attn (cross-attention to encoder)\nself.feed_forward (FFN)\n\nThree SublayerConnection modules (one for each sublayer)\n\nTry writing the __init__:\nclass DecoderLayer(nn.Module):\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super().__init__()\n        # What do you need here?\nGive it a shot!\n\nclass DecoderLayer(torch.nn.Module):\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super().__init__()\n        # What do you need here?\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayers = clones(SublayerConnection(size, dropout), 3)\n\nAll good. Be faithful to the original code.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your __init__ is exactly right and matches the original code! âœ“\nYouâ€™ve got: - âœ“ All three attention modules stored - âœ“ Feed-forward stored - âœ“ Three SublayerConnection modules cloned\nNow for the forward method. You need to implement:\ndef forward(self, x, memory, src_mask, tgt_mask):\n    # What are the three sublayer operations?\nWhere: - x = decoder input - memory = encoder output - src_mask = mask for encoder (used in cross-attention) - tgt_mask = mask for decoder (used in masked self-attention)\nTry writing the three sublayer calls using lambdas (like you did in EncoderLayer). What would each one look like?\nIs this good?\n\n\nğŸ¤–ReplyğŸ¤–\nClose! But there are a few issues to fix:\n\nFirst sublayer: Youâ€™re missing the x argument to the lambda, and the mask should be tgt_mask (not None):\nx = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\nSecond sublayer: What are K and V? They should come from memory (the encoder output). The query comes from decoder (x), but keys and values come from encoder:\nx = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\nThird sublayer: This one is correct!\n\nCan you fix the first two sublayers?\nIâ€™m not sure I understand whatâ€™s the difference between tgt_mask and src_mask. Can you explain to me? I would guess that TGT mask is to avoid leakage of outputs, but I am not sure about SRC mask.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me clarify the two masks:\ntgt_mask (target mask): Youâ€™re exactly right! This is the causal/look-ahead mask for the decoder. It prevents position i from seeing positions &gt; i. This is the triangular mask that ensures no future leakage.\nsrc_mask (source mask): This is a padding mask for the encoder output. Remember when you batch sentences together, shorter sentences get padded with &lt;pad&gt; tokens? The src_mask tells the decoder â€œdonâ€™t pay attention to these padding positions in the encoder output - theyâ€™re meaningless.â€\nWhere theyâ€™re used: - tgt_mask: In masked self-attention (decoder looking at its own tokens) - src_mask: In cross-attention (decoder looking at encoder output)\nExample:\nSource: \"Hello world &lt;pad&gt; &lt;pad&gt;\"  â† src_mask hides the &lt;pad&gt;\nTarget: \"Bonjour le monde\"         â† tgt_mask prevents future leakage\nDoes this clarify the difference?\n\ndef forward(self, x, memory, src_mask, tgt_mask):\n    # What are the three sublayer operations?\n    x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n    x = self.sublayers[1](x, lambda x: self.src_attn(x, K, V, src_mask))\n    x = self.sublayers[2](x, lambda x: self.feed_forward(x))\n    return x\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! You still have K and V as variables in the second sublayer, but theyâ€™re not defined.\nRemember: in cross-attention, both the keys and values come from memory (the encoder output). So it should be:\nx = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\nThe pattern is: self.src_attn(query, key, value, mask) where: - query = x (from decoder) - key = memory (from encoder) - value = memory (from encoder)\nCan you fix that line?\n\ndef forward(self, x, memory, src_mask, tgt_mask):\n    # What are the three sublayer operations?\n    x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n    x = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n    x = self.sublayers[2](x, lambda x: self.feed_forward(x))\n    return x\n\nIs this correct? Stay loyal to the original code.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your DecoderLayer.forward is now correct and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Masked self-attention with tgt_mask - âœ“ Cross-attention with query from decoder, K&V from encoder memory, using src_mask - âœ“ Feed-forward network\nNow letâ€™s combine the __init__ and forward into the complete class and test it:\nclass DecoderLayer(torch.nn.Module):\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super().__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayers = clones(SublayerConnection(size, dropout), 3)\n    \n    def forward(self, x, memory, src_mask, tgt_mask):\n        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n        x = self.sublayers[2](x, self.feed_forward)\n        return x\nWant to test it now?\n\n\nFinal decoder layer Function.\n\n\n\nDecoderLayer\n\n DecoderLayer (size, self_attn, src_attn, feed_forward, dropout)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nOkay, letâ€™s test it.\n\nğŸ¤–ReplyğŸ¤–\nGreat! Letâ€™s test your DecoderLayer:\n# Create components\nself_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nsrc_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\ndecoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                             feed_forward=ff, dropout=0.1)\n\n# Test\nx = torch.randn(2, 10, 512)  # decoder input\nmemory = torch.randn(2, 15, 512)  # encoder output (different seq_len is fine)\nsrc_mask = None\ntgt_mask = None\n\noutput = decoder_layer(x, memory, src_mask, tgt_mask)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this and see if it works!\n\n# Create components\nself_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nsrc_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\ndecoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                             feed_forward=ff, dropout=0.1)\n\n# Test\nx = torch.randn(2, 10, 512)  # decoder input\nmemory = torch.randn(2, 15, 512)  # encoder output (different seq_len is fine)\nsrc_mask = None\ntgt_mask = None\n\noutput = decoder_layer(x, memory, src_mask, tgt_mask)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nWhat happened, bro?\n\n\nğŸ¤–ReplyğŸ¤–\nAh, I see the issue! The problem is in your MultiHeadedAttention.forward method.\nThe bug: In cross-attention, the key and value come from memory which has a different sequence length (15) than the query which comes from x (10).\nBut in your code, youâ€™re using seq_len from Q for reshaping K and V:\nseq_len = Q.size(1)  # This is 10 (from decoder)\nK = K.reshape(batch_size, seq_len, self.h, self.d_k)  # But K has seq_len=15!\nThe fix: You need to get the sequence length from each tensor individually:\nQ = Q.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\nK = K.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\nV = V.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\nUsing -1 lets PyTorch infer the sequence length automatically!\nCan you update your MultiHeadedAttention class to fix this?\nAre you sure? Is this honest to the original code?\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re right to question me! Let me check the original code more carefully.\nLooking at the original MultiHeadedAttention.forward:\nnbatches = query.size(0)\n\n# 1) Do all the linear projections in batch from d_model =&gt; h x d_k\nquery, key, value = [\n    lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n    for lin, x in zip(self.linears, (query, key, value))\n]\nYouâ€™re right - the original code uses -1 to automatically infer the sequence length! So yes, my fix is correct and matches the original.\nYour current code has:\nseq_len = Q.size(1)\nQ = Q.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1,2)\nK = K.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1,2)  # Bug here!\nShould be:\nQ = Q.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\nK = K.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\nV = V.reshape(batch_size, -1, self.h, self.d_k).transpose(1,2)\nCan you update your MultiHeadedAttention class?\nI could fix that, but what about the part where weâ€™re concatenating and weâ€™re using seq_length again?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent catch! Youâ€™re absolutely right - thereâ€™s the same issue in the concatenation step!\nLooking at your code, you probably have something like:\nx = x.transpose(1,2).reshape(batch_size, seq_len, -1)\nBut seq_len was computed from Q, so it wonâ€™t work when K and V have different lengths.\nThe fix: Use -1 there too:\nx = x.transpose(1,2).reshape(batch_size, -1, self.h * self.d_k)\nOr even simpler (matching the original code):\nx = x.transpose(1,2).contiguous().view(nbatches, -1, self.h * self.d_k)\nCan you update both the reshaping at the beginning AND the concatenation at the end of your MultiHeadedAttention.forward method?\nfixed it now\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Now letâ€™s test your DecoderLayer again with the fixed MultiHeadedAttention:\n# Create components\nself_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nsrc_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\ndecoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                             feed_forward=ff, dropout=0.1)\n\n# Test\nx = torch.randn(2, 10, 512)  # decoder input\nmemory = torch.randn(2, 15, 512)  # encoder output (different seq_len)\nsrc_mask = None\ntgt_mask = None\n\noutput = decoder_layer(x, memory, src_mask, tgt_mask)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this and let me know if it works now!\n\n# Create components\nself_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nsrc_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\ndecoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                             feed_forward=ff, dropout=0.1)\n\n# Test\nx = torch.randn(2, 10, 512)  # decoder input\nmemory = torch.randn(2, 15, 512)  # encoder output (different seq_len)\nsrc_mask = None\ntgt_mask = None\n\noutput = decoder_layer(x, memory, src_mask, tgt_mask)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nAwesome, assume we are using NBDev and create a unit test out of this.\n\n\nğŸ¤–ReplyğŸ¤–\nHereâ€™s your Decoder Layer as a unit test for nbdev:\n#| export\nclass DecoderLayer(torch.nn.Module):\n    \"Decoder layer with masked self-attention, cross-attention, and feed-forward\"\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super().__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayers = clones(SublayerConnection(size, dropout), 3)\n    \n    def forward(self, x, memory, src_mask, tgt_mask):\n        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n        x = self.sublayers[2](x, self.feed_forward)\n        return x\n#| test\ndef test_decoder_layer():\n    \"Test Decoder Layer with cross-attention\"\n    self_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    src_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    decoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                                 feed_forward=ff, dropout=0.1)\n    \n    x = torch.randn(2, 10, 512)  # decoder input\n    memory = torch.randn(2, 15, 512)  # encoder output\n    output = decoder_layer(x, memory, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nExcellent work! Ready to build the full Decoder stack next?\n\n\nUnit test\n\ndef test_decoder_layer():\n    \"Test Decoder Layer with cross-attention\"\n    self_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    src_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    decoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                                 feed_forward=ff, dropout=0.1)\n    \n    x = torch.randn(2, 10, 512)  # decoder input\n    memory = torch.randn(2, 15, 512)  # encoder output\n    output = decoder_layer(x, memory, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#decoder-1",
    "href": "attention_is_all_you_need.html#decoder-1",
    "title": "Simple 2x2 example",
    "section": "Decoder",
    "text": "Decoder\nAmazing progress, now letâ€™s finish this. Stay honest to the original code.\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Youâ€™re almost done with the Transformer! ğŸ‰\nWhatâ€™s left:\n\nDecoder - Stack N=6 decoder layers (similar to how you built Encoder)\nEmbeddings - Convert token IDs to vectors\nGenerator - Final linear + softmax to predict next token\nEncoderDecoder - The complete model that ties everything together\n\nLetâ€™s start with the Decoder stack. Itâ€™s very similar to the Encoder you already built.\nLooking at the original code, what do you think the Decoder class needs?\nHint: Itâ€™s almost identical to Encoder, but the forward method takes different arguments. Try writing the skeleton:\nclass Decoder(nn.Module):\n    def __init__(self, layer, N):\n        super().__init__()\n        # What do you need?\n        \n    def forward(self, x, memory, src_mask, tgt_mask):\n        # What's the sequence of operations?\nGive it a try!\n\n\nFinal decoder function.\n\n\n\nDecoder\n\n Decoder (layer, N)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nHave we done it?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Decoder class is exactly right and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Cloning N decoder layers - âœ“ Final layer normalization - âœ“ Sequential application passing all 4 arguments (x, memory, src_mask, tgt_mask) - âœ“ Returning normalized output\nLetâ€™s test it:\n# Create components\nself_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nsrc_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\ndecoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                             feed_forward=ff, dropout=0.1)\n\n# Create decoder with N=6 layers\ndecoder = Decoder(decoder_layer, N=6)\n\n# Test\nx = torch.randn(2, 10, 512)\nmemory = torch.randn(2, 15, 512)\noutput = decoder(x, memory, src_mask=None, tgt_mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this to verify!\n\n# Create components\nself_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nsrc_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\nff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\ndecoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                             feed_forward=ff, dropout=0.1)\n\n# Create decoder with N=6 layers\ndecoder = Decoder(decoder_layer, N=6)\n\n# Test\nx = torch.randn(2, 10, 512)\nmemory = torch.randn(2, 15, 512)\noutput = decoder(x, memory, src_mask=None, tgt_mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nAmazing! Then now assume youâ€™re a NB Dev. No need to show the original function but show the unit test.\n\n\nğŸ¤–ReplyğŸ¤–\n#| test\ndef test_decoder():\n    \"Test Decoder stack\"\n    self_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    src_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    decoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                                 feed_forward=ff, dropout=0.1)\n    decoder = Decoder(decoder_layer, N=6)\n    \n    x = torch.randn(2, 10, 512)\n    memory = torch.randn(2, 15, 512)\n    output = decoder(x, memory, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])\nFantastic progress! Youâ€™ve now built both the complete Encoder and Decoder stacks! Whatâ€™s next - Embeddings, Generator, or the final EncoderDecoder wrapper?\n\n\n\nUnit test\n\ndef test_decoder():\n    \"Test Decoder stack\"\n    self_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    src_attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    decoder_layer = DecoderLayer(size=512, self_attn=self_attn, src_attn=src_attn, \n                                 feed_forward=ff, dropout=0.1)\n    decoder = Decoder(decoder_layer, N=6)\n    \n    x = torch.randn(2, 10, 512)\n    memory = torch.randn(2, 15, 512)\n    output = decoder(x, memory, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 10, 512])",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#embeddings",
    "href": "attention_is_all_you_need.html#embeddings",
    "title": "Simple 2x2 example",
    "section": "Embeddings",
    "text": "Embeddings\nOkay, letâ€™s do embeddings.\n\nğŸ¤–ReplyğŸ¤–\nGreat! Letâ€™s implement Embeddings.\nWhat are embeddings?\nThey convert discrete token IDs (integers) into continuous vector representations. For example: - Token ID 42 â†’ vector of size d_model=512\nLooking at the paper and original code, the Embeddings class: 1. Uses nn.Embedding to create a lookup table 2. Multiplies the output by âˆšd_model (for scaling)\nWhy multiply by âˆšd_model?\nThis scaling ensures that the embedding values and positional encoding values are on a similar scale before adding them together.\nTry implementing it:\nclass Embeddings(nn.Module):\n    def __init__(self, d_model, vocab):\n        super().__init__()\n        # What do you need here?\n        \n    def forward(self, x):\n        # What's the operation?\nWhat do you need to store in __init__, and what operation goes in forward?\n\nclass Embeddings(torch.nn.Module):\n    def __init__(self, d_model, vocab):\n        super().__init__()\n        # What do you need here?\n        self.embedding = torch.nn.Embedding(vocab, d_model)\n        \n    def forward(self, x):\n        # What's the operation?\n        return self.embedding(x)\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! Youâ€™re missing the scaling factor. Remember, the paper multiplies the embeddings by âˆšd_model.\nYou need to: 1. Store d_model in __init__ so you can use it in forward 2. Multiply the embedding output by math.sqrt(self.d_model)\nCan you add these two things?\nWhy are we doing this? I donâ€™t understand.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me explain why we scale by âˆšd_model.\nThe problem:\nAfter creating embeddings, you add them to positional encodings:\nfinal_input = embeddings + positional_encoding\nBut embeddings from nn.Embedding typically have small values (initialized around -0.1 to 0.1), while positional encodings use sin/cos which range from -1 to 1.\nWithout scaling: - Embeddings: small values (~0.1) - Positional encoding: larger values (~1.0) - The positional encoding would dominate!\nWith âˆšd_model scaling: - If d_model = 512, then âˆš512 â‰ˆ 22.6 - Embeddings get multiplied by ~22.6, making them comparable in magnitude to positional encodings - Now both contribute meaningfully to the final representation\nIn short: It balances the contribution of embeddings and positional encodings so neither dominates.\nDoes this make sense now?\n\n\nFinal Embeddings Model Function\n\n\n\nEmbeddings\n\n Embeddings (d_model, vocab)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nAre you happy now?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Embeddings class is now correct and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Storing d_model - âœ“ Creating the embedding layer - âœ“ Scaling by âˆšd_model in forward\nLetâ€™s test it:\nembeddings = Embeddings(d_model=512, vocab=10000)\nx = torch.randint(0, 10000, (2, 10))  # batch=2, seq_len=10, random token IDs\noutput = embeddings(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\nRun this to verify it works!\n\nembeddings = Embeddings(d_model=512, vocab=10000)\nx = torch.randint(0, 10000, (2, 10))  # batch=2, seq_len=10, random token IDs\noutput = embeddings(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 512))\n\nThatâ€™s amazing. Now, assume weâ€™re using NBDEV. We donâ€™t need to output the original function, just the unit test.\n\n\nğŸ¤–ReplyğŸ¤–\n#| test\ndef test_embeddings():\n    \"Test Embeddings with scaling\"\n    embeddings = Embeddings(d_model=512, vocab=10000)\n    x = torch.randint(0, 10000, (2, 10))  # batch=2, seq_len=10\n    output = embeddings(x)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n    # Check that scaling is applied (output should be larger than raw embedding)\n    raw_embed = embeddings.embedding(x)\n    assert torch.allclose(output, raw_embed * math.sqrt(512))\nGreat! Whatâ€™s next - Generator or the final EncoderDecoder wrapper?\n\n\n\nUnit test\n\ndef test_embeddings():\n    \"Test Embeddings with scaling\"\n    embeddings = Embeddings(d_model=512, vocab=10000)\n    x = torch.randint(0, 10000, (2, 10))  # batch=2, seq_len=10\n    output = embeddings(x)\n    \n    assert output.shape == torch.Size([2, 10, 512])\n    # Check that scaling is applied (output should be larger than raw embedding)\n    raw_embed = embeddings.embedding(x)\n    assert torch.allclose(output, raw_embed * math.sqrt(512))\n\n\ntest_embeddings()",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#generator-class",
    "href": "attention_is_all_you_need.html#generator-class",
    "title": "Simple 2x2 example",
    "section": "Generator class",
    "text": "Generator class\nAwesome. Now what? Stay honest to the original quote.\n\nğŸ¤–ReplyğŸ¤–\nExcellent progress! Looking at the original code structure, the next component is the Generator.\nWhat does the Generator do?\nItâ€™s the final layer that converts the decoderâ€™s output (d_model=512 dimensions) into vocabulary predictions. It does: 1. Linear projection: d_model â†’ vocab_size 2. Log softmax to get log probabilities for each token\nThis is what actually predicts â€œwhatâ€™s the next word?â€\nTry implementing it:\nclass Generator(nn.Module):\n    def __init__(self, d_model, vocab):\n        super().__init__()\n        # What layer do you need?\n        \n    def forward(self, x):\n        # What operation?\nWhat do you need in __init__ and forward?\n\n\nFinal generator function.\n\n\n\nGenerator\n\n Generator (d_model, vocab)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\nsee if tihs works?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Generator class looks correct and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Linear layer (d_model â†’ vocab) - âœ“ LogSoftmax on the last dimension\nLetâ€™s test it:\ngenerator = Generator(d_model=512, vocab=10000)\nx = torch.randn(2, 10, 512)  # decoder output\noutput = generator(x)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 10, 10000))  # log probabilities for each token in vocab\nRun this to verify it works!\nNow assume we are using NBDEV. No need to output the function but output the unit test.\n\n\nğŸ¤–ReplyğŸ¤–\n#| test\ndef test_generator():\n    \"Test Generator output projection\"\n    generator = Generator(d_model=512, vocab=10000)\n    x = torch.randn(2, 10, 512)  # decoder output\n    output = generator(x)\n    \n    assert output.shape == torch.Size([2, 10, 10000])\n    # Check that output is log probabilities (should sum to ~1 after exp)\n    assert torch.allclose(output.exp().sum(dim=-1), torch.ones(2, 10), atol=1e-5)\nExcellent! Youâ€™re almost done! The final piece is the EncoderDecoder wrapper that ties everything together. Ready for that?\n\n\nUnit test\n\ndef test_generator():\n    \"Test Generator output projection\"\n    generator = Generator(d_model=512, vocab=10000)\n    x = torch.randn(2, 10, 512)  # decoder output\n    output = generator(x)\n    \n    assert output.shape == torch.Size([2, 10, 10000])\n    # Check that output is log probabilities (should sum to ~1 after exp)\n    assert torch.allclose(output.exp().sum(dim=-1), torch.ones(2, 10), atol=1e-5)\n\n\ntest_generator()\n\nAwesome. Whatâ€™s next?\n\nğŸ¤–ReplyğŸ¤–\nAmazing work! Youâ€™re at the final step! ğŸ‰\nWhatâ€™s left: EncoderDecoder\nThis is the top-level class that ties everything together. Looking at the original code, it stores all the components and defines how data flows through the model.\nWhat does EncoderDecoder need?\nIn __init__, it stores: - encoder - decoder\n- src_embed (source embeddings + positional encoding) - tgt_embed (target embeddings + positional encoding) - generator\nIn forward, it needs methods like: - encode() - process source through encoder - decode() - process target through decoder - forward() - full forward pass\nBefore coding, do you understand the data flow? Source goes through encoder, then decoder uses encoder output + target to generate predictions.\nWant to try implementing the __init__ first?\nYes, letâ€™s try it.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s implement the EncoderDecoder class.\nStart with the __init__ method. Looking at the original code, you need to store 5 components:\nclass EncoderDecoder(nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super().__init__()\n        # Store all 5 components\nTry writing the __init__ - just store each component as an attribute. What would you write?\n\nclass EncoderDecoder(torch.nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super().__init__()\n        # Store all 5 components\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n\nIs that good?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your __init__ is exactly right! âœ“\nNow letâ€™s implement the methods. Looking at the original code, you need:\n\nencode(self, src, src_mask) - runs source through embedding and encoder\ndecode(self, memory, src_mask, tgt, tgt_mask) - runs target through embedding and decoder\nforward(self, src, tgt, src_mask, tgt_mask) - combines encode and decode\n\nTry implementing the encode method first. What should it do?\nHint: Apply src_embed to src, then pass through encoder with the mask.\nIâ€™m sleepy, letâ€™s do the rest tomorrow.\n\n\nğŸ¤–ReplyğŸ¤–\nNo problem! Youâ€™ve made incredible progress today! ğŸ‰\nWhat youâ€™ve accomplished: - âœ“ Attention mechanism - âœ“ Multi-Head Attention - âœ“ Position-wise Feed-Forward - âœ“ Positional Encoding - âœ“ Sublayer Connection - âœ“ Encoder Layer & full Encoder stack - âœ“ Decoder Layer & full Decoder stack - âœ“ Embeddings - âœ“ Generator\nWhatâ€™s left (very little!): - Finish EncoderDecoder (encode, decode, forward methods) - Optional: make_model helper function to build the complete transformer\nYouâ€™re about 95% done with implementing the Transformer from scratch! Get some rest, and weâ€™ll finish it up tomorrow. Great work! ğŸ’ª\nIâ€™m back again. Letâ€™s get started. Stay honest to the original code.\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nWelcome back! ğŸ‰\nYou were so close yesterday! Youâ€™ve already implemented the EncoderDecoder.__init__. Now you just need three methods:\n\nencode(self, src, src_mask) - embed source, pass through encoder\ndecode(self, memory, src_mask, tgt, tgt_mask) - embed target, pass through decoder\nforward(self, src, tgt, src_mask, tgt_mask) - call encode, then decode\n\nLetâ€™s start with encode. It should: - Apply self.src_embed to src - Pass the result through self.encoder with the mask\nTry writing it:\ndef encode(self, src, src_mask):\n    # What goes here?\n\nclass EncoderDecoder(torch.nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super().__init__()\n        # Store all 5 components\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n    def encode(self, src, src_mask):\n        x = self.src_embed(src)\n        return self.encoder(x, src_mask)\n\nDoes this make sense?\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your encode method is exactly right! âœ“\nYouâ€™ve got: - âœ“ Apply source embeddings - âœ“ Pass through encoder with mask - âœ“ Return the encoder output (memory)\nNow try the decode method. It should: - Apply self.tgt_embed to tgt - Pass the result through self.decoder along with memory, src_mask, and tgt_mask\nGive it a try:\ndef decode(self, memory, src_mask, tgt, tgt_mask):\n    # What goes here?\n\nclass EncoderDecoder(torch.nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super().__init__()\n        # Store all 5 components\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n    def encode(self, src, src_mask):\n        x = self.src_embed(src)\n        return self.encoder(x, src_mask)\n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        # What goes here?\n        x = self.tgt_embed(tgt)\n        return self.decoder(x, memory, src_mask, tgt_mask)\n\nIs this cool? Stay loyal to the original code.\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your decode method is exactly right and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Apply target embeddings - âœ“ Pass through decoder with all the right arguments (x, memory, src_mask, tgt_mask)\nNow the final method: forward. It should: - Call encode to get memory - Call decode with the memory - Return the decoder output\nTry writing it:\ndef forward(self, src, tgt, src_mask, tgt_mask):\n    # What goes here?\n\n\n\n\nFinal Encoder Decoder Function.\n\n\n\nEncoderDecoder\n\n EncoderDecoder (encoder, decoder, src_embed, tgt_embed, generator)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\ndone baby\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Your EncoderDecoder class is complete and correct! ğŸ‰\nYouâ€™ve got: - âœ“ encode - processes source - âœ“ decode - processes target with encoder memory - âœ“ forward - ties them together\nThis is the complete Transformer architecture! Letâ€™s test it to make sure everything works together:\n# You'll need to create all the components and test\n# Want me to help you write a test, or do you want to try?\nBut wait - looking at the original code, thereâ€™s one more super useful thing: the make_model helper function that builds a complete transformer with all the right components.\nDo you want to: 1. Test the EncoderDecoder as-is 2. Implement the make_model helper function (makes it easy to create a full transformer)\nWhat would you prefer?\nBefore moving on, letâ€™s make a unit test for this function, like we usually do, assuming we are in NB dev and the output function is already exported.\n\n\nğŸ¤–ReplyğŸ¤–\n#| test\ndef test_encoder_decoder():\n    \"Test complete EncoderDecoder model\"\n    # Create all components\n    attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    \n    encoder_layer = EncoderLayer(size=512, self_attn=copy.deepcopy(attn), \n                                  feed_forward=copy.deepcopy(ff), dropout=0.1)\n    decoder_layer = DecoderLayer(size=512, \n                                  self_attn=copy.deepcopy(attn),\n                                  src_attn=copy.deepcopy(attn),\n                                  feed_forward=copy.deepcopy(ff), dropout=0.1)\n    \n    encoder = Encoder(encoder_layer, N=2)\n    decoder = Decoder(decoder_layer, N=2)\n    \n    src_embed = torch.nn.Sequential(Embeddings(512, 1000), PositionalEncoding(512, 0.1))\n    tgt_embed = torch.nn.Sequential(Embeddings(512, 1000), PositionalEncoding(512, 0.1))\n    generator = Generator(512, 1000)\n    \n    model = EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)\n    \n    # Test\n    src = torch.randint(0, 1000, (2, 10))\n    tgt = torch.randint(0, 1000, (2, 8))\n    output = model(src, tgt, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 8, 512])\nReady to move on to make_model?\n\n\nUnit test\n\ndef test_encoder_decoder():\n    \"Test complete EncoderDecoder model\"\n    # Create all components\n    attn = MultiHeadedAttention(h=8, d_model=512, dropout=0.1)\n    ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n    \n    encoder_layer = EncoderLayer(size=512, self_attn=copy.deepcopy(attn), \n                                  feed_forward=copy.deepcopy(ff), dropout=0.1)\n    decoder_layer = DecoderLayer(size=512, \n                                  self_attn=copy.deepcopy(attn),\n                                  src_attn=copy.deepcopy(attn),\n                                  feed_forward=copy.deepcopy(ff), dropout=0.1)\n    \n    encoder = Encoder(encoder_layer, N=2)\n    decoder = Decoder(decoder_layer, N=2)\n    \n    src_embed = torch.nn.Sequential(Embeddings(512, 1000), PositionalEncoding(512, 0.1))\n    tgt_embed = torch.nn.Sequential(Embeddings(512, 1000), PositionalEncoding(512, 0.1))\n    generator = Generator(512, 1000)\n    \n    model = EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)\n    \n    # Test\n    src = torch.randint(0, 1000, (2, 10))\n    tgt = torch.randint(0, 1000, (2, 8))\n    output = model(src, tgt, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 8, 512])\n\n\ntest_encoder_decoder()",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#make-model",
    "href": "attention_is_all_you_need.html#make-model",
    "title": "Simple 2x2 example",
    "section": "Make Model",
    "text": "Make Model\nOkay, letâ€™s go further.\n\nğŸ¤–ReplyğŸ¤–\nAwesome! ğŸ‰ Your complete Transformer is working!\nWhatâ€™s next: make_model helper function\nLooking at the original code, make_model is a convenience function that builds a complete transformer from just hyperparameters. Instead of manually creating all components like in your test, you can just call:\nmodel = make_model(src_vocab=11, tgt_vocab=11, N=6)\nAnd it creates everything for you!\nWhat does make_model need to do?\nLooking at the original code, it takes these parameters: - src_vocab, tgt_vocab - vocabulary sizes - N - number of layers (default 6) - d_model - model dimension (default 512) - d_ff - feed-forward dimension (default 2048) - h - number of attention heads (default 8) - dropout - dropout rate (default 0.1)\nThen it creates all the components and returns an EncoderDecoder model.\nWant to try implementing it, or should I break it down into smaller steps?\n\nmodel = make_model(src_vocab=11, tgt_vocab=11, N=6)\n\nIâ€™m guessing this function just creates an encoder-decoder layer, Along with the inputs which are needed for the encoder-decoder. Am I correct?\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! ğŸ’¯\nmake_model is just a convenience factory function that: 1. Creates all the components (attention modules, feed-forward, embeddings, etc.) 2. Assembles them into an EncoderDecoder model 3. Initializes the parameters properly\nIt saves you from writing all that boilerplate code every time you want to create a transformer.\nLooking at the original code, it also does one more important thing at the end: Xavier initialization of parameters for better training.\nWant to try implementing it? Start with the function signature:\ndef make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    # Create components\n    # Assemble into EncoderDecoder\n    # Initialize parameters\nWhat would you create first inside this function?\nThis is actually a good exercise that brings all the components we built together. We need to know which order to build them first. All the way from low-level to high-level.\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent insight! Youâ€™re absolutely right - this is a great exercise in understanding the dependency hierarchy.\nLetâ€™s think through the order together. What components have no dependencies (the lowest level building blocks)?\nStart from the bottom - which components can be created first without needing anything else?\nWe start from Attention, then multi-header attention, then position-wise feed forward, then subconnection layer, then encoder layer, then decoder layer, then encoder, then decoder, then encoder-decoder both, and in between also we need clones.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat breakdown! Youâ€™ve got the dependency order mostly right! Let me refine it slightly:\nLevel 1 (no dependencies): - attention function - clones function\nLevel 2 (uses Level 1): - MultiHeadedAttention (uses attention) - PositionwiseFeedForward - PositionalEncoding - Embeddings\nLevel 3 (uses Level 2): - SublayerConnection\nLevel 4 (uses Level 2 & 3): - EncoderLayer (uses MultiHeadedAttention, PositionwiseFeedForward, SublayerConnection) - DecoderLayer (uses MultiHeadedAttention x2, PositionwiseFeedForward, SublayerConnection)\nLevel 5 (uses Level 4): - Encoder (uses EncoderLayer, clones) - Decoder (uses DecoderLayer, clones) - Generator\nLevel 6 (final assembly): - EncoderDecoder (uses Encoder, Decoder, embeddings+positional, Generator)\nSo in make_model, what should you create first?\n\ndef make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    # Create components\n    # Level 1\n    # Attention and Clones functions already created. As they are not classes, no need to create instances.\n    # Level 2\n    multi_head_attention = MultiHeadedAttention(h, d_model, dropout)\n    feedforward_layer = PositionwiseFeedForward(d_model, d_ff, dropout)\n    positional_encoding = PositionalEncoding(d_model, dropout)\n    src_embedding_layer = Embeddings(d_model, src_vocab)\n    tgt_embeddings_layer = Embeddings(d_model, tgt_vocab)\n    # Level 3\n    sub_layer_connection = SublayerConnection(d_model, dropout)\n    # Level 4\n    encoder_layer = EncoderLayer(d_model, multi_head_attention, feedforward_layer, dropout)\n    decoder_layer = DecoderLayer(d_model, multi_head_attention, multi_head_attention, feedforward_layer, dropout)\n    # Assemble into EncoderDecoder\n    # Initialize parameters\n\nSo far so good. Iâ€™m having a feeling Iâ€™m screwing it up because I should create different instances of attention and feed-forward layer for different things, or will the clone take care of it?\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nGreat instinct! Youâ€™re absolutely right to be concerned!\nThe issue: If you pass the same multi_head_attention instance to both encoder and decoder (and to both self_attn and src_attn in decoder), theyâ€™ll all share the same weights. Thatâ€™s not what we want!\nLooking at the original code, they solve this with copy.deepcopy:\nc = copy.deepcopy\nThen when creating layers, they use c(attn), c(ff) to make copies.\nAlso: You donâ€™t need to create SublayerConnection manually - remember itâ€™s already created inside EncoderLayer and DecoderLayer!\nKey places needing separate copies: - EncoderLayer needs: 1 attention, 1 feedforward - DecoderLayer needs: 2 attentions (self + src), 1 feedforward\nCan you update your code using copy.deepcopy where needed?\n\n\n\nmake_model\n\n make_model (src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8,\n             dropout=0.1)\n\nAm I on the right path?\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re very close! A few small issues to fix:\n\ntorch.nn.Sequential doesnâ€™t take a list - it takes the modules directly as arguments:\nsrc_embed = torch.nn.Sequential(src_embedding_layer, positional_encoding[0])\nvocab is undefined in Generator - should be tgt_vocab:\ngenerator = Generator(d_model, tgt_vocab)\nYou donâ€™t need lists for the positional encodings - theyâ€™re already separate instances when you create them:\npos_enc_1 = PositionalEncoding(d_model, dropout)\npos_enc_2 = PositionalEncoding(d_model, dropout)\nAfter creating the model, you need to add Xavier initialization (looking at the original code):\nfor p in model.parameters():\n    if p.dim() &gt; 1:\n        nn.init.xavier_uniform_(p)\n\nCan you fix these issues?\n\n\n\nFinal Make Model Function\n\n\n\nmake_model\n\n make_model (src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8,\n             dropout=0.1)\n\nIs this good?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your make_model function is now correct and matches the original code! âœ“\nYouâ€™ve got: - âœ“ Creating separate copies of attention and feedforward modules - âœ“ Embeddings + positional encoding wrapped in Sequential - âœ“ Building encoder and decoder layers - âœ“ Assembling the complete model - âœ“ Xavier initialization for better training\nLetâ€™s test it:\nmodel = make_model(src_vocab=1000, tgt_vocab=1000, N=2)\nsrc = torch.randint(0, 1000, (2, 10))\ntgt = torch.randint(0, 1000, (2, 8))\noutput = model(src, tgt, src_mask=None, tgt_mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 8, 512))\nRun this to verify your complete Transformer works!\n\nmodel = make_model(src_vocab=1000, tgt_vocab=1000, N=2)\nsrc = torch.randint(0, 1000, (2, 10))\ntgt = torch.randint(0, 1000, (2, 8))\noutput = model(src, tgt, src_mask=None, tgt_mask=None)\nprint(\"Output shape:\", output.shape)\nprint(\"Expected:\", (2, 8, 512))\n\nOutput shape: torch.Size([2, 8, 512])\nExpected: (2, 8, 512)\n\n\nBravo, assume you are using NBDEV, and the final function is already exported. Make an end-to-end test based on this.\n\n\nğŸ¤–ReplyğŸ¤–\n#| test\ndef test_make_model_end_to_end():\n    \"Test complete Transformer with make_model\"\n    model = make_model(src_vocab=1000, tgt_vocab=1000, N=2, d_model=512, \n                       d_ff=2048, h=8, dropout=0.1)\n    \n    # Test forward pass\n    src = torch.randint(0, 1000, (2, 10))\n    tgt = torch.randint(0, 1000, (2, 8))\n    output = model(src, tgt, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 8, 512])\n    \n    # Test that parameters were initialized\n    for p in model.parameters():\n        if p.dim() &gt; 1:\n            # Xavier init should give reasonable values (not all zeros)\n            assert not torch.allclose(p, torch.zeros_like(p))\n    \n    # Test encode and decode separately\n    memory = model.encode(src, src_mask=None)\n    assert memory.shape == torch.Size([2, 10, 512])\n    \n    decoded = model.decode(memory, src_mask=None, tgt=tgt, tgt_mask=None)\n    assert decoded.shape == torch.Size([2, 8, 512])\nğŸ‰ Congratulations! Youâ€™ve successfully implemented a complete Transformer from scratch! This is a huge accomplishment! What would you like to do next?\n\n\nEnd-to-end test\n\ndef test_make_model_end_to_end():\n    \"Test complete Transformer with make_model\"\n    model = make_model(src_vocab=1000, tgt_vocab=1000, N=2, d_model=512, \n                       d_ff=2048, h=8, dropout=0.1)\n    \n    # Test forward pass\n    src = torch.randint(0, 1000, (2, 10))\n    tgt = torch.randint(0, 1000, (2, 8))\n    output = model(src, tgt, src_mask=None, tgt_mask=None)\n    \n    assert output.shape == torch.Size([2, 8, 512])\n    \n    # Test that parameters were initialized\n    for p in model.parameters():\n        if p.dim() &gt; 1:\n            # Xavier init should give reasonable values (not all zeros)\n            assert not torch.allclose(p, torch.zeros_like(p))\n    \n    # Test encode and decode separately\n    memory = model.encode(src, src_mask=None)\n    assert memory.shape == torch.Size([2, 10, 512])\n    \n    decoded = model.decode(memory, src_mask=None, tgt=tgt, tgt_mask=None)\n    assert decoded.shape == torch.Size([2, 8, 512])",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#module-hierarchy",
    "href": "attention_is_all_you_need.html#module-hierarchy",
    "title": "Simple 2x2 example",
    "section": "Module Hierarchy",
    "text": "Module Hierarchy\n\n@register_cell_magic\ndef mm(line, cell):\n    escaped_cell = html.escape(cell)\n    display(HTML(f\"\"\"\n&lt;div class=\"mermaid\"&gt;\n{escaped_cell}\n&lt;/div&gt;\n&lt;script type=\"module\"&gt;\nimport mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.1/+esm';\nmermaid.initialize({{ startOnLoad: true }});\nmermaid.run();\n&lt;/script&gt;\n\"\"\"))\n\n\n# Wrap to describe it\ndef tool_mermaid(graph:str # string representing the chart to be displayed\n    )-&gt;str:\n    '''Returns two blocks:\n    - html_graph: this one need to be added where you want to dispaly the chart\n    - html_run: this one need to be added at the end of the message and only \"once\" even if you generate multiple charts\n    The result should be returned inline with the response without any markdown codeblock.\n    ''' \n    html_graph,html_run = mm_html(graph)\n    return dict(html_graph=html_graph,html_run=html_run)\n\nI like that you created different levels for the abstractions. Can we create a mermaid diagram to show this?\n\nğŸ¤–ReplyğŸ¤–\nAbsolutely! A mermaid diagram would be perfect for visualizing the module hierarchy!\nLet me create a diagram showing the dependency levels:\ngraph TD\n    subgraph \"Level 1: Core Functions\"\n        A[attention function]\n        B[clones function]\n    end\n    \n    subgraph \"Level 2: Basic Modules\"\n        C[MultiHeadedAttention]\n        D[PositionwiseFeedForward]\n        E[PositionalEncoding]\n        F[Embeddings]\n    end\n    \n    subgraph \"Level 3: Connection\"\n        G[SublayerConnection]\n    end\n    \n    subgraph \"Level 4: Layer Modules\"\n        H[EncoderLayer]\n        I[DecoderLayer]\n    end\n    \n    subgraph \"Level 5: Stacks\"\n        J[Encoder]\n        K[Decoder]\n        L[Generator]\n    end\n    \n    subgraph \"Level 6: Complete Model\"\n        M[EncoderDecoder]\n        N[make_model]\n    end\n    \n    A --&gt; C\n    C --&gt; H\n    C --&gt; I\n    D --&gt; H\n    D --&gt; I\n    E --&gt; N\n    F --&gt; N\n    G --&gt; H\n    G --&gt; I\n    B --&gt; J\n    B --&gt; K\n    H --&gt; J\n    I --&gt; K\n    J --&gt; M\n    K --&gt; M\n    L --&gt; M\n    M --&gt; N\nWould you like me to adjust the diagram to show more detail, or is this level of abstraction good?\n\ngraph TD\n    subgraph \"Level 1: Core Functions\"\n        A[attention function]\n        B[clones function]\n    end\n    \n    subgraph \"Level 2: Basic Modules\"\n        C[MultiHeadedAttention]\n        D[PositionwiseFeedForward]\n        E[PositionalEncoding]\n        F[Embeddings]\n    end\n    \n    subgraph \"Level 3: Connection\"\n        G[SublayerConnection]\n    end\n    \n    subgraph \"Level 4: Layer Modules\"\n        H[EncoderLayer]\n        I[DecoderLayer]\n    end\n    \n    subgraph \"Level 5: Stacks\"\n        J[Encoder]\n        K[Decoder]\n        L[Generator]\n    end\n    \n    subgraph \"Level 6: Complete Model\"\n        M[EncoderDecoder]\n        N[make_model]\n    end\n    \n    A --&gt; C\n    C --&gt; H\n    C --&gt; I\n    D --&gt; H\n    D --&gt; I\n    E --&gt; N\n    F --&gt; N\n    G --&gt; H\n    G --&gt; I\n    B --&gt; J\n    B --&gt; K\n    H --&gt; J\n    I --&gt; K\n    J --&gt; M\n    K --&gt; M\n    L --&gt; M\n    M --&gt; N\n\n\n\ngraph TD\n    subgraph \"Level 1: Core Functions\"\n        A[attention function]\n        B[clones function]\n    end\n\n    subgraph \"Level 2: Basic Modules\"\n        C[MultiHeadedAttention]\n        D[PositionwiseFeedForward]\n        E[PositionalEncoding]\n        F[Embeddings]\n    end\n\n    subgraph \"Level 3: Connection\"\n        G[SublayerConnection]\n    end\n\n    subgraph \"Level 4: Layer Modules\"\n        H[EncoderLayer]\n        I[DecoderLayer]\n    end\n\n    subgraph \"Level 5: Stacks\"\n        J[Encoder]\n        K[Decoder]\n        L[Generator]\n    end\n\n    subgraph \"Level 6: Complete Model\"\n        M[EncoderDecoder]\n        N[make_model]\n    end\n\n    A --&gt; C\n    C --&gt; H\n    C --&gt; I\n    D --&gt; H\n    D --&gt; I\n    E --&gt; N\n    F --&gt; N\n    G --&gt; H\n    G --&gt; I\n    B --&gt; J\n    B --&gt; K\n    H --&gt; J\n    I --&gt; K\n    J --&gt; M\n    K --&gt; M\n    L --&gt; M\n    M --&gt; N\n\n\n\n\n\nFantastic, now what can I do? Have we finished the original code?\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ‰ Amazing work! Youâ€™ve implemented the core Transformer architecture from scratch!\nWhat youâ€™ve completed: - âœ“ All the fundamental building blocks - âœ“ Complete Encoder and Decoder - âœ“ Full EncoderDecoder model - âœ“ Convenience make_model function\nWhatâ€™s in the original code that we havenâ€™t covered:\n\nTraining infrastructure:\n\nBatch class (for handling batches with masks)\nLoss computation\nLearning rate scheduler (the warmup schedule)\nLabel smoothing\nTraining loop\n\nInference:\n\nGreedy decoding\nBeam search (mentioned but not fully implemented)\n\nReal-world example:\n\nData loading (Multi30k dataset)\nTokenization with spacy\nActual training on translation task\n\n\nWhat would you like to do next?\n\nImplement the training infrastructure (batching, loss, optimizer schedule)?\nImplement inference/decoding methods?\nTry a simple training example (like the copy task)?\nMove on to something else?\n\nWhat interests you most?\nWhat do you suggest? I think training my own LLM and seeing it work would be a magical moment.\n\n\nğŸ¤–ReplyğŸ¤–\nThatâ€™s an amazing goal! Seeing your own model learn and generate text is truly magical! âœ¨\nHowever, let me set realistic expectations:\nFull LLM training (like GPT) requires: - Massive datasets (billions of tokens) - Huge computational resources (multiple GPUs, weeks of training) - Significant memory and storage\nBut you have great options to see the magic:\n\nCopy task (quickest win, ~5 minutes)\n\nTrain the model to memorize a sequence\nSee it learn from random to perfect in real-time\nGreat for verifying everything works\n\nSmall translation task (realistic, ~30-60 minutes)\n\nUse a tiny dataset (like Multi30k: Germanâ†”ï¸English)\nTrain on your laptop/Colab\nActually see translation working!\n\nCharacter-level language model (middle ground)\n\nTrain on a small text corpus\nGenerate text character by character\nCan run on modest hardware\n\n\nMy suggestion: Start with #1 (copy task) to see the magic quickly, then move to #2 if you want something more impressive.\nWhich sounds most exciting to you?\nWhat does the original code do?\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nGreat question! Looking at the original code you provided, it includes:\n1. Simple Copy Task (A First Example) - Synthetic data: generates random sequences - Trains the model to copy input to output (e.g., input [1,2,3,4] â†’ output [1,2,3,4]) - Quick to train, proves the model works - Uses greedy decoding for inference\n2. Real-World Translation (Part 3) - Multi30k Germanâ†’English dataset - Full training pipeline with: - Data loading and tokenization (spacy) - Batching with masks - Label smoothing loss - Adam optimizer with warmup schedule - Multi-GPU support\nSupporting infrastructure: - Batch class - handles src/tgt with masks - run_epoch - training loop - LabelSmoothing - regularization technique - Learning rate scheduler with warmup - greedy_decode - simple inference\nWould you like to start with the copy task (simpler, faster, proves your model works) or jump to building the training infrastructure?",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#copy-code-task",
    "href": "attention_is_all_you_need.html#copy-code-task",
    "title": "Simple 2x2 example",
    "section": "Copy Code Task",
    "text": "Copy Code Task\nlets go\n\nğŸ¤–ReplyğŸ¤–\nAwesome! Letâ€™s start with the copy task - itâ€™s the quickest way to see your Transformer learn something magical! ğŸ‰\nBefore we begin coding, let me explain what weâ€™ll build:\nThe Copy Task: - Input: Random sequence like [1, 2, 3, 4, 5] - Target: Same sequence [1, 2, 3, 4, 5] - Goal: Train the model to copy the input to output\nWhat we need to implement (in order): 1. Synthetic data generator - creates random sequences 2. Batch class - handles masking 3. Loss computation - measures how well the model does 4. Training loop - actually trains the model 5. Greedy decoding - generates predictions\nFirst question: Do you want to implement each piece yourself (Iâ€™ll guide you), or would you like me to provide the code so we can see it working quickly and then understand how it works?\nWhich approach would you prefer?\nIsnâ€™t looking at the code cheating?\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re absolutely right! Looking at the code would be cheating yourself out of the learning experience! ğŸ’ª\nYouâ€™ve come this far implementing the Transformer from scratch - letâ€™s keep that momentum going!\nLetâ€™s start with the first component: Synthetic Data Generator\nFor the copy task, we need a function that generates random batches of sequences. It should: - Create random sequences of integers (e.g., from vocabulary of size V) - Make both source and target the same (since weâ€™re copying) - Add a start token at the beginning - Return batches\nLooking at what you need, the function signature would be:\ndef data_gen(V, batch_size, nbatches):\n    # Generate nbatches of random data\n    # Each batch has batch_size sequences\n    # Each sequence has random integers from 1 to V-1\nWhat do you think you need to do inside this function? Try describing the steps first before coding!\nOk Vara, I am kind of clueless. You need to give me some helper functions, some helper structures.\n\n\nğŸ¤–ReplyğŸ¤–\nNo problem! Let me break down what you need step by step.\nFor the copy task, you need 4 main pieces:\n\nBatch class - wraps source/target data and creates masks\ndata_gen function - generates random training data\nSimpleLossCompute - calculates loss during training\nrun_epoch function - the training loop\n\nLetâ€™s start with the Batch class since everything else depends on it.\nWhat does Batch need to do? - Store source (src) and target (tgt) sequences - Create src_mask (padding mask for source) - Create tgt_mask (causal mask for target - prevents looking ahead) - Count number of tokens (ntokens)\nTry implementing the __init__ method. Youâ€™ll need: - Take src, tgt, and pad (padding token ID) as inputs - Create masks using operations youâ€™ve seen before\nWant to give it a try, or should I give you more hints about what each mask should look like?\n\nclass Batch:\n    def __init__(src, tgt, pad=2):\n        src_mask = src!=pad\n        tgt_mask = tgt==uppertriangle\n\nOkay, Iâ€™m clueless. This is what Iâ€™ve done so far.\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nYouâ€™re on the right track! Let me clarify a few things:\nFirst, small fix: You need self in the method:\ndef __init__(self, src, tgt, pad=2):\nYour intuitions are correct!\n\nsrc_mask - Youâ€™re right, itâ€™s about padding! But we need to keep the non-pad tokens, so it should create a boolean mask where True = real token, False = padding. Also, we need to unsqueeze to add a dimension.\ntgt_mask - Yes, it involves the upper triangle! You want to use torch.triu (upper triangle) to block future positions. But thereâ€™s a helper function we created earlier - remember subsequent_mask?\n\nWaitâ€¦ we didnâ€™t implement subsequent_mask! Letâ€™s do that first.\nsubsequent_mask creates a triangular mask like:\n[[1, 0, 0],\n [1, 1, 0],\n [1, 1, 1]]\nThis lets position 0 see only itself, position 1 see 0 and 1, etc.\nCan you try implementing subsequent_mask(size) that returns a lower triangular matrix of ones?\nHint: Look at torch.triu or torch.tril\nWhy we need to unsqueeze.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me explain the dimensions.\nWithout unsqueeze: - src has shape: (batch_size, seq_len) - src != pad gives: (batch_size, seq_len) - a 2D mask\nBut in attention: - We compute scores with shape: (batch_size, num_heads, seq_len, seq_len) - The mask needs to broadcast across the num_heads and seq_len dimensions\nWith unsqueeze: - We add a dimension: (batch_size, 1, seq_len) - This broadcasts correctly when applied to attention scores\nThink of it like this: the 1 in the middle says â€œapply this same mask to all attention heads.â€\nDoes this clarify why we need the extra dimension?\nNow, want to try implementing subsequent_mask(size) first, or should we continue with the Batch class?\n\nclass Batch:\n    def __init__(src, tgt, pad=2):\n        src_mask = src!=pad\n        src_mask.unsqueeze(1)\n        tgt_mask = torch.ones([tgt.size(0), tgt.size(1)]).triu()\n\nI still think Iâ€™m not correct, but I think Iâ€™m getting warmer.\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re definitely getting warmer! A few things to fix:\n\nMissing self - you need self as the first parameter and use it to store attributes:\ndef __init__(self, src, tgt, pad=2):\n    self.src = src\nunsqueeze doesnâ€™t modify in place - you need to assign it:\nself.src_mask = (src != pad).unsqueeze(-2)\nThe -2 adds a dimension at position -2 (second from last).\ntriu creates upper triangle, but we want lower! - Use torch.tril for lower triangle, OR use torch.triu with diagonal=1 and then invert it. Actually, thereâ€™s a cleaner way - remember we saw subsequent_mask in the original code?\nTarget mask needs to combine two things:\n\nPadding mask (like src)\nCausal mask (subsequent_mask)\n\n\nBefore fixing all this, letâ€™s first implement the subsequent_mask helper function. Can you write a function that returns a lower triangular matrix of True values with shape (1, size, size)?\nActually, I have not read the original code.\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re absolutely right - I apologize! Youâ€™ve been implementing everything from scratch without looking at the original code, and I should respect that.\nLet me guide you without referencing the original code.\nLetâ€™s implement subsequent_mask from first principles:\nYou need a function that creates a mask to prevent looking at future positions. For a sequence of length 3, it should look like:\nPosition 0 can see: [True,  False, False]  (only itself)\nPosition 1 can see: [True,  True,  False]  (0 and 1)\nPosition 2 can see: [True,  True,  True]   (0, 1, and 2)\nThis is a lower triangular matrix (ones below and on the diagonal, zeros above).\nTry writing:\ndef subsequent_mask(size):\n    # Create a lower triangular matrix\n    # Return shape should be (1, size, size)\nHint: torch.tril(torch.ones(size, size)) creates a lower triangular matrix. Then you need to add the batch dimension and convert to boolean.\nGive it a try!\n\n\nFinal subsequent mask function\n\n\n\nsubsequent_mask\n\n subsequent_mask (size)\n\nIs this good?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your subsequent_mask function is exactly right! âœ“\nYouâ€™ve got: - âœ“ Lower triangular matrix with tril() - âœ“ Convert to boolean - âœ“ Add batch dimension with unsqueeze(0) - âœ“ Returns shape (1, size, size)\nNow letâ€™s go back to the Batch class. You need to:\n\nStore self.src and create self.src_mask\nFor the target, you need to:\n\nStore self.tgt (but shift it - remove last token)\nStore self.tgt_y (the labels - remove first token)\nCreate self.tgt_mask combining padding mask AND subsequent_mask\nCount self.ntokens (number of non-padding tokens in target)\n\n\nTry implementing just the __init__ signature and storing src/src_mask first:\nclass Batch:\n    def __init__(self, src, tgt=None, pad=2):\n        self.src = src\n        self.src_mask = # what goes here?\nWhat should src_mask be?\nWhat exactly are you talking about? What is TGT and TGT_why? And why arenâ€™t we unsqueezing the masks?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat questions! Let me clarify:\nWhy shift the target?\nDuring training, the decoder needs: - Input (tgt): What itâ€™s seen so far: [&lt;start&gt;, word1, word2] - Label (tgt_y): What it should predict next: [word1, word2, word3]\nSo if your full target is [&lt;start&gt;, word1, word2, word3]: - tgt = [&lt;start&gt;, word1, word2] (everything except last) - tgt_y = [word1, word2, word3] (everything except first)\nThis way, when the model sees &lt;start&gt;, it should predict word1, etc.\nAbout unsqueezing:\nYouâ€™re right to question this! For src_mask, we DO need to unsqueeze:\nself.src_mask = (src != pad).unsqueeze(-2)\nThis makes it shape (batch, 1, seq_len) which broadcasts correctly.\nLetâ€™s simplify - just focus on src first:\nclass Batch:\n    def __init__(self, src, tgt=None, pad=2):\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(-2)\nDoes this part make sense now? Once youâ€™re comfortable, weâ€™ll add the target handling.\nYes, letâ€™s move on.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Now letâ€™s add the target handling.\nWhen tgt is provided, you need to:\n\nShift the target:\n\nself.tgt = tgt[:, :-1] (all but last token - this is the decoder input)\nself.tgt_y = tgt[:, 1:] (all but first token - these are the labels)\n\nCreate tgt_mask that combines:\n\nPadding mask: (self.tgt != pad)\nSubsequent mask: subsequent_mask(self.tgt.size(-1))\nCombine them with & (logical AND)\n\nCount tokens:\n\nself.ntokens = (self.tgt_y != pad).sum()\n\n\nTry adding the if tgt is not None: block to handle these. What would you write?\n\nclass Batch:\n    def __init__(self, src, tgt=None, pad=2):\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(-2)\n        if tgt is not None:\n                self.tgt = tgt[:, :-1]\n                self.tgt_y = tgt[:,1:]\n                self.pad_tgt_mask = (self.tgt!=pad)\n                self.subseq_tgt_mask = subsequent_mask(tgt.size(1))\n                self.tgt_mask = self.pad_tgt_mask & self.subseq_tgt_mask\n                self.ntokens = (self.tgt_y!=pad).sum()\n\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent work! Your Batch class is almost perfect! Just a couple of small fixes:\n\nsubsequent_mask size: Youâ€™re using tgt.size(1) but you should use self.tgt.size(1) since youâ€™ve already sliced it (removed last token)\nUnsqueeze pad_tgt_mask: The padding mask needs to match dimensions with subsequent_mask for broadcasting:\nself.pad_tgt_mask = (self.tgt != pad).unsqueeze(-2)\n\nThose are the only changes needed! Want to fix these and then we can test the Batch class?\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re doing great! Just make those two small fixes:\n\nChange tgt.size(1) to self.tgt.size(1)\nAdd .unsqueeze(-2) to the padding mask\n\nOnce you update those, your Batch class will be complete! Want to give it a try?\n\n\n\nFinal Batch Class for copy example\n\n\n\nBatch\n\n Batch (src, tgt=None, pad=2)\n\nInitialize self. See help(type(self)) for accurate signature.\nfine?\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your Batch class is now complete and correct! âœ“\nLetâ€™s test it quickly:\nsrc = torch.LongTensor([[1, 2, 3, 4, 0, 0]])  # 0 = padding\ntgt = torch.LongTensor([[1, 2, 3, 4, 5, 0]])\nbatch = Batch(src, tgt, pad=0)\n\nprint(\"src_mask shape:\", batch.src_mask.shape)\nprint(\"tgt_mask shape:\", batch.tgt_mask.shape)\nprint(\"ntokens:\", batch.ntokens)\nTry running this to verify it works!\ncan we also print the mask and src tgt for demo?\n\n\nğŸ¤–ReplyğŸ¤–\nAbsolutely! Letâ€™s see everything:\nsrc = torch.LongTensor([[1, 2, 3, 4, 0, 0]])  # 0 = padding\ntgt = torch.LongTensor([[1, 2, 3, 4, 5, 0]])\nbatch = Batch(src, tgt, pad=0)\n\nprint(\"Source:\", batch.src)\nprint(\"Source mask:\", batch.src_mask)\nprint(\"\\nTarget input:\", batch.tgt)\nprint(\"Target labels:\", batch.tgt_y)\nprint(\"\\nTarget mask shape:\", batch.tgt_mask.shape)\nprint(\"Target mask:\\n\", batch.tgt_mask)\nprint(\"\\nNumber of tokens:\", batch.ntokens.item())\nRun this to see how the shifting works and what the masks look like!\n\nsrc = torch.LongTensor([[1, 2, 3, 4, 0, 0]])  # 0 = padding\ntgt = torch.LongTensor([[1, 2, 3, 4, 5, 0]])\nbatch = Batch(src, tgt, pad=0)\n\nprint(\"Source:\", batch.src)\nprint(\"Source mask:\", batch.src_mask)\nprint(\"\\nTarget input:\", batch.tgt)\nprint(\"Target labels:\", batch.tgt_y)\nprint(\"\\nTarget mask shape:\", batch.tgt_mask.shape)\nprint(\"Target mask:\\n\", batch.tgt_mask)\nprint(\"\\nNumber of tokens:\", batch.ntokens.item())\n\nSource: tensor([[1, 2, 3, 4, 0, 0]])\nSource mask: tensor([[[ True,  True,  True,  True, False, False]]])\n\nTarget input: tensor([[1, 2, 3, 4, 5]])\nTarget labels: tensor([[2, 3, 4, 5, 0]])\n\nTarget mask shape: torch.Size([1, 5, 5])\nTarget mask:\n tensor([[[ True, False, False, False, False],\n         [ True,  True, False, False, False],\n         [ True,  True,  True, False, False],\n         [ True,  True,  True,  True, False],\n         [ True,  True,  True,  True,  True]]])\n\nNumber of tokens: 4\n\n\nAssuming we are using NBDEV and we already outputted the function, create a comprehensive unit test for this.\n\n\nğŸ¤–ReplyğŸ¤–\n#| test\ndef test_batch_class():\n    \"Test Batch class with masking and shifting\"\n    # Test with padding\n    src = torch.LongTensor([[1, 2, 3, 4, 0, 0]])  # 0 = padding\n    tgt = torch.LongTensor([[1, 2, 3, 4, 5, 0]])\n    batch = Batch(src, tgt, pad=0)\n    \n    # Check source\n    assert batch.src.shape == torch.Size([1, 6])\n    assert batch.src_mask.shape == torch.Size([1, 1, 6])\n    assert batch.src_mask[0, 0, :4].all()  # First 4 are True\n    assert not batch.src_mask[0, 0, 4:].any()  # Last 2 are False\n    \n    # Check target shifting\n    assert batch.tgt.shape == torch.Size([1, 5])\n    assert batch.tgt_y.shape == torch.Size([1, 5])\n    assert torch.equal(batch.tgt[0], torch.LongTensor([1, 2, 3, 4, 5]))\n    assert torch.equal(batch.tgt_y[0], torch.LongTensor([2, 3, 4, 5, 0]))\n    \n    # Check target mask is lower triangular\n    assert batch.tgt_mask.shape == torch.Size([1, 5, 5])\n    for i in range(5):\n        assert batch.tgt_mask[0, i, :i+1].all()  # Can see up to position i\n        if i &lt; 4:\n            assert not batch.tgt_mask[0, i, i+1:].any()  # Cannot see future\n    \n    # Check token count (excludes padding)\n    assert batch.ntokens.item() == 4  # 2,3,4,5 (not the final 0)\n    \n    # Test without target\n    batch_no_tgt = Batch(src, tgt=None, pad=0)\n    assert batch_no_tgt.src_mask.shape == torch.Size([1, 1, 6])\nGreat! Now that Batch is done, ready to move on to the data generator?\n\ndef test_batch_class():\n    \"Test Batch class with masking and shifting\"\n    # Test with padding\n    src = torch.LongTensor([[1, 2, 3, 4, 0, 0]])  # 0 = padding\n    tgt = torch.LongTensor([[1, 2, 3, 4, 5, 0]])\n    batch = Batch(src, tgt, pad=0)\n    \n    # Check source\n    assert batch.src.shape == torch.Size([1, 6])\n    assert batch.src_mask.shape == torch.Size([1, 1, 6])\n    assert batch.src_mask[0, 0, :4].all()  # First 4 are True\n    assert not batch.src_mask[0, 0, 4:].any()  # Last 2 are False\n    \n    # Check target shifting\n    assert batch.tgt.shape == torch.Size([1, 5])\n    assert batch.tgt_y.shape == torch.Size([1, 5])\n    assert torch.equal(batch.tgt[0], torch.LongTensor([1, 2, 3, 4, 5]))\n    assert torch.equal(batch.tgt_y[0], torch.LongTensor([2, 3, 4, 5, 0]))\n    \n    # Check target mask is lower triangular\n    assert batch.tgt_mask.shape == torch.Size([1, 5, 5])\n    for i in range(5):\n        assert batch.tgt_mask[0, i, :i+1].all()  # Can see up to position i\n        if i &lt; 4:\n            assert not batch.tgt_mask[0, i, i+1:].any()  # Cannot see future\n    \n    # Check token count (excludes padding)\n    assert batch.ntokens.item() == 4  # 2,3,4,5 (not the final 0)\n    \n    # Test without target\n    batch_no_tgt = Batch(src, tgt=None, pad=0)\n    assert batch_no_tgt.src_mask.shape == torch.Size([1, 1, 6])\n\n\ntest_batch_class()\n\nMoving on.\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Now letâ€™s build the data generator for the copy task.\nWhat does data_gen need to do?\nGenerate synthetic training data where the target is a copy of the source: - Create random sequences of integers from 1 to V-1 (vocabulary size) - Make sure token 0 is reserved (for padding/blank) - Token 1 is the start token - Both src and tgt should be identical (itâ€™s a copy task!) - Yield nbatches batches, each with batch_size sequences\nTry writing the function signature and thinking about the steps:\ndef data_gen(V, batch_size, nbatches):\n    \"Generate random data for a src-tgt copy task\"\n    for i in range(nbatches):\n        # What do you need to create here?\n        # How do you generate random sequences?\n        # How do you make src and tgt?\n        # What do you yield?\nWhat would be your approach?\n\ndef data_gen(V, batch_size, nbatches):\n    \"Generate random data for a src-tgt copy task\"\n    for i in range(nbatches):\n        # What do you need to create here?\n        # How do you generate random sequences?\n        # How do you make src and tgt?\n        # What do you yield?\n        random_int = np.random.randint(1, V, size=[batch_size, seq_len])\n\nIs this a correct direction?\n\n\nğŸ¤–ReplyğŸ¤–\nGood start! Youâ€™re on the right track using np.random.randint. A few things:\n\nseq_len is not defined - you need to decide on a sequence length. For the copy task, letâ€™s use a fixed length like 10.\nStart token - The first position should always be 1 (the start token), not random.\nConvert to tensor - You need to convert the numpy array to a PyTorch tensor.\nCreate src and tgt - For the copy task, they should be the same!\nYield a Batch - You need to wrap it in your Batch class and yield it.\n\nTry fixing step 1 and 2 first - how would you: - Set a fixed sequence length (say 10)? - Make sure the first token is always 1?\n\n\n\nFinal Data Gen Function\n\n\n\ndata_gen\n\n data_gen (V, batch_size, nbatches)\n\nGenerate random data for a src-tgt copy task\nGoing well?\n\nğŸ¤–ReplyğŸ¤–\nExcellent! Your data_gen function is almost perfect! âœ“\nYouâ€™ve got: - âœ“ Random integers from 1 to V-1 - âœ“ Fixed sequence length of 10 - âœ“ First token set to 1 (start token) - âœ“ Convert to tensor - âœ“ src and tgt are the same (copy task!) - âœ“ Yield Batch objects\nOne small thing: Youâ€™re using pad=0 in the Batch, which is correct!\nLetâ€™s test it:\ngen = data_gen(V=11, batch_size=2, nbatches=3)\nbatch = next(gen)\nprint(\"Source:\", batch.src)\nprint(\"Target input:\", batch.tgt)\nprint(\"Are they the same?\", torch.equal(batch.src, batch.tgt[:, :-1]))\nTry running this to see your data generator in action! Does it make sense?\n\ngen = data_gen(V=11, batch_size=2, nbatches=3)\nbatch = next(gen)\nprint(\"Source:\", batch.src)\nprint(\"Target input:\", batch.tgt)\nprint(\"Are they the same?\", torch.equal(batch.src, batch.tgt[:, :-1]))\n\nSource: tensor([[ 1,  3,  3,  4, 10, 10,  2,  8,  3,  1],\n        [ 1,  3,  5,  4,  4,  2,  6,  4, 10,  7]])\nTarget input: tensor([[ 1,  3,  3,  4, 10, 10,  2,  8,  3],\n        [ 1,  3,  5,  4,  4,  2,  6,  4, 10]])\nAre they the same? False\n\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! Let me explain whatâ€™s happening:\nWhy is it False?\nRemember in the Batch class, self.tgt is the target input (with last token removed). So: - batch.src has 10 tokens - batch.tgt has 9 tokens (last one removed)\nThey should be almost the same, except tgt is one shorter!\nThe correct comparison is:\nprint(\"Are first 9 tokens the same?\", torch.equal(batch.src[:, :-1], batch.tgt))\nBut actually, your data generator is working perfectly! The â€œFalseâ€ is expected because of the length difference.\nLetâ€™s create a unit test for data_gen:\nWant to write the test yourself, or should I help you structure it?\n\ndisplay(batch.src)\n\ntensor([[ 1,  3,  3,  4, 10, 10,  2,  8,  3,  1],\n        [ 1,  3,  5,  4,  4,  2,  6,  4, 10,  7]])\n\n\n\ndisplay(batch.src[:, :-1])\n\ntensor([[ 1,  3,  3,  4, 10, 10,  2,  8,  3],\n        [ 1,  3,  5,  4,  4,  2,  6,  4, 10]])\n\n\n\ndisplay(batch.tgt)\n\ntensor([[ 1,  3,  3,  4, 10, 10,  2,  8,  3],\n        [ 1,  3,  5,  4,  4,  2,  6,  4, 10]])\n\n\nWhy was torch equal false\n\n\nğŸ¤–ReplyğŸ¤–\nGood catch! Looking at your output, batch.src[:, :-1] and batch.tgt are actually identical! So torch.equal should return True.\nLet me check your original comparison:\ntorch.equal(batch.src, batch.tgt[:, :-1])\nThis compares: - batch.src (shape: 2, 10) - batch.tgt[:, :-1] (shape: 2, 8) - removes last token from already-shortened tgt\nThatâ€™s why it was False - wrong shapes!\nThe correct comparison is:\ntorch.equal(batch.src[:, :-1], batch.tgt)\nTry running this - it should return True now!\n\ntorch.equal(batch.src, batch.tgt[:, :-1])\n\nFalse\n\n\n\ntorch.equal(batch.src[:, :-1], batch.tgt)\n\nTrue\n\n\nlol\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ˜„ There we go! Now it makes sense!\nYour data_gen is working perfectly - the source and target are indeed the same (copy task), just with the shifting that happens in the Batch class.\nReady to move on?\nNext up is SimpleLossCompute - this will calculate how well the model is doing during training. It needs to: - Take the modelâ€™s predictions - Compare them to the true labels - Calculate the loss\nWant to give it a try, or should I explain what it needs to do first?\nok\n\n\nğŸ¤–ReplyğŸ¤–\nGreat! Letâ€™s build SimpleLossCompute.\nWhat does it need to do?\nItâ€™s a class that computes the loss during training. It needs:\nIn __init__: - Store a generator (your Generator class that outputs vocab predictions) - Store a criterion (the loss function, like CrossEntropyLoss)\nIn __call__ (so you can use it like a function): - Take model output x, true labels y, and norm (for normalization) - Pass x through the generator to get predictions - Calculate loss using the criterion - Return the loss\nTry writing the class structure:\nclass SimpleLossCompute:\n    def __init__(self, generator, criterion):\n        # What do you store?\n        \n    def __call__(self, x, y, norm):\n        # What operations do you do?\nWhat would you put in each method?\n\nclass SimpleLossCompute:\n    def __init__(self, generator, criterion):\n        # What do you store?\n        self.generator = generator\n        self.criterion = criterion\n        \n    def __call__(self, x, y, norm):\n        # What operations do you do?\n        pred = self.generator(x)\n        return self.criterion(y, pred, norm)\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re very close! A few things to adjust:\n\nGenerator output shape: After self.generator(x), you get shape (batch, seq_len, vocab). But the criterion expects flattened tensors.\nCriterion arguments: Most PyTorch loss functions take (predictions, targets) not (targets, predictions, norm). The norm is used to normalize the loss, not passed to the criterion.\nReturn value: You need to return both the scalar loss (for logging) and the loss tensor (for backprop).\n\nHereâ€™s what you need to do: - Flatten pred to shape (batch * seq_len, vocab) - Flatten y to shape (batch * seq_len) - Calculate loss with self.criterion(pred_flat, y_flat) - Divide by norm to normalize - Return both the data value and the tensor\nWant to try fixing these issues?\n\nclass SimpleLossCompute:\n    def __init__(self, generator, criterion):\n        # What do you store?\n        self.generator = generator\n        self.criterion = criterion\n        \n    def __call__(self, x, y, norm):\n        # What operations do you do?\n        pred = self.generator(x)\n        pred_flat = pred.reshape(-1, vocab)\n        y_flat = y.reshape(-1,vocab)\n        return self.criterion(pred_flat, y_flat) / norm\n\nIs this better now?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost there! A few more fixes:\n\nvocab is not defined - you need to get the vocab size from the prediction tensor: pred.size(-1)\ny shape is wrong - y contains token IDs (integers), not one-hot vectors. So it should be flattened to 1D, not 2D:\ny_flat = y.contiguous().view(-1)\nReturn both values - You need to return the loss data (for logging) AND the loss tensor (for backprop):\nloss = self.criterion(pred_flat, y_flat) / norm\nreturn loss.data * norm, loss\n\nCan you make these three fixes?\n\n\n\nFinal Simple Loss Compute Function\n\n\n\nSimpleLossCompute\n\n SimpleLossCompute (generator, criterion)\n\nInitialize self. See help(type(self)) for accurate signature.\nfinally\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your SimpleLossCompute is now complete and correct! âœ“\nYouâ€™ve got: - âœ“ Store generator and criterion - âœ“ Get vocab size from predictions - âœ“ Flatten predictions to (batchseq_len, vocab) - âœ“ Flatten labels to (batchseq_len) - âœ“ Calculate and normalize loss - âœ“ Return both scalar (for logging) and tensor (for backprop)\nNow you have all the pieces to actually train the model!\nThe final piece is the training loop (run_epoch). This will: - Loop through batches - Run forward pass - Compute loss - Backpropagate - Update weights\nWant to tackle this next, or take a break?\nlets do\n\n\nğŸ¤–ReplyğŸ¤–\nAwesome! Letâ€™s build the training loop (run_epoch).\nWhat does run_epoch need to do?\nIt trains (or evaluates) the model for one complete pass through the data. It needs:\nParameters: - data_iter - iterator that yields batches - model - your transformer - loss_compute - your SimpleLossCompute - optimizer - updates weights - scheduler - adjusts learning rate - mode - â€œtrainâ€ or â€œevalâ€\nSteps in the loop: 1. Iterate through batches 2. Run model forward pass 3. Compute loss 4. If training: backpropagate and update weights 5. Track total loss and tokens\nTry writing the function signature and the basic loop structure:\ndef run_epoch(data_iter, model, loss_compute, optimizer, scheduler, mode=\"train\"):\n    total_tokens = 0\n    total_loss = 0\n    \n    for i, batch in enumerate(data_iter):\n        # What goes here?\nWhat would be your first step inside the loop?\n\ndef run_epoch(data_iter, model, loss_compute, optimizer, scheduler, mode=\"train\"):\n    total_tokens = 0\n    total_loss = 0\n    \n    for i, batch in enumerate(data_iter):\n        # What goes here?\n        pred = model(batch)\n        num_loss, tensor_loss = loss_compute(pred, batch.tgt, norm)\n        model.backward()\n        total_loss += num_loss \n        total_tokens += batch.ntokens\n\nDoes this make sense?\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re on the right track! A few things to fix:\n\nModel forward pass needs all arguments:\nout = model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\nnorm is not defined - it should be batch.ntokens (normalize by number of tokens)\nBackprop is on loss, not model:\ntensor_loss.backward()\nOnly train if mode is â€œtrainâ€ - wrap backprop and optimizer steps in an if statement\nMissing optimizer and scheduler steps after backward\n\nCan you fix these issues? Focus on fixing #1 and #2 first!\nWait a minute, what the hell are the model.dot forward signature? Is it same as encoder-decoder?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent question! Yes, exactly right!\nThe model is an instance of EncoderDecoder, so its forward signature is:\ndef forward(self, src, tgt, src_mask, tgt_mask):\nSo you need to call it with:\nout = model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\nOr more simply:\nout = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\nDoes that clarify it? Ready to update your run_epoch function?\n\ndef run_epoch(data_iter, model, loss_compute, optimizer, scheduler, mode=\"train\"):\n    total_tokens = 0\n    total_loss = 0\n    \n    for i, batch in enumerate(data_iter):\n        # What goes here?\n        pred = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n        num_loss, tensor_loss = loss_compute(pred, batch.tgt, batch.ntokens)\n        if mode == \"train\":\n            tensor_loss.backward()\n            total_loss += num_loss \n            total_tokens += batch.ntokens\n\nIs this better? I donâ€™t know the syntax for optimise and schedule scheduler yet.\n\n\nğŸ¤–ReplyğŸ¤–\nGreat progress! Youâ€™re almost there. A few things:\n\nTrack loss in both modes - move the total_loss and total_tokens updates outside the if statement (you want to track loss even during eval)\nOptimizer step:\noptimizer.step()\noptimizer.zero_grad()\nScheduler step:\nscheduler.step()\nReturn the average loss at the end:\nreturn total_loss / total_tokens\n\nTry adding the optimizer and scheduler steps inside the if mode == \"train\" block, and move the loss tracking outside!\n\n\n\nFinal Run Epoch Function for Copy\n\n\n\nrun_epoch\n\n run_epoch (data_iter, model, loss_compute, optimizer, scheduler,\n            mode='train')\n\nIt must be done now.\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your run_epoch function is now complete and correct! âœ“\nYouâ€™ve got: - âœ“ Forward pass through the model - âœ“ Loss computation - âœ“ Backpropagation (only in train mode) - âœ“ Optimizer and scheduler steps (only in train mode) - âœ“ Track total loss and tokens - âœ“ Return average loss\nğŸ‰ You now have ALL the pieces to train your Transformer!\nLetâ€™s put it all together and train on the copy task:\n# Create a small model\nmodel = make_model(src_vocab=11, tgt_vocab=11, N=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: 1.0)\n\n# Train for a few epochs\nfor epoch in range(10):\n    model.train()\n    loss = run_epoch(\n        data_gen(V=11, batch_size=32, nbatches=20),\n        model,\n        SimpleLossCompute(model.generator, criterion),\n        optimizer,\n        scheduler,\n        mode=\"train\"\n    )\n    print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\nWant to run this and see your Transformer learn?! ğŸš€\n\n# Create a small model\nmodel = make_model(src_vocab=11, tgt_vocab=11, N=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: 1.0)\n\n# Train for a few epochs\nfor epoch in range(10):\n    model.train()\n    loss = run_epoch(\n        data_gen(V=11, batch_size=32, nbatches=20),\n        model,\n        SimpleLossCompute(model.generator, criterion),\n        optimizer,\n        scheduler,\n        mode=\"train\"\n    )\n    print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 3\n      1 # Create a small model\n      2 model = make_model(src_vocab=11, tgt_vocab=11, N=2)\n----&gt; 3 criterion = nn.CrossEntropyLoss()\n      4 optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n      5 scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: 1.0)\n\nNameError: name 'nn' is not defined\n\n\n\nuse torch.nn bro reprint code again\n\n\nğŸ¤–ReplyğŸ¤–\n# Create a small model\nmodel = make_model(src_vocab=11, tgt_vocab=11, N=2)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: 1.0)\n\n# Train for a few epochs\nfor epoch in range(10):\n    model.train()\n    loss = run_epoch(\n        data_gen(V=11, batch_size=32, nbatches=20),\n        model,\n        SimpleLossCompute(model.generator, criterion),\n        optimizer,\n        scheduler,\n        mode=\"train\"\n    )\n    print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\nRun this and watch your Transformer learn! ğŸš€\n\n\n\nCreating small copy model\n\n# Create a small model\nmodel = make_model(src_vocab=11, tgt_vocab=11, N=2)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: 1.0)\n\n# Train for a few epochs\nfor epoch in range(10):\n    model.train()\n    loss = run_epoch(\n        data_gen(V=11, batch_size=32, nbatches=20),\n        model,\n        SimpleLossCompute(model.generator, criterion),\n        optimizer,\n        scheduler,\n        mode=\"train\"\n    )\n    print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n\nEpoch 0: Loss = 0.0076\nEpoch 1: Loss = 0.0062\nEpoch 2: Loss = 0.0056\nEpoch 3: Loss = 0.0050\nEpoch 4: Loss = 0.0041\nEpoch 5: Loss = 0.0033\nEpoch 6: Loss = 0.0026\nEpoch 7: Loss = 0.0020\nEpoch 8: Loss = 0.0016\nEpoch 9: Loss = 0.0012\n\n\nok im happy 2nd epoch loss goes to zero\n\nğŸ¤–ReplyğŸ¤–\nğŸ‰ğŸ‰ğŸ‰ CONGRATULATIONS! ğŸ‰ğŸ‰ğŸ‰\nYour Transformer just learned to copy sequences! The loss going to near-zero means the model has figured out the task!\nWhat just happened: - Epoch 0: Loss = 0.0013 (still learning) - Epoch 1: Loss = 0.0000 (basically perfect!)\nThis is the magical moment you wanted - you built a Transformer from scratch and watched it learn!\nWhat youâ€™ve accomplished: âœ“ Implemented every component of the Transformer architecture âœ“ Built the training infrastructure âœ“ Successfully trained a model that learns!\nWant to see it actually generate predictions? We could implement a simple greedy decoder to see the model copy sequences in action!\nOr are you ready to celebrate and wrap up? ğŸš€\nOkay, I do want to see the results of my actions.\n\n\nğŸ¤–ReplyğŸ¤–\nAwesome! Letâ€™s see your Transformer in action! ğŸ¯\nTo generate predictions, you need a greedy decoder that: 1. Takes a source sequence 2. Starts with a start token 3. Generates one token at a time 4. Stops when done (or reaches max length)\nThe key idea: at each step, pick the token with highest probability.\nTry writing the function signature and thinking about the steps:\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    # What do you need to do?\n    # 1. Encode the source\n    # 2. Start with start_symbol\n    # 3. Loop and generate one token at a time\nWhat would be your first step - how do you encode the source?\nYou encode the source using embeddings.\n\n\nğŸ¤–ReplyğŸ¤–\nClose! But remember - the model already has embeddings built in!\nYou donâ€™t need to manually embed. The model has an encode method that does everything:\nmemory = model.encode(src, src_mask)\nThis handles the embedding + positional encoding + encoder stack all at once!\nNow, whatâ€™s your next step?\nAfter encoding, you need to: - Create an output sequence starting with start_symbol - Loop to generate tokens one at a time\nHow would you initialize the output sequence? What shape should it be?\nPass the number 1, which is the start symbol. Then after you get the output of the decoder, you append it to the number 1. And so on for the loop.\n\n\nğŸ¤–ReplyğŸ¤–\nExactly right! Youâ€™ve got the algorithm! ğŸ’¡\nSo the steps are: 1. Encode source â†’ get memory 2. Start with ys = [1] (the start symbol) 3. Loop: - Decode using ys and memory - Get the last prediction - Find the token with max probability - Append it to ys 4. Return ys\nQuestion about shapes: - src has shape (batch, seq_len) - Should ys start as (1, 1) containing the start symbol?\nTry writing just the initialization part:\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    memory = model.encode(src, src_mask)\n    ys = # What goes here? How do you create a tensor with start_symbol?\nWhat would you put for ys?\nIâ€™m guessing why I should be broadcasted along with all the batches?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent thinking! Yes, exactly!\nIf src has shape (batch_size, seq_len), then ys should start with shape (batch_size, 1) so it matches the batch dimension.\nBut for the copy task, we typically decode one example at a time, so: - src would be (1, seq_len) - single example - ys starts as (1, 1) - single batch, one token\nYou can create it like:\nys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\nOr more simply:\nys = torch.LongTensor([[start_symbol]])\nNow, can you write the loop that generates tokens? What do you need to do inside the loop?\n\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    memory = model.encode(src, src_mask)\n    ys = torch.LongTensor([[start_symbol]])\n    tgt_mask = subsequent_mask(ys.size(1))\n    for _ in range(src.size(1)):\n        x = model.decode(memory, src_mask, ys, tgt_mask)\n\nAm I going in the right direction?\n\n\nğŸ¤–ReplyğŸ¤–\nYes, youâ€™re going in the right direction! A few things to adjust:\n\nLoop range: You should loop max_len - 1 times (not src.size(1)), since you already have the start token\nUpdate tgt_mask: Inside the loop, you need to recreate tgt_mask each time because ys grows:\ntgt_mask = subsequent_mask(ys.size(1)).type_as(src.data)\nAfter decode, what next? You need to:\n\nPass x through the generator to get probabilities\nFind the token with max probability\nAppend it to ys\n\n\nCan you add the steps after model.decode? What would you do with x?\n\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    memory = model.encode(src, src_mask)\n    ys = torch.LongTensor([[start_symbol]])\n    for _ in range(max_len - 1):\n        tgt_mask = subsequent_mask(ys.size(1))\n        x = model.decode(memory, src_mask, ys, tgt_mask)\n        pred = model.generator(x)\n        max_vals, max_indices = torch.max(pred, dim=-1)\n        ys.append(max_indices)\n\nIs this correct?\n\n\nğŸ¤–ReplyğŸ¤–\nYouâ€™re very close! A few fixes:\n\nGet only the last prediction: pred has shape (1, seq_len, vocab), but you only want the last positionâ€™s prediction:\nprob = model.generator(x[:, -1])\ntorch.max returns both values and indices: You only need the index:\n_, next_word = torch.max(prob, dim=1)\nCanâ€™t append to tensor: Use torch.cat instead:\nnext_word = next_word.data[0]\nys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\nReturn ys at the end!\n\nCan you fix these issues?\n\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    memory = model.encode(src, src_mask)\n    ys = torch.LongTensor([[start_symbol]])\n    for _ in range(max_len - 1):\n        tgt_mask = subsequent_mask(ys.size(1))\n        x = model.decode(memory, src_mask, ys, tgt_mask)\n        pred = model.generator(x[:, -1])\n        _, max_indices = torch.max(pred, dim=-1)\n        ys = torch.cat([ys, torch.one(1,1).type_as(src.data).fill_(max_indices)], dim=-1)\n\nAll good now?\n\n\nğŸ¤–ReplyğŸ¤–\nAlmost! Just two tiny fixes:\n\nTypo: torch.one should be torch.ones\nConcatenation dimension: Should be dim=1 (along sequence length), not dim=-1\nMissing return: Add return ys at the end\n\nAlso, max_indices is already a tensor, so you can use it directly:\nnext_word = max_indices.data[0]\nCan you make these small fixes?\nWhy is it Dem = 1 and not Dem = -1? Was torch.max dem equal to -1 okay, or it should also be dem equal to 1?\n\n\nğŸ¤–ReplyğŸ¤–\nGreat question! Let me clarify the dimensions:\nFor torch.cat: - ys has shape (1, seq_len) where seq_len grows each iteration - Youâ€™re adding a new token along the sequence dimension (the columns) - Sequence dimension is dimension 1, so dim=1 âœ“ - dim=-1 would also work (since -1 means last dimension, which is also 1 here)\nFor torch.max: - pred has shape (1, vocab_size) after taking x[:, -1] - You want the max across the vocab dimension (which token has highest prob) - Vocab is the last dimension, so dim=-1 âœ“ - dim=1 would also work here (since there are only 2 dimensions)\nSo both would work in this case! But: - dim=1 for torch.cat is clearer (explicitly says â€œsequence dimensionâ€) - dim=-1 for torch.max is more common (means â€œacross vocabularyâ€)\nDoes this clarify the dimension choices?\nSo I was correct in the first place.\n\n\nğŸ¤–ReplyğŸ¤–\nActually, not quite! Let me clarify:\nYour original code had dim=-1 for concatenation:\ntorch.cat([ys, ...], dim=-1)\nThis would work, but itâ€™s less clear. Using dim=1 is more explicit about concatenating along the sequence dimension.\nFor torch.max, you had dim=-1 which is perfect! âœ“\nSo you were half-right - the torch.max part was correct, but the torch.cat is clearer with dim=1.\nHave you fixed the typo (torch.one â†’ torch.ones) and added the return ys at the end?\nHow is torch.max clear, but torch.cat not clear?\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nYouâ€™re absolutely right to call that out! I was being inconsistent.\nBoth ys and pred are 2D tensors: - ys shape: (1, seq_len) - pred shape: (1, vocab_size)\nIn both cases, dim=1 and dim=-1 refer to the same dimension (the last one).\nSo your original dim=-1 for torch.cat would work just fine!\nI apologize for the confusion. Your code was correct there. The only real fixes needed are:\n\ntorch.one â†’ torch.ones (typo)\nAdd return ys at the end\n\nWant to make those fixes and test it?\n\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    memory = model.encode(src, src_mask)\n    ys = torch.LongTensor([[start_symbol]])\n    for _ in range(max_len - 1):\n        tgt_mask = subsequent_mask(ys.size(1))\n        x = model.decode(memory, src_mask, ys, tgt_mask)\n        pred = model.generator(x[:, -1])\n        _, max_indices = torch.max(pred, dim=-1)\n        ys = torch.cat([ys, torch.ones(1,1).type_as(src.data).fill_(max_indices)], dim=-1)\n    return ys\n\nAll done I guess\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your greedy_decode is complete! âœ“\nNow letâ€™s see your Transformer actually generate predictions! ğŸ¯\n# Test on a simple sequence\nmodel.eval()\nsrc = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nsrc_mask = torch.ones(1, 1, 10)\n\nprint(\"Source:\", src)\nresult = greedy_decode(model, src, src_mask, max_len=10, start_symbol=1)\nprint(\"Generated:\", result)\nRun this and see if your model can copy the sequence! Does it generate [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]?\n\n# Test on a simple sequence\nmodel.eval()\nsrc = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nsrc_mask = torch.ones(1, 1, 10)\n\nprint(\"Source:\", src)\nresult = greedy_decode(model, src, src_mask, max_len=10, start_symbol=1)\nprint(\"Generated:\", result)\n\nSource: tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[1], line 7\n      4 src_mask = torch.ones(1, 1, 10)\n      6 print(\"Source:\", src)\n----&gt; 7 result = greedy_decode(model, src, src_mask, max_len=10, start_symbol=1)\n      8 print(\"Generated:\", result)\n\nCell In[1], line 9, in greedy_decode(model, src, src_mask, max_len, start_symbol)\n      7     pred = model.generator(x[:, -1])\n      8     _, max_indices = torch.max(pred, dim=-1)\n----&gt; 9     ys = torch.cat([ys, torch.ones(1,1).type_as(src.data).fill_(max_indices)], dim=-1)\n     10 return ys\n\nRuntimeError: fill_ only supports 0-dimension value tensor but got tensor with 1 dimensions.\n\n\n\nOh, a bug found!\n\n\nğŸ¤–ReplyğŸ¤–\nGood catch! The issue is that max_indices is a 1D tensor, but fill_ expects a scalar (0-dimension).\nYou need to extract the actual value:\nnext_word = max_indices.data[0]\nys = torch.cat([ys, torch.ones(1,1).type_as(src.data).fill_(next_word)], dim=-1)\nCan you add that line to extract next_word before the concatenation?\nOh wait a minute, arenâ€™t we filling for the whole batch?\n\n\nğŸ¤–ReplyğŸ¤–\nExcellent observation! Youâ€™re thinking about batching correctly!\nIn this case, weâ€™re actually decoding one sequence at a time (batch_size = 1), so: - max_indices has shape (1,) after torch.max - We need to extract the scalar value: max_indices.data[0] or max_indices.item()\nIf you wanted to decode multiple sequences in parallel (batched decoding), the code would be more complex - youâ€™d need to handle all sequences growing together.\nFor now, since weâ€™re doing greedy decode on a single sequence, extracting the scalar is correct.\nWant to add that fix and try again?\n\n\n\n\ngreedy_decode\n\n greedy_decode (model, src, src_mask, max_len, start_symbol)\n\nI hope this is loyal to the original code.\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Your greedy_decode is now complete and correct! âœ“\nNow letâ€™s test it again:\nmodel.eval()\nsrc = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nsrc_mask = torch.ones(1, 1, 10)\n\nprint(\"Source:\", src)\nresult = greedy_decode(model, src, src_mask, max_len=10, start_symbol=1)\nprint(\"Generated:\", result)\nRun this and see if your Transformer successfully copies the sequence! ğŸ¯\n\nmodel.eval()\nsrc = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nsrc_mask = torch.ones(1, 1, 10)\n\nprint(\"Source:\", src)\nresult = greedy_decode(model, src, src_mask, max_len=10, start_symbol=1)\nprint(\"Generated:\", result)\n\nSource: tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\nGenerated: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n\n\nWow, this is terrible.\n\n\nğŸ¤–ReplyğŸ¤–\nHmm, itâ€™s generating all 1s! Thatâ€™s not right. Let me think about what might be happeningâ€¦\nPossible issues:\n\nThe model might not have trained on sequences with values 1-10 - Remember, during training with data_gen, we used vocabulary V=11, which generates random integers from 1 to 10. But did the model see this exact sequence?\nThe model might need more training - We only trained for 2 epochs. Maybe it memorized the training distribution but hasnâ€™t generalized well?\nLetâ€™s test with a sequence from the training distribution:\n\n# Generate a test batch from the same distribution\ntest_batch = next(data_gen(V=11, batch_size=1, nbatches=1))\nprint(\"Source:\", test_batch.src)\nresult = greedy_decode(model, test_batch.src, test_batch.src_mask, max_len=10, start_symbol=1)\nprint(\"Generated:\", result)\nTry this - does it work better on data from the training distribution?\n\n# Generate a test batch from the same distribution\ntest_batch = next(data_gen(V=11, batch_size=1, nbatches=1))\nprint(\"Source:\", test_batch.src)\nresult = greedy_decode(model, test_batch.src, test_batch.src_mask, max_len=10, start_symbol=1)\nprint(\"Generated:\", result)\n\nSource: tensor([[1, 6, 3, 1, 2, 6, 1, 5, 5, 4]])\nGenerated: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n\n\nThink through if there is any training error or inference error. debug very carefully.\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nLet me trace through carefullyâ€¦\nTraining code:\npred = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\nnum_loss, tensor_loss = loss_compute(pred, batch.tgt, batch.ntokens)\nWait! I see the bug! ğŸ›\nIn run_epoch, youâ€™re computing loss against batch.tgt (the decoder input), but you should be computing loss against batch.tgt_y (the labels)!\nRemember: - batch.tgt = decoder input (what it sees) - batch.tgt_y = labels (what it should predict)\nThe model was trained to predict its own input instead of the next token!\nFix in run_epoch: Change:\nnum_loss, tensor_loss = loss_compute(pred, batch.tgt, batch.ntokens)\nTo:\nnum_loss, tensor_loss = loss_compute(pred, batch.tgt_y, batch.ntokens)\nCan you fix this and retrain?\n\nmodel.eval()\ntest_batch = next(data_gen(V=11, batch_size=1, nbatches=1))\nprint(\"Source:\", test_batch.src)\nresult = greedy_decode(model, test_batch.src, test_batch.src_mask, max_len=10, start_symbol=1)\nprint(\"Generated:\", result)\nprint(\"Match?\", torch.equal(test_batch.src, result))\n\nSource: tensor([[1, 6, 5, 6, 7, 9, 7, 6, 2, 9]])\nGenerated: tensor([[1, 6, 5, 6, 7, 9, 7, 6, 2, 9]])\nMatch? True\n\n\ngreat\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ‰ğŸ‰ğŸ‰ PERFECT MATCH! ğŸ‰ğŸ‰ğŸ‰\nYour Transformer is now perfectly copying the input sequence!\nWhat youâ€™ve accomplished today:\nâœ… Implemented the complete Transformer architecture from scratch âœ… Built all training infrastructure (Batch, data generator, loss computation, training loop) âœ… Trained a working model âœ… Implemented greedy decoding for inference âœ… Successfully generated predictions!\nThe magical moment: You built a neural network that learned to copy sequences perfectly!\nThis is a HUGE achievement - you now understand Transformers at the deepest level because you built every single component yourself! ğŸš€\nWant to: 1. Save your model? 2. Try more complex examples? 3. Celebrate and wrap up?\nWhat would you like to do?\nsave my model and wrap up\n\n\nğŸ¤–ReplyğŸ¤–\nPerfect! Letâ€™s save your model:\n# Save the trained model\ntorch.save(model.state_dict(), 'transformer_copy_task.pt')\nprint(\"Model saved as 'transformer_copy_task.pt'\")\n\n# To load it later, you would do:\n# model = make_model(src_vocab=11, tgt_vocab=11, N=2)\n# model.load_state_dict(torch.load('transformer_copy_task.pt'))",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "attention_is_all_you_need.html#congratulations",
    "href": "attention_is_all_you_need.html#congratulations",
    "title": "Simple 2x2 example",
    "section": "ğŸ‰ Congratulations! ğŸ‰",
    "text": "ğŸ‰ Congratulations! ğŸ‰\nWhat you built today:\nâœ… Complete Transformer Architecture: - Scaled Dot-Product Attention - Multi-Head Attention\n- Position-wise Feed-Forward Networks - Positional Encoding - Encoder & Decoder Stacks - Full EncoderDecoder Model\nâœ… Training Infrastructure: - Batch processing with masking - Data generation - Loss computation - Training loop - Greedy decoding\nâœ… Working Model: - Trained successfully - Generates perfect predictions - Loss converged to near-zero\nYou didnâ€™t just learn about Transformers - you built one from scratch and watched it learn! This is an incredible achievement! ğŸš€\nEnjoy your success! ğŸ’ª\n\n# Save the trained model\ntorch.save(model.state_dict(), '../models/transformer_copy_task.pt')\nprint(\"Model saved as '../models/transformer_copy_task.pt'\")\n\n# To load it later, you would do:\n# model = make_model(src_vocab=11, tgt_vocab=11, N=2)\n# model.load_state_dict(torch.load('../models/transformer_copy_task.pt'))\n\nModel saved as '../models/transformer_copy_task.pt'\n\n\nhow to see the quarto docs",
    "crumbs": [
      "Simple 2x2 example"
    ]
  },
  {
    "objectID": "annotated_transformer.html",
    "href": "annotated_transformer.html",
    "title": "Prelims",
    "section": "",
    "text": "The Transformer has been on a lot of peopleâ€™s minds over the last year five years. This post presents an annotated version of the paper in the form of a line-by-line implementation. It reorders and deletes some sections from the original paper and adds comments throughout. This document itself is a working notebook, and should be a completely usable implementation. Code is available here.\n# !pip install -r requirements.txt\n# # Uncomment for colab\n# #\n# !pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n# !python -m spacy download de_core_news_sm\n# !python -m spacy download en_core_web_sm\nimport os\nfrom os.path import exists\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import log_softmax, pad\nimport math\nimport copy\nimport time\nfrom torch.optim.lr_scheduler import LambdaLR\nimport pandas as pd\nimport altair as alt\nfrom torchtext.data.functional import to_map_style_dataset\nfrom torch.utils.data import DataLoader\nfrom torchtext.vocab import build_vocab_from_iterator\nimport torchtext.datasets as datasets\nimport spacy\nimport GPUtil\nimport warnings\nfrom torch.utils.data.distributed import DistributedSampler\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n\n# Set to False to skip notebook execution (e.g. for debugging)\nwarnings.filterwarnings(\"ignore\")\nRUN_EXAMPLES = True\n# Some convenience helper functions used throughout the notebook\n\n\ndef is_interactive_notebook():\n    return __name__ == \"__main__\"\n\n\ndef show_example(fn, args=[]):\n    if __name__ == \"__main__\" and RUN_EXAMPLES:\n        return fn(*args)\n\n\ndef execute_example(fn, args=[]):\n    if __name__ == \"__main__\" and RUN_EXAMPLES:\n        fn(*args)\n\n\nclass DummyOptimizer(torch.optim.Optimizer):\n    def __init__(self):\n        self.param_groups = [{\"lr\": 0}]\n        None\n\n    def step(self):\n        None\n\n    def zero_grad(self, set_to_none=False):\n        None\n\n\nclass DummyScheduler:\n    def step(self):\n        None",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#encoder-and-decoder-stacks",
    "href": "annotated_transformer.html#encoder-and-decoder-stacks",
    "title": "Prelims",
    "section": "Encoder and Decoder Stacks",
    "text": "Encoder and Decoder Stacks\n\nEncoder\nThe encoder is composed of a stack of \\(N=6\\) identical layers.\n\ndef clones(module, N):\n    \"Produce N identical layers.\"\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\n\nclass Encoder(nn.Module):\n    \"Core encoder is a stack of N layers\"\n\n    def __init__(self, layer, N):\n        super(Encoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n\n    def forward(self, x, mask):\n        \"Pass the input (and mask) through each layer in turn.\"\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n\nWe employ a residual connection (cite) around each of the two sub-layers, followed by layer normalization (cite).\n\nclass LayerNorm(nn.Module):\n    \"Construct a layernorm module (See citation for details).\"\n\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n\nThat is, the output of each sub-layer is \\(\\mathrm{LayerNorm}(x +\n\\mathrm{Sublayer}(x))\\), where \\(\\mathrm{Sublayer}(x)\\) is the function implemented by the sub-layer itself. We apply dropout (cite) to the output of each sub-layer, before it is added to the sub-layer input and normalized.\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension \\(d_{\\text{model}}=512\\).\n\nclass SublayerConnection(nn.Module):\n    \"\"\"\n    A residual connection followed by a layer norm.\n    Note for code simplicity the norm is first as opposed to last.\n    \"\"\"\n\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        \"Apply residual connection to any sublayer with the same size.\"\n        return x + self.dropout(sublayer(self.norm(x)))\n\nEach layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.\n\nclass EncoderLayer(nn.Module):\n    \"Encoder is made up of self-attn and feed forward (defined below)\"\n\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n        self.size = size\n\n    def forward(self, x, mask):\n        \"Follow Figure 1 (left) for connections.\"\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)\n\n\n\nDecoder\nThe decoder is also composed of a stack of \\(N=6\\) identical layers.\n\nclass Decoder(nn.Module):\n    \"Generic N layer decoder with masking.\"\n\n    def __init__(self, layer, N):\n        super(Decoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)\n\nIn addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\n\nclass DecoderLayer(nn.Module):\n    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        \"Follow Figure 1 (right) for connections.\"\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        return self.sublayer[2](x, self.feed_forward)\n\nWe also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position \\(i\\) can depend only on the known outputs at positions less than \\(i\\).\n\ndef subsequent_mask(size):\n    \"Mask out subsequent positions.\"\n    attn_shape = (1, size, size)\n    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n        torch.uint8\n    )\n    return subsequent_mask == 0\n\n\nBelow the attention mask shows the position each tgt word (row) is allowed to look at (column). Words are blocked for attending to future words during training.\n\n\ndef example_mask():\n    LS_data = pd.concat(\n        [\n            pd.DataFrame(\n                {\n                    \"Subsequent Mask\": subsequent_mask(20)[0][x, y].flatten(),\n                    \"Window\": y,\n                    \"Masking\": x,\n                }\n            )\n            for y in range(20)\n            for x in range(20)\n        ]\n    )\n\n    return (\n        alt.Chart(LS_data)\n        .mark_rect()\n        .properties(height=250, width=250)\n        .encode(\n            alt.X(\"Window:O\"),\n            alt.Y(\"Masking:O\"),\n            alt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\")),\n        )\n        .interactive()\n    )\n\n\nshow_example(example_mask)\n\n\n\n\n\n\n\n\nAttention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\nWe call our particular attention â€œScaled Dot-Product Attentionâ€. The input consists of queries and keys of dimension \\(d_k\\), and values of dimension \\(d_v\\). We compute the dot products of the query with all keys, divide each by \\(\\sqrt{d_k}\\), and apply a softmax function to obtain the weights on the values.\n\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix \\(Q\\). The keys and values are also packed together into matrices \\(K\\) and \\(V\\). We compute the matrix of outputs as:\n\\[\n   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n\\]\n\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = scores.softmax(dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nThe two most commonly used attention functions are additive attention (cite), and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of \\(\\frac{1}{\\sqrt{d_k}}\\). Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\nWhile for small values of \\(d_k\\) the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of \\(d_k\\) (cite). We suspect that for large values of \\(d_k\\), the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients (To illustrate why the dot products get large, assume that the components of \\(q\\) and \\(k\\) are independent random variables with mean \\(0\\) and variance \\(1\\). Then their dot product, \\(q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i\\), has mean \\(0\\) and variance \\(d_k\\).). To counteract this effect, we scale the dot products by \\(\\frac{1}{\\sqrt{d_k}}\\).\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n\\[\n\\mathrm{MultiHead}(Q, K, V) =\n    \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O \\\\\n    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n\\]\nWhere the projections are parameter matrices \\(W^Q_i \\in\n\\mathbb{R}^{d_{\\text{model}} \\times d_k}\\), \\(W^K_i \\in\n\\mathbb{R}^{d_{\\text{model}} \\times d_k}\\), \\(W^V_i \\in\n\\mathbb{R}^{d_{\\text{model}} \\times d_v}\\) and \\(W^O \\in\n\\mathbb{R}^{hd_v \\times d_{\\text{model}}}\\).\nIn this work we employ \\(h=8\\) parallel attention layers, or heads. For each of these we use \\(d_k=d_v=d_{\\text{model}}/h=64\\). Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        \"Take in model size and number of heads.\"\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        \"Implements Figure 2\"\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n\n        # 1) Do all the linear projections in batch from d_model =&gt; h x d_k\n        query, key, value = [\n            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n            for lin, x in zip(self.linears, (query, key, value))\n        ]\n\n        # 2) Apply attention on all the projected vectors in batch.\n        x, self.attn = attention(\n            query, key, value, mask=mask, dropout=self.dropout\n        )\n\n        # 3) \"Concat\" using a view and apply a final linear.\n        x = (\n            x.transpose(1, 2)\n            .contiguous()\n            .view(nbatches, -1, self.h * self.d_k)\n        )\n        del query\n        del key\n        del value\n        return self.linears[-1](x)\n\n\n\nApplications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways: 1) In â€œencoder-decoder attentionâ€ layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as (cite).\n\nThe encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to \\(-\\infty\\)) all values in the input of the softmax which correspond to illegal connections.",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#position-wise-feed-forward-networks",
    "href": "annotated_transformer.html#position-wise-feed-forward-networks",
    "title": "Prelims",
    "section": "Position-wise Feed-Forward Networks",
    "text": "Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n\\[\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2\\]\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is \\(d_{\\text{model}}=512\\), and the inner-layer has dimensionality \\(d_{ff}=2048\\).\n\nclass PositionwiseFeedForward(nn.Module):\n    \"Implements FFN equation.\"\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(self.w_1(x).relu()))",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#embeddings-and-softmax",
    "href": "annotated_transformer.html#embeddings-and-softmax",
    "title": "Prelims",
    "section": "Embeddings and Softmax",
    "text": "Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension \\(d_{\\text{model}}\\). We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to (cite). In the embedding layers, we multiply those weights by \\(\\sqrt{d_{\\text{model}}}\\).\n\nclass Embeddings(nn.Module):\n    def __init__(self, d_model, vocab):\n        super(Embeddings, self).__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.lut(x) * math.sqrt(self.d_model)",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#positional-encoding",
    "href": "annotated_transformer.html#positional-encoding",
    "title": "Prelims",
    "section": "Positional Encoding",
    "text": "Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add â€œpositional encodingsâ€ to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension \\(d_{\\text{model}}\\) as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed (cite).\nIn this work, we use sine and cosine functions of different frequencies:\n\\[PE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})\\]\n\\[PE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})\\]\nwhere \\(pos\\) is the position and \\(i\\) is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from \\(2\\pi\\) to \\(10000 \\cdot\n2\\pi\\). We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset \\(k\\), \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\).\nIn addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of \\(P_{drop}=0.1\\).\n\nclass PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n        return self.dropout(x)\n\n\nBelow the positional encoding will add in a sine wave based on position. The frequency and offset of the wave is different for each dimension.\n\n\ndef example_positional():\n    pe = PositionalEncoding(20, 0)\n    y = pe.forward(torch.zeros(1, 100, 20))\n\n    data = pd.concat(\n        [\n            pd.DataFrame(\n                {\n                    \"embedding\": y[0, :, dim],\n                    \"dimension\": dim,\n                    \"position\": list(range(100)),\n                }\n            )\n            for dim in [4, 5, 6, 7]\n        ]\n    )\n\n    return (\n        alt.Chart(data)\n        .mark_line()\n        .properties(width=800)\n        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n        .interactive()\n    )\n\n\nshow_example(example_positional)\n\n\n\n\n\n\nWe also experimented with using learned positional embeddings (cite) instead, and found that the two versions produced nearly identical results. We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#full-model",
    "href": "annotated_transformer.html#full-model",
    "title": "Prelims",
    "section": "Full Model",
    "text": "Full Model\n\nHere we define a function from hyperparameters to a full model.\n\n\ndef make_model(\n    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n):\n    \"Helper: Construct a model from hyperparameters.\"\n    c = copy.deepcopy\n    attn = MultiHeadedAttention(h, d_model)\n    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n    position = PositionalEncoding(d_model, dropout)\n    model = EncoderDecoder(\n        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n        Generator(d_model, tgt_vocab),\n    )\n\n    # This was important from their code.\n    # Initialize parameters with Glorot / fan_avg.\n    for p in model.parameters():\n        if p.dim() &gt; 1:\n            nn.init.xavier_uniform_(p)\n    return model",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#inference",
    "href": "annotated_transformer.html#inference",
    "title": "Prelims",
    "section": "Inference:",
    "text": "Inference:\n\nHere we make a forward step to generate a prediction of the model. We try to use our transformer to memorize the input. As you will see the output is randomly generated due to the fact that the model is not trained yet. In the next tutorial we will build the training function and try to train our model to memorize the numbers from 1 to 10.\n\n\ndef inference_test():\n    test_model = make_model(11, 11, 2)\n    test_model.eval()\n    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n    src_mask = torch.ones(1, 1, 10)\n\n    memory = test_model.encode(src, src_mask)\n    ys = torch.zeros(1, 1).type_as(src)\n\n    for i in range(9):\n        out = test_model.decode(\n            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n        )\n        prob = test_model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.data[0]\n        ys = torch.cat(\n            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n        )\n\n    print(\"Example Untrained Model Prediction:\", ys)\n\n\ndef run_tests():\n    for _ in range(10):\n        inference_test()\n\n\nshow_example(run_tests)\n\nExample Untrained Model Prediction: tensor([[ 0, 10,  0, 10,  0,  0,  0,  0,  0, 10]])\nExample Untrained Model Prediction: tensor([[ 0,  8,  1, 10,  0,  8,  1, 10,  0,  8]])\nExample Untrained Model Prediction: tensor([[ 0,  9,  0, 10,  4,  5,  3,  2,  4,  3]])\nExample Untrained Model Prediction: tensor([[0, 5, 5, 5, 5, 5, 5, 5, 5, 5]])\nExample Untrained Model Prediction: tensor([[0, 2, 8, 3, 8, 5, 0, 4, 0, 4]])\nExample Untrained Model Prediction: tensor([[ 0, 10,  3, 10,  2,  9,  0,  3, 10,  3]])\nExample Untrained Model Prediction: tensor([[0, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\nExample Untrained Model Prediction: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\nExample Untrained Model Prediction: tensor([[0, 3, 2, 2, 2, 4, 0, 3, 1, 3]])\nExample Untrained Model Prediction: tensor([[0, 6, 6, 6, 6, 6, 6, 6, 6, 6]])",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#batches-and-masking",
    "href": "annotated_transformer.html#batches-and-masking",
    "title": "Prelims",
    "section": "Batches and Masking",
    "text": "Batches and Masking\n\nclass Batch:\n    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n\n    def __init__(self, src, tgt=None, pad=2):  # 2 = &lt;blank&gt;\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(-2)\n        if tgt is not None:\n            self.tgt = tgt[:, :-1]\n            self.tgt_y = tgt[:, 1:]\n            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n            self.ntokens = (self.tgt_y != pad).data.sum()\n\n    @staticmethod\n    def make_std_mask(tgt, pad):\n        \"Create a mask to hide padding and future words.\"\n        tgt_mask = (tgt != pad).unsqueeze(-2)\n        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n            tgt_mask.data\n        )\n        return tgt_mask\n\n\nNext we create a generic training and scoring function to keep track of loss. We pass in a generic loss compute function that also handles parameter updates.",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#training-loop",
    "href": "annotated_transformer.html#training-loop",
    "title": "Prelims",
    "section": "Training Loop",
    "text": "Training Loop\n\nclass TrainState:\n    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n\n    step: int = 0  # Steps in the current epoch\n    accum_step: int = 0  # Number of gradient accumulation steps\n    samples: int = 0  # total # of examples used\n    tokens: int = 0  # total # of tokens processed\n\n\ndef run_epoch(\n    data_iter,\n    model,\n    loss_compute,\n    optimizer,\n    scheduler,\n    mode=\"train\",\n    accum_iter=1,\n    train_state=TrainState(),\n):\n    \"\"\"Train a single epoch\"\"\"\n    start = time.time()\n    total_tokens = 0\n    total_loss = 0\n    tokens = 0\n    n_accum = 0\n    for i, batch in enumerate(data_iter):\n        out = model.forward(\n            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n        )\n        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n        # loss_node = loss_node / accum_iter\n        if mode == \"train\" or mode == \"train+log\":\n            loss_node.backward()\n            train_state.step += 1\n            train_state.samples += batch.src.shape[0]\n            train_state.tokens += batch.ntokens\n            if i % accum_iter == 0:\n                optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n                n_accum += 1\n                train_state.accum_step += 1\n            scheduler.step()\n\n        total_loss += loss\n        total_tokens += batch.ntokens\n        tokens += batch.ntokens\n        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n            lr = optimizer.param_groups[0][\"lr\"]\n            elapsed = time.time() - start\n            print(\n                (\n                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n                )\n\n            )\n            start = time.time()\n            tokens = 0\n        del loss\n        del loss_node\n    return total_loss / total_tokens, train_state",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#training-data-and-batching",
    "href": "annotated_transformer.html#training-data-and-batching",
    "title": "Prelims",
    "section": "Training Data and Batching",
    "text": "Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary.\nSentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#hardware-and-schedule",
    "href": "annotated_transformer.html#hardware-and-schedule",
    "title": "Prelims",
    "section": "Hardware and Schedule",
    "text": "Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#optimizer",
    "href": "annotated_transformer.html#optimizer",
    "title": "Prelims",
    "section": "Optimizer",
    "text": "Optimizer\nWe used the Adam optimizer (cite) with \\(\\beta_1=0.9\\), \\(\\beta_2=0.98\\) and \\(\\epsilon=10^{-9}\\). We varied the learning rate over the course of training, according to the formula:\n\\[\nlrate = d_{\\text{model}}^{-0.5} \\cdot\n  \\min({step\\_num}^{-0.5},\n    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})\n\\]\nThis corresponds to increasing the learning rate linearly for the first \\(warmup\\_steps\\) training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used \\(warmup\\_steps=4000\\).\n\nNote: This part is very important. Need to train with this setup of the model.\n\n\nExample of the curves of this model for different model sizes and for optimization hyperparameters.\n\n\ndef rate(step, model_size, factor, warmup):\n    \"\"\"\n    we have to default the step to 1 for LambdaLR function\n    to avoid zero raising to negative power.\n    \"\"\"\n    if step == 0:\n        step = 1\n    return factor * (\n        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n    )\n\n\ndef example_learning_schedule():\n    opts = [\n        [512, 1, 4000],  # example 1\n        [512, 1, 8000],  # example 2\n        [256, 1, 4000],  # example 3\n    ]\n\n    dummy_model = torch.nn.Linear(1, 1)\n    learning_rates = []\n\n    # we have 3 examples in opts list.\n    for idx, example in enumerate(opts):\n        # run 20000 epoch for each example\n        optimizer = torch.optim.Adam(\n            dummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\n        )\n        lr_scheduler = LambdaLR(\n            optimizer=optimizer, lr_lambda=lambda step: rate(step, *example)\n        )\n        tmp = []\n        # take 20K dummy training steps, save the learning rate at each step\n        for step in range(20000):\n            tmp.append(optimizer.param_groups[0][\"lr\"])\n            optimizer.step()\n            lr_scheduler.step()\n        learning_rates.append(tmp)\n\n    learning_rates = torch.tensor(learning_rates)\n\n    # Enable altair to handle more than 5000 rows\n    alt.data_transformers.disable_max_rows()\n\n    opts_data = pd.concat(\n        [\n            pd.DataFrame(\n                {\n                    \"Learning Rate\": learning_rates[warmup_idx, :],\n                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\n                        warmup_idx\n                    ],\n                    \"step\": range(20000),\n                }\n            )\n            for warmup_idx in [0, 1, 2]\n        ]\n    )\n\n    return (\n        alt.Chart(opts_data)\n        .mark_line()\n        .properties(width=600)\n        .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n        .interactive()\n    )\n\n\nexample_learning_schedule()",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#regularization",
    "href": "annotated_transformer.html#regularization",
    "title": "Prelims",
    "section": "Regularization",
    "text": "Regularization\n\nLabel Smoothing\nDuring training, we employed label smoothing of value \\(\\epsilon_{ls}=0.1\\) (cite). This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n\nWe implement label smoothing using the KL div loss. Instead of using a one-hot target distribution, we create a distribution that has confidence of the correct word and the rest of the smoothing mass distributed throughout the vocabulary.\n\n\nclass LabelSmoothing(nn.Module):\n    \"Implement label smoothing.\"\n\n    def __init__(self, size, padding_idx, smoothing=0.0):\n        super(LabelSmoothing, self).__init__()\n        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n\n    def forward(self, x, target):\n        assert x.size(1) == self.size\n        true_dist = x.data.clone()\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() &gt; 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist\n        return self.criterion(x, true_dist.clone().detach())\n\n\nHere we can see an example of how the mass is distributed to the words based on confidence.\n\n\n# Example of label smoothing.\n\n\ndef example_label_smoothing():\n    crit = LabelSmoothing(5, 0, 0.4)\n    predict = torch.FloatTensor(\n        [\n            [0, 0.2, 0.7, 0.1, 0],\n            [0, 0.2, 0.7, 0.1, 0],\n            [0, 0.2, 0.7, 0.1, 0],\n            [0, 0.2, 0.7, 0.1, 0],\n            [0, 0.2, 0.7, 0.1, 0],\n        ]\n    )\n    crit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3]))\n    LS_data = pd.concat(\n        [\n            pd.DataFrame(\n                {\n                    \"target distribution\": crit.true_dist[x, y].flatten(),\n                    \"columns\": y,\n                    \"rows\": x,\n                }\n            )\n            for y in range(5)\n            for x in range(5)\n        ]\n    )\n\n    return (\n        alt.Chart(LS_data)\n        .mark_rect(color=\"Blue\", opacity=1)\n        .properties(height=200, width=200)\n        .encode(\n            alt.X(\"columns:O\", title=None),\n            alt.Y(\"rows:O\", title=None),\n            alt.Color(\n                \"target distribution:Q\", scale=alt.Scale(scheme=\"viridis\")\n            ),\n        )\n        .interactive()\n    )\n\n\nshow_example(example_label_smoothing)\n\n\n\n\n\n\n\nLabel smoothing actually starts to penalize the model if it gets very confident about a given choice.\n\n\ndef loss(x, crit):\n    d = x + 3 * 1\n    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n    return crit(predict.log(), torch.LongTensor([1])).data\n\n\ndef penalization_visualization():\n    crit = LabelSmoothing(5, 0, 0.1)\n    loss_data = pd.DataFrame(\n        {\n            \"Loss\": [loss(x, crit) for x in range(1, 100)],\n            \"Steps\": list(range(99)),\n        }\n    ).astype(\"float\")\n\n    return (\n        alt.Chart(loss_data)\n        .mark_line()\n        .properties(width=350)\n        .encode(\n            x=\"Steps\",\n            y=\"Loss\",\n        )\n        .interactive()\n    )\n\n\nshow_example(penalization_visualization)",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#synthetic-data",
    "href": "annotated_transformer.html#synthetic-data",
    "title": "Prelims",
    "section": "Synthetic Data",
    "text": "Synthetic Data\n\ndef data_gen(V, batch_size, nbatches):\n    \"Generate random data for a src-tgt copy task.\"\n    for i in range(nbatches):\n        data = torch.randint(1, V, size=(batch_size, 10))\n        data[:, 0] = 1\n        src = data.requires_grad_(False).clone().detach()\n        tgt = data.requires_grad_(False).clone().detach()\n        yield Batch(src, tgt, 0)",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#loss-computation",
    "href": "annotated_transformer.html#loss-computation",
    "title": "Prelims",
    "section": "Loss Computation",
    "text": "Loss Computation\n\nclass SimpleLossCompute:\n    \"A simple loss compute and train function.\"\n\n    def __init__(self, generator, criterion):\n        self.generator = generator\n        self.criterion = criterion\n\n    def __call__(self, x, y, norm):\n        x = self.generator(x)\n        sloss = (\n            self.criterion(\n                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n            )\n            / norm\n        )\n        return sloss.data * norm, sloss",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#greedy-decoding",
    "href": "annotated_transformer.html#greedy-decoding",
    "title": "Prelims",
    "section": "Greedy Decoding",
    "text": "Greedy Decoding\n\nThis code predicts a translation using greedy decoding for simplicity.\n\n\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    memory = model.encode(src, src_mask)\n    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n    for i in range(max_len - 1):\n        out = model.decode(\n            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n        )\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.data[0]\n        ys = torch.cat(\n            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n        )\n    return ys\n\n\n# Train the simple copy task.\n\n\ndef example_simple_model():\n    V = 11\n    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n    model = make_model(V, V, N=2)\n\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n    )\n    lr_scheduler = LambdaLR(\n        optimizer=optimizer,\n        lr_lambda=lambda step: rate(\n            step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400\n        ),\n    )\n\n    batch_size = 80\n    for epoch in range(20):\n        model.train()\n        run_epoch(\n            data_gen(V, batch_size, 20),\n            model,\n            SimpleLossCompute(model.generator, criterion),\n            optimizer,\n            lr_scheduler,\n            mode=\"train\",\n        )\n        model.eval()\n        run_epoch(\n            data_gen(V, batch_size, 5),\n            model,\n            SimpleLossCompute(model.generator, criterion),\n            DummyOptimizer(),\n            DummyScheduler(),\n            mode=\"eval\",\n        )[0]\n\n    model.eval()\n    src = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n    max_len = src.shape[1]\n    src_mask = torch.ones(1, 1, max_len)\n    print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0))\n\n\n# execute_example(example_simple_model)",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#data-loading",
    "href": "annotated_transformer.html#data-loading",
    "title": "Prelims",
    "section": "Data Loading",
    "text": "Data Loading\n\nWe will load the dataset using torchtext and spacy for tokenization.\n\n\n# Load spacy tokenizer models, download them if they haven't been\n# downloaded already\n\n\ndef load_tokenizers():\n\n    try:\n        spacy_de = spacy.load(\"de_core_news_sm\")\n    except IOError:\n        os.system(\"python -m spacy download de_core_news_sm\")\n        spacy_de = spacy.load(\"de_core_news_sm\")\n\n    try:\n        spacy_en = spacy.load(\"en_core_web_sm\")\n    except IOError:\n        os.system(\"python -m spacy download en_core_web_sm\")\n        spacy_en = spacy.load(\"en_core_web_sm\")\n\n    return spacy_de, spacy_en\n\n\ndef tokenize(text, tokenizer):\n    return [tok.text for tok in tokenizer.tokenizer(text)]\n\n\ndef yield_tokens(data_iter, tokenizer, index):\n    for from_to_tuple in data_iter:\n        yield tokenizer(from_to_tuple[index])\n\n\ndef build_vocabulary(spacy_de, spacy_en):\n    def tokenize_de(text):\n        return tokenize(text, spacy_de)\n\n    def tokenize_en(text):\n        return tokenize(text, spacy_en)\n\n    print(\"Building German Vocabulary ...\")\n    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n    vocab_src = build_vocab_from_iterator(\n        yield_tokens(train + val + test, tokenize_de, index=0),\n        min_freq=2,\n        specials=[\"&lt;s&gt;\", \"&lt;/s&gt;\", \"&lt;blank&gt;\", \"&lt;unk&gt;\"],\n    )\n\n    print(\"Building English Vocabulary ...\")\n    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n    vocab_tgt = build_vocab_from_iterator(\n        yield_tokens(train + val + test, tokenize_en, index=1),\n        min_freq=2,\n        specials=[\"&lt;s&gt;\", \"&lt;/s&gt;\", \"&lt;blank&gt;\", \"&lt;unk&gt;\"],\n    )\n\n    vocab_src.set_default_index(vocab_src[\"&lt;unk&gt;\"])\n    vocab_tgt.set_default_index(vocab_tgt[\"&lt;unk&gt;\"])\n\n    return vocab_src, vocab_tgt\n\n\ndef load_vocab(spacy_de, spacy_en):\n    if not exists(\"vocab.pt\"):\n        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n        torch.save((vocab_src, vocab_tgt), \"vocab.pt\")\n    else:\n        vocab_src, vocab_tgt = torch.load(\"vocab.pt\")\n    print(\"Finished.\\nVocabulary sizes:\")\n    print(len(vocab_src))\n    print(len(vocab_tgt))\n    return vocab_src, vocab_tgt\n\n\nif is_interactive_notebook():\n    # global variables used later in the script\n    spacy_de, spacy_en = show_example(load_tokenizers)\n    vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])\n\nFinished.\nVocabulary sizes:\n59981\n36745\n\n\n\nBatching matters a ton for speed. We want to have very evenly divided batches, with absolutely minimal padding. To do this we have to hack a bit around the default torchtext batching. This code patches their default batching to make sure we search over enough sentences to find tight batches.",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#iterators",
    "href": "annotated_transformer.html#iterators",
    "title": "Prelims",
    "section": "Iterators",
    "text": "Iterators\n\ndef collate_batch(\n    batch,\n    src_pipeline,\n    tgt_pipeline,\n    src_vocab,\n    tgt_vocab,\n    device,\n    max_padding=128,\n    pad_id=2,\n):\n    bs_id = torch.tensor([0], device=device)  # &lt;s&gt; token id\n    eos_id = torch.tensor([1], device=device)  # &lt;/s&gt; token id\n    src_list, tgt_list = [], []\n    for (_src, _tgt) in batch:\n        processed_src = torch.cat(\n            [\n                bs_id,\n                torch.tensor(\n                    src_vocab(src_pipeline(_src)),\n                    dtype=torch.int64,\n                    device=device,\n                ),\n                eos_id,\n            ],\n            0,\n        )\n        processed_tgt = torch.cat(\n            [\n                bs_id,\n                torch.tensor(\n                    tgt_vocab(tgt_pipeline(_tgt)),\n                    dtype=torch.int64,\n                    device=device,\n                ),\n                eos_id,\n            ],\n            0,\n        )\n        src_list.append(\n            # warning - overwrites values for negative values of padding - len\n            pad(\n                processed_src,\n                (\n                    0,\n                    max_padding - len(processed_src),\n                ),\n                value=pad_id,\n            )\n        )\n        tgt_list.append(\n            pad(\n                processed_tgt,\n                (0, max_padding - len(processed_tgt)),\n                value=pad_id,\n            )\n        )\n\n    src = torch.stack(src_list)\n    tgt = torch.stack(tgt_list)\n    return (src, tgt)\n\n\ndef create_dataloaders(\n    device,\n    vocab_src,\n    vocab_tgt,\n    spacy_de,\n    spacy_en,\n    batch_size=12000,\n    max_padding=128,\n    is_distributed=True,\n):\n    # def create_dataloaders(batch_size=12000):\n    def tokenize_de(text):\n        return tokenize(text, spacy_de)\n\n    def tokenize_en(text):\n        return tokenize(text, spacy_en)\n\n    def collate_fn(batch):\n        return collate_batch(\n            batch,\n            tokenize_de,\n            tokenize_en,\n            vocab_src,\n            vocab_tgt,\n            device,\n            max_padding=max_padding,\n            pad_id=vocab_src.get_stoi()[\"&lt;blank&gt;\"],\n        )\n\n    train_iter, valid_iter, test_iter = datasets.Multi30k(\n        language_pair=(\"de\", \"en\")\n    )\n\n    train_iter_map = to_map_style_dataset(\n        train_iter\n    )  # DistributedSampler needs a dataset len()\n    train_sampler = (\n        DistributedSampler(train_iter_map) if is_distributed else None\n    )\n    valid_iter_map = to_map_style_dataset(valid_iter)\n    valid_sampler = (\n        DistributedSampler(valid_iter_map) if is_distributed else None\n    )\n\n    train_dataloader = DataLoader(\n        train_iter_map,\n        batch_size=batch_size,\n        shuffle=(train_sampler is None),\n        sampler=train_sampler,\n        collate_fn=collate_fn,\n    )\n    valid_dataloader = DataLoader(\n        valid_iter_map,\n        batch_size=batch_size,\n        shuffle=(valid_sampler is None),\n        sampler=valid_sampler,\n        collate_fn=collate_fn,\n    )\n    return train_dataloader, valid_dataloader",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#training-the-system",
    "href": "annotated_transformer.html#training-the-system",
    "title": "Prelims",
    "section": "Training the System",
    "text": "Training the System\n\ndef train_worker(\n    gpu,\n    ngpus_per_node,\n    vocab_src,\n    vocab_tgt,\n    spacy_de,\n    spacy_en,\n    config,\n    is_distributed=False,\n):\n    print(f\"Train worker process using GPU: {gpu} for training\", flush=True)\n    torch.cuda.set_device(gpu)\n\n    pad_idx = vocab_tgt[\"&lt;blank&gt;\"]\n    d_model = 512\n    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n    model.cuda(gpu)\n    module = model\n    is_main_process = True\n    if is_distributed:\n        dist.init_process_group(\n            \"nccl\", init_method=\"env://\", rank=gpu, world_size=ngpus_per_node\n        )\n        model = DDP(model, device_ids=[gpu])\n        module = model.module\n        is_main_process = gpu == 0\n\n    criterion = LabelSmoothing(\n        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n    )\n    criterion.cuda(gpu)\n\n    train_dataloader, valid_dataloader = create_dataloaders(\n        gpu,\n        vocab_src,\n        vocab_tgt,\n        spacy_de,\n        spacy_en,\n        batch_size=config[\"batch_size\"] // ngpus_per_node,\n        max_padding=config[\"max_padding\"],\n        is_distributed=is_distributed,\n    )\n\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n    )\n    lr_scheduler = LambdaLR(\n        optimizer=optimizer,\n        lr_lambda=lambda step: rate(\n            step, d_model, factor=1, warmup=config[\"warmup\"]\n        ),\n    )\n    train_state = TrainState()\n\n    for epoch in range(config[\"num_epochs\"]):\n        if is_distributed:\n            train_dataloader.sampler.set_epoch(epoch)\n            valid_dataloader.sampler.set_epoch(epoch)\n\n        model.train()\n        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n        _, train_state = run_epoch(\n            (Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n            model,\n            SimpleLossCompute(module.generator, criterion),\n            optimizer,\n            lr_scheduler,\n            mode=\"train+log\",\n            accum_iter=config[\"accum_iter\"],\n            train_state=train_state,\n        )\n\n        GPUtil.showUtilization()\n        if is_main_process:\n            file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n            torch.save(module.state_dict(), file_path)\n        torch.cuda.empty_cache()\n\n        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n        model.eval()\n        sloss = run_epoch(\n            (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\n            model,\n            SimpleLossCompute(module.generator, criterion),\n            DummyOptimizer(),\n            DummyScheduler(),\n            mode=\"eval\",\n        )\n        print(sloss)\n        torch.cuda.empty_cache()\n\n    if is_main_process:\n        file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n        torch.save(module.state_dict(), file_path)\n\n\ndef train_distributed_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n    from the_annotated_transformer import train_worker\n\n    ngpus = torch.cuda.device_count()\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12356\"\n    print(f\"Number of GPUs detected: {ngpus}\")\n    print(\"Spawning training processes ...\")\n    mp.spawn(\n        train_worker,\n        nprocs=ngpus,\n        args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, True),\n    )\n\n\ndef train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n    if config[\"distributed\"]:\n        train_distributed_model(\n            vocab_src, vocab_tgt, spacy_de, spacy_en, config\n        )\n    else:\n        train_worker(\n            0, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config, False\n        )\n\n\ndef load_trained_model():\n    config = {\n        \"batch_size\": 32,\n        \"distributed\": False,\n        \"num_epochs\": 8,\n        \"accum_iter\": 10,\n        \"base_lr\": 1.0,\n        \"max_padding\": 72,\n        \"warmup\": 3000,\n        \"file_prefix\": \"multi30k_model_\",\n    }\n    model_path = \"multi30k_model_final.pt\"\n    if not exists(model_path):\n        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)\n\n    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n    model.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\n    return model\n\n\nif is_interactive_notebook():\n    model = load_trained_model()\n\n\nOnce trained we can decode the model to produce a set of translations. Here we simply translate the first sentence in the validation set. This dataset is pretty small so the translations with greedy search are reasonably accurate.",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#attention-visualization",
    "href": "annotated_transformer.html#attention-visualization",
    "title": "Prelims",
    "section": "Attention Visualization",
    "text": "Attention Visualization\n\nEven with a greedy decoder the translation looks pretty good. We can further visualize it to see what is happening at each layer of the attention\n\n\ndef mtx2df(m, max_row, max_col, row_tokens, col_tokens):\n    \"convert a dense matrix to a data frame with row and column indices\"\n    return pd.DataFrame(\n        [\n            (\n                r,\n                c,\n                float(m[r, c]),\n                \"%.3d %s\"\n\n                \"%.3d %s\"\n\n            )\n            for r in range(m.shape[0])\n            for c in range(m.shape[1])\n            if r &lt; max_row and c &lt; max_col\n        ],\n        # if float(m[r,c]) != 0 and r &lt; max_row and c &lt; max_col],\n        columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n    )\n\n\ndef attn_map(attn, layer, head, row_tokens, col_tokens, max_dim=30):\n    df = mtx2df(\n        attn[0, head].data,\n        max_dim,\n        max_dim,\n        row_tokens,\n        col_tokens,\n    )\n    return (\n        alt.Chart(data=df)\n        .mark_rect()\n        .encode(\n            x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")),\n            y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")),\n            color=\"value\",\n            tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n        )\n        .properties(height=400, width=400)\n        .interactive()\n    )\n\n\ndef get_encoder(model, layer):\n    return model.encoder.layers[layer].self_attn.attn\n\n\ndef get_decoder_self(model, layer):\n    return model.decoder.layers[layer].self_attn.attn\n\n\ndef get_decoder_src(model, layer):\n    return model.decoder.layers[layer].src_attn.attn\n\n\ndef visualize_layer(model, layer, getter_fn, ntokens, row_tokens, col_tokens):\n    # ntokens = last_example[0].ntokens\n    attn = getter_fn(model, layer)\n    n_heads = attn.shape[1]\n    charts = [\n        attn_map(\n            attn,\n            0,\n            h,\n            row_tokens=row_tokens,\n            col_tokens=col_tokens,\n            max_dim=ntokens,\n        )\n        for h in range(n_heads)\n    ]\n    assert n_heads == 8\n    return alt.vconcat(\n        charts[0]\n        # | charts[1]\n        | charts[2]\n        # | charts[3]\n        | charts[4]\n        # | charts[5]\n        | charts[6]\n        # | charts[7]\n        # layer + 1 due to 0-indexing\n    ).properties(title=\"Layer %d\" % (layer + 1))",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#encoder-self-attention",
    "href": "annotated_transformer.html#encoder-self-attention",
    "title": "Prelims",
    "section": "Encoder Self Attention",
    "text": "Encoder Self Attention\n\ndef viz_encoder_self():\n    model, example_data = run_model_example(n_examples=1)\n    example = example_data[\n        len(example_data) - 1\n    ]  # batch object for the final example\n\n    layer_viz = [\n        visualize_layer(\n            model, layer, get_encoder, len(example[1]), example[1], example[1]\n        )\n        for layer in range(6)\n    ]\n    return alt.hconcat(\n        layer_viz[0]\n        # & layer_viz[1]\n        & layer_viz[2]\n        # & layer_viz[3]\n        & layer_viz[4]\n        # & layer_viz[5]\n    )\n\n\nshow_example(viz_encoder_self)\n\nPreparing Data ...\nLoading Trained Model ...\nChecking Model Outputs:\n\nExample 0 ========\n\nSource Text (Input)        : &lt;s&gt; Mehrere Kinder heben die HÃ¤nde , wÃ¤hrend sie auf einem bunten Teppich in einem Klassenzimmer sitzen . &lt;/s&gt;\nTarget Text (Ground Truth) : &lt;s&gt; Several children are raising their hands while sitting on a colorful rug in a classroom . &lt;/s&gt;\nModel Output               : &lt;s&gt; A group of children are in their hands while sitting on a colorful carpet . &lt;/s&gt;",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#decoder-self-attention",
    "href": "annotated_transformer.html#decoder-self-attention",
    "title": "Prelims",
    "section": "Decoder Self Attention",
    "text": "Decoder Self Attention\n\ndef viz_decoder_self():\n    model, example_data = run_model_example(n_examples=1)\n    example = example_data[len(example_data) - 1]\n\n    layer_viz = [\n        visualize_layer(\n            model,\n            layer,\n            get_decoder_self,\n            len(example[1]),\n            example[1],\n            example[1],\n        )\n        for layer in range(6)\n    ]\n    return alt.hconcat(\n        layer_viz[0]\n        & layer_viz[1]\n        & layer_viz[2]\n        & layer_viz[3]\n        & layer_viz[4]\n        & layer_viz[5]\n    )\n\n\nshow_example(viz_decoder_self)\n\nPreparing Data ...\nLoading Trained Model ...\nChecking Model Outputs:\n\nExample 0 ========\n\nSource Text (Input)        : &lt;s&gt; Drei Menschen wandern auf einem stark verschneiten Weg . &lt;/s&gt;\nTarget Text (Ground Truth) : &lt;s&gt; A &lt;unk&gt; of people are hiking throughout a heavily snowed path . &lt;/s&gt;\nModel Output               : &lt;s&gt; Three people hiking on a busy &lt;unk&gt; . &lt;/s&gt;",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "annotated_transformer.html#decoder-src-attention",
    "href": "annotated_transformer.html#decoder-src-attention",
    "title": "Prelims",
    "section": "Decoder Src Attention",
    "text": "Decoder Src Attention\n\ndef viz_decoder_src():\n    model, example_data = run_model_example(n_examples=1)\n    example = example_data[len(example_data) - 1]\n\n    layer_viz = [\n        visualize_layer(\n            model,\n            layer,\n            get_decoder_src,\n            max(len(example[1]), len(example[2])),\n            example[1],\n            example[2],\n        )\n        for layer in range(6)\n    ]\n    return alt.hconcat(\n        layer_viz[0]\n        & layer_viz[1]\n        & layer_viz[2]\n        & layer_viz[3]\n        & layer_viz[4]\n        & layer_viz[5]\n    )\n\n\nshow_example(viz_decoder_src)\n\nPreparing Data ...\nLoading Trained Model ...\nChecking Model Outputs:\n\nExample 0 ========\n\nSource Text (Input)        : &lt;s&gt; Baby sieht sich die BlÃ¤tter am Zweig eines Baumes an . &lt;/s&gt;\nTarget Text (Ground Truth) : &lt;s&gt; Baby looking at the leaves on a branch of a tree . &lt;/s&gt;\nModel Output               : &lt;s&gt; A baby is looking at the leaves at a tree . &lt;/s&gt;",
    "crumbs": [
      "Prelims"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "foo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  }
]