<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Simple 2x2 example – annotated-transformer</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-fa8f4b57214c5fa5fd583aab1b651259.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Simple 2x2 example – annotated-transformer">
<meta property="og:description" content="An annotated implementation of the Transformer paper.">
<meta property="og:image" content="https://mettafore.github.io/annotated-transformer/images/aiayn.png">
<meta property="og:site_name" content="annotated-transformer">
<meta name="twitter:title" content="Simple 2x2 example – annotated-transformer">
<meta name="twitter:description" content="An annotated implementation of the Transformer paper.">
<meta name="twitter:image" content="https://mettafore.github.io/annotated-transformer/images/aiayn.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">annotated-transformer</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./attention_is_all_you_need.html">Simple 2x2 example</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">annotated-transformer</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./core.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">core</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./annotated_transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prelims</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./attention_is_all_you_need.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Simple 2x2 example</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#imports" id="toc-imports" class="nav-link active" data-scroll-target="#imports">Imports</a></li>
  <li><a href="#coding-utf-8---" id="toc-coding-utf-8---" class="nav-link" data-scroll-target="#coding-utf-8---">-<em>- coding: utf-8 -</em>-</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section">—</a></li>
  <li><a href="#jupyter" id="toc-jupyter" class="nav-link" data-scroll-target="#jupyter">jupyter:</a></li>
  <li><a href="#jupytext" id="toc-jupytext" class="nav-link" data-scroll-target="#jupytext">jupytext:</a></li>
  <li><a href="#formats-ipynbpypercent" id="toc-formats-ipynbpypercent" class="nav-link" data-scroll-target="#formats-ipynbpypercent">formats: ipynb,py:percent</a></li>
  <li><a href="#text_representation" id="toc-text_representation" class="nav-link" data-scroll-target="#text_representation">text_representation:</a></li>
  <li><a href="#extension-.py" id="toc-extension-.py" class="nav-link" data-scroll-target="#extension-.py">extension: .py</a></li>
  <li><a href="#format_name-percent" id="toc-format_name-percent" class="nav-link" data-scroll-target="#format_name-percent">format_name: percent</a></li>
  <li><a href="#format_version-1.3" id="toc-format_version-1.3" class="nav-link" data-scroll-target="#format_version-1.3">format_version: ‘1.3’</a></li>
  <li><a href="#jupytext_version-1.13.0" id="toc-jupytext_version-1.13.0" class="nav-link" data-scroll-target="#jupytext_version-1.13.0">jupytext_version: 1.13.0</a></li>
  <li><a href="#kernelspec" id="toc-kernelspec" class="nav-link" data-scroll-target="#kernelspec">kernelspec:</a></li>
  <li><a href="#display_name-python-3-ipykernel" id="toc-display_name-python-3-ipykernel" class="nav-link" data-scroll-target="#display_name-python-3-ipykernel">display_name: Python 3 (ipykernel)</a></li>
  <li><a href="#language-python" id="toc-language-python" class="nav-link" data-scroll-target="#language-python">language: python</a></li>
  <li><a href="#name-python3" id="toc-name-python3" class="nav-link" data-scroll-target="#name-python3">name: python3</a></li>
  <li><a href="#section-1" id="toc-section-1" class="nav-link" data-scroll-target="#section-1">—</a></li>
  <li><a href="#markdown-idsx7uc-8jtsp7-tags" id="toc-markdown-idsx7uc-8jtsp7-tags" class="nav-link" data-scroll-target="#markdown-idsx7uc-8jtsp7-tags">%% [markdown] id=“SX7UC-8jTsp7” tags=[]</a></li>
  <li><a href="#section-2" id="toc-section-2" class="nav-link" data-scroll-target="#section-2"></a></li>
  <li><a href="#section-3" id="toc-section-3" class="nav-link" data-scroll-target="#section-3"></a></li>
  <li><a href="#section-4" id="toc-section-4" class="nav-link" data-scroll-target="#section-4"></a></li>
  <li><a href="#section-5" id="toc-section-5" class="nav-link" data-scroll-target="#section-5"></a></li>
  <li><a href="#section-6" id="toc-section-6" class="nav-link" data-scroll-target="#section-6"><img src="images/aiayn.png" width="70%"></a></li>
  <li><a href="#section-7" id="toc-section-7" class="nav-link" data-scroll-target="#section-7"></a></li>
  <li><a href="#v2022-austin-huang-suraj-subramanian-jonathan-sum-khalid-almubarak" id="toc-v2022-austin-huang-suraj-subramanian-jonathan-sum-khalid-almubarak" class="nav-link" data-scroll-target="#v2022-austin-huang-suraj-subramanian-jonathan-sum-khalid-almubarak">* *v2022: Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak,</a></li>
  <li><a href="#and-stella-biderman." id="toc-and-stella-biderman." class="nav-link" data-scroll-target="#and-stella-biderman.">and Stella Biderman.*</a></li>
  <li><a href="#original" id="toc-original" class="nav-link" data-scroll-target="#original">* *Original:</a></li>
  <li><a href="#sasha-rush." id="toc-sasha-rush." class="nav-link" data-scroll-target="#sasha-rush.">Sasha Rush.*</a></li>
  <li><a href="#section-8" id="toc-section-8" class="nav-link" data-scroll-target="#section-8"></a></li>
  <li><a href="#section-9" id="toc-section-9" class="nav-link" data-scroll-target="#section-9"></a></li>
  <li><a href="#the-transformer-has-been-on-a-lot-of" id="toc-the-transformer-has-been-on-a-lot-of" class="nav-link" data-scroll-target="#the-transformer-has-been-on-a-lot-of">The Transformer has been on a lot of</a></li>
  <li><a href="#peoples-minds-over-the-last-year-five-years." id="toc-peoples-minds-over-the-last-year-five-years." class="nav-link" data-scroll-target="#peoples-minds-over-the-last-year-five-years.">people’s minds over the last <s>year</s> five years.</a></li>
  <li><a href="#this-post-presents-an-annotated-version-of-the-paper-in-the" id="toc-this-post-presents-an-annotated-version-of-the-paper-in-the" class="nav-link" data-scroll-target="#this-post-presents-an-annotated-version-of-the-paper-in-the">This post presents an annotated version of the paper in the</a></li>
  <li><a href="#form-of-a-line-by-line-implementation.-it-reorders-and-deletes" id="toc-form-of-a-line-by-line-implementation.-it-reorders-and-deletes" class="nav-link" data-scroll-target="#form-of-a-line-by-line-implementation.-it-reorders-and-deletes">form of a line-by-line implementation. It reorders and deletes</a></li>
  <li><a href="#some-sections-from-the-original-paper-and-adds-comments" id="toc-some-sections-from-the-original-paper-and-adds-comments" class="nav-link" data-scroll-target="#some-sections-from-the-original-paper-and-adds-comments">some sections from the original paper and adds comments</a></li>
  <li><a href="#throughout.-this-document-itself-is-a-working-notebook-and-should" id="toc-throughout.-this-document-itself-is-a-working-notebook-and-should" class="nav-link" data-scroll-target="#throughout.-this-document-itself-is-a-working-notebook-and-should">throughout. This document itself is a working notebook, and should</a></li>
  <li><a href="#be-a-completely-usable-implementation." id="toc-be-a-completely-usable-implementation." class="nav-link" data-scroll-target="#be-a-completely-usable-implementation.">be a completely usable implementation.</a></li>
  <li><a href="#code-is-available" id="toc-code-is-available" class="nav-link" data-scroll-target="#code-is-available">Code is available</a></li>
  <li><a href="#here." id="toc-here." class="nav-link" data-scroll-target="#here.">here.</a></li>
  <li><a href="#section-10" id="toc-section-10" class="nav-link" data-scroll-target="#section-10"></a></li>
  <li><a href="#markdown-idrsntdwkhtsp-" id="toc-markdown-idrsntdwkhtsp-" class="nav-link" data-scroll-target="#markdown-idrsntdwkhtsp-">%% [markdown] id=“RSntDwKhTsp-”</a></li>
  <li><a href="#markdown-idbhmohn9ltsp8" id="toc-markdown-idbhmohn9ltsp8" class="nav-link" data-scroll-target="#markdown-idbhmohn9ltsp8">%% [markdown] id=“BhmOhn9lTsp8”</a></li>
  <li><a href="#prelims" id="toc-prelims" class="nav-link" data-scroll-target="#prelims"># Prelims</a></li>
  <li><a href="#section-11" id="toc-section-11" class="nav-link" data-scroll-target="#section-11"></a></li>
  <li><a href="#skip" id="toc-skip" class="nav-link" data-scroll-target="#skip"></a><a href="#background" class="nav-link" data-scroll-target="#background">Skip</a></li>
  <li><a href="#idnwclcbh6tsp8" id="toc-idnwclcbh6tsp8" class="nav-link" data-scroll-target="#idnwclcbh6tsp8">%% id=“NwClcbH6Tsp8”</a></li>
  <li><a href="#pip-install--r-requirements.txt" id="toc-pip-install--r-requirements.txt" class="nav-link" data-scroll-target="#pip-install--r-requirements.txt"># !pip install -r requirements.txt</a></li>
  <li><a href="#idnwclcbh6tsp8-1" id="toc-idnwclcbh6tsp8-1" class="nav-link" data-scroll-target="#idnwclcbh6tsp8-1">%% id=“NwClcbH6Tsp8”</a></li>
  <li><a href="#uncomment-for-colab" id="toc-uncomment-for-colab" class="nav-link" data-scroll-target="#uncomment-for-colab"># Uncomment for colab</a></li>
  <li><a href="#section-12" id="toc-section-12" class="nav-link" data-scroll-target="#section-12"></a></li>
  <li><a href="#pip-install--q-torchdata0.3.0-torchtext0.12-spacy3.2-altair-gputil" id="toc-pip-install--q-torchdata0.3.0-torchtext0.12-spacy3.2-altair-gputil" class="nav-link" data-scroll-target="#pip-install--q-torchdata0.3.0-torchtext0.12-spacy3.2-altair-gputil"># !pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil</a></li>
  <li><a href="#python--m-spacy-download-de_core_news_sm" id="toc-python--m-spacy-download-de_core_news_sm" class="nav-link" data-scroll-target="#python--m-spacy-download-de_core_news_sm"># !python -m spacy download de_core_news_sm</a></li>
  <li><a href="#python--m-spacy-download-en_core_web_sm" id="toc-python--m-spacy-download-en_core_web_sm" class="nav-link" data-scroll-target="#python--m-spacy-download-en_core_web_sm"># !python -m spacy download en_core_web_sm</a></li>
  <li><a href="#idv1-1mx6otsp9" id="toc-idv1-1mx6otsp9" class="nav-link" data-scroll-target="#idv1-1mx6otsp9">%% id=“v1-1MX6oTsp9”</a></li>
  <li><a href="#set-to-false-to-skip-notebook-execution-e.g.-for-debugging" id="toc-set-to-false-to-skip-notebook-execution-e.g.-for-debugging" class="nav-link" data-scroll-target="#set-to-false-to-skip-notebook-execution-e.g.-for-debugging">Set to False to skip notebook execution (e.g.&nbsp;for debugging)</a></li>
  <li><a href="#section-13" id="toc-section-13" class="nav-link" data-scroll-target="#section-13">%%</a></li>
  <li><a href="#some-convenience-helper-functions-used-throughout-the-notebook" id="toc-some-convenience-helper-functions-used-throughout-the-notebook" class="nav-link" data-scroll-target="#some-convenience-helper-functions-used-throughout-the-notebook">Some convenience helper functions used throughout the notebook</a></li>
  <li><a href="#markdown-idjx49wryftsp-" id="toc-markdown-idjx49wryftsp-" class="nav-link" data-scroll-target="#markdown-idjx49wryftsp-">%% [markdown] id=“jx49WRyfTsp-”</a></li>
  <li><a href="#my-comments-are-blockquoted.-the-main-text-is-all-from-the-paper-itself." id="toc-my-comments-are-blockquoted.-the-main-text-is-all-from-the-paper-itself." class="nav-link" data-scroll-target="#my-comments-are-blockquoted.-the-main-text-is-all-from-the-paper-itself.">&gt; My comments are blockquoted. The main text is all from the paper itself.</a></li>
  <li><a href="#markdown-id7phvewghtsp_" id="toc-markdown-id7phvewghtsp_" class="nav-link" data-scroll-target="#markdown-id7phvewghtsp_">%% [markdown] id=“7phVeWghTsp_”</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"># Background</a></li>
  <li><a href="#markdown-id83zds91dtsqa" id="toc-markdown-id83zds91dtsqa" class="nav-link" data-scroll-target="#markdown-id83zds91dtsqa">%% [markdown] id=“83ZDS91dTsqA”</a></li>
  <li><a href="#section-14" id="toc-section-14" class="nav-link" data-scroll-target="#section-14"></a></li>
  <li><a href="#the-goal-of-reducing-sequential-computation-also-forms-the" id="toc-the-goal-of-reducing-sequential-computation-also-forms-the" class="nav-link" data-scroll-target="#the-goal-of-reducing-sequential-computation-also-forms-the">The goal of reducing sequential computation also forms the</a></li>
  <li><a href="#foundation-of-the-extended-neural-gpu-bytenet-and-convs2s-all-of" id="toc-foundation-of-the-extended-neural-gpu-bytenet-and-convs2s-all-of" class="nav-link" data-scroll-target="#foundation-of-the-extended-neural-gpu-bytenet-and-convs2s-all-of">foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of</a></li>
  <li><a href="#which-use-convolutional-neural-networks-as-basic-building-block" id="toc-which-use-convolutional-neural-networks-as-basic-building-block" class="nav-link" data-scroll-target="#which-use-convolutional-neural-networks-as-basic-building-block">which use convolutional neural networks as basic building block,</a></li>
  <li><a href="#computing-hidden-representations-in-parallel-for-all-input-and" id="toc-computing-hidden-representations-in-parallel-for-all-input-and" class="nav-link" data-scroll-target="#computing-hidden-representations-in-parallel-for-all-input-and">computing hidden representations in parallel for all input and</a></li>
  <li><a href="#output-positions.-in-these-models-the-number-of-operations-required" id="toc-output-positions.-in-these-models-the-number-of-operations-required" class="nav-link" data-scroll-target="#output-positions.-in-these-models-the-number-of-operations-required">output positions. In these models, the number of operations required</a></li>
  <li><a href="#to-relate-signals-from-two-arbitrary-input-or-output-positions-grows" id="toc-to-relate-signals-from-two-arbitrary-input-or-output-positions-grows" class="nav-link" data-scroll-target="#to-relate-signals-from-two-arbitrary-input-or-output-positions-grows">to relate signals from two arbitrary input or output positions grows</a></li>
  <li><a href="#in-the-distance-between-positions-linearly-for-convs2s-and" id="toc-in-the-distance-between-positions-linearly-for-convs2s-and" class="nav-link" data-scroll-target="#in-the-distance-between-positions-linearly-for-convs2s-and">in the distance between positions, linearly for ConvS2S and</a></li>
  <li><a href="#logarithmically-for-bytenet.-this-makes-it-more-difficult-to-learn" id="toc-logarithmically-for-bytenet.-this-makes-it-more-difficult-to-learn" class="nav-link" data-scroll-target="#logarithmically-for-bytenet.-this-makes-it-more-difficult-to-learn">logarithmically for ByteNet. This makes it more difficult to learn</a></li>
  <li><a href="#dependencies-between-distant-positions.-in-the-transformer-this-is" id="toc-dependencies-between-distant-positions.-in-the-transformer-this-is" class="nav-link" data-scroll-target="#dependencies-between-distant-positions.-in-the-transformer-this-is">dependencies between distant positions. In the Transformer this is</a></li>
  <li><a href="#reduced-to-a-constant-number-of-operations-albeit-at-the-cost-of" id="toc-reduced-to-a-constant-number-of-operations-albeit-at-the-cost-of" class="nav-link" data-scroll-target="#reduced-to-a-constant-number-of-operations-albeit-at-the-cost-of">reduced to a constant number of operations, albeit at the cost of</a></li>
  <li><a href="#reduced-effective-resolution-due-to-averaging-attention-weighted" id="toc-reduced-effective-resolution-due-to-averaging-attention-weighted" class="nav-link" data-scroll-target="#reduced-effective-resolution-due-to-averaging-attention-weighted">reduced effective resolution due to averaging attention-weighted</a></li>
  <li><a href="#positions-an-effect-we-counteract-with-multi-head-attention." id="toc-positions-an-effect-we-counteract-with-multi-head-attention." class="nav-link" data-scroll-target="#positions-an-effect-we-counteract-with-multi-head-attention.">positions, an effect we counteract with Multi-Head Attention.</a></li>
  <li><a href="#section-15" id="toc-section-15" class="nav-link" data-scroll-target="#section-15"></a></li>
  <li><a href="#self-attention-sometimes-called-intra-attention-is-an-attention" id="toc-self-attention-sometimes-called-intra-attention-is-an-attention" class="nav-link" data-scroll-target="#self-attention-sometimes-called-intra-attention-is-an-attention">Self-attention, sometimes called intra-attention is an attention</a></li>
  <li><a href="#mechanism-relating-different-positions-of-a-single-sequence-in-order" id="toc-mechanism-relating-different-positions-of-a-single-sequence-in-order" class="nav-link" data-scroll-target="#mechanism-relating-different-positions-of-a-single-sequence-in-order">mechanism relating different positions of a single sequence in order</a></li>
  <li><a href="#to-compute-a-representation-of-the-sequence.-self-attention-has-been" id="toc-to-compute-a-representation-of-the-sequence.-self-attention-has-been" class="nav-link" data-scroll-target="#to-compute-a-representation-of-the-sequence.-self-attention-has-been">to compute a representation of the sequence. Self-attention has been</a></li>
  <li><a href="#used-successfully-in-a-variety-of-tasks-including-reading" id="toc-used-successfully-in-a-variety-of-tasks-including-reading" class="nav-link" data-scroll-target="#used-successfully-in-a-variety-of-tasks-including-reading">used successfully in a variety of tasks including reading</a></li>
  <li><a href="#comprehension-abstractive-summarization-textual-entailment-and" id="toc-comprehension-abstractive-summarization-textual-entailment-and" class="nav-link" data-scroll-target="#comprehension-abstractive-summarization-textual-entailment-and">comprehension, abstractive summarization, textual entailment and</a></li>
  <li><a href="#learning-task-independent-sentence-representations.-end-to-end" id="toc-learning-task-independent-sentence-representations.-end-to-end" class="nav-link" data-scroll-target="#learning-task-independent-sentence-representations.-end-to-end">learning task-independent sentence representations. End-to-end</a></li>
  <li><a href="#memory-networks-are-based-on-a-recurrent-attention-mechanism-instead" id="toc-memory-networks-are-based-on-a-recurrent-attention-mechanism-instead" class="nav-link" data-scroll-target="#memory-networks-are-based-on-a-recurrent-attention-mechanism-instead">memory networks are based on a recurrent attention mechanism instead</a></li>
  <li><a href="#of-sequencealigned-recurrence-and-have-been-shown-to-perform-well-on" id="toc-of-sequencealigned-recurrence-and-have-been-shown-to-perform-well-on" class="nav-link" data-scroll-target="#of-sequencealigned-recurrence-and-have-been-shown-to-perform-well-on">of sequencealigned recurrence and have been shown to perform well on</a></li>
  <li><a href="#simple-language-question-answering-and-language-modeling-tasks." id="toc-simple-language-question-answering-and-language-modeling-tasks." class="nav-link" data-scroll-target="#simple-language-question-answering-and-language-modeling-tasks.">simple-language question answering and language modeling tasks.</a></li>
  <li><a href="#section-16" id="toc-section-16" class="nav-link" data-scroll-target="#section-16"></a></li>
  <li><a href="#to-the-best-of-our-knowledge-however-the-transformer-is-the-first" id="toc-to-the-best-of-our-knowledge-however-the-transformer-is-the-first" class="nav-link" data-scroll-target="#to-the-best-of-our-knowledge-however-the-transformer-is-the-first">To the best of our knowledge, however, the Transformer is the first</a></li>
  <li><a href="#transduction-model-relying-entirely-on-self-attention-to-compute" id="toc-transduction-model-relying-entirely-on-self-attention-to-compute" class="nav-link" data-scroll-target="#transduction-model-relying-entirely-on-self-attention-to-compute">transduction model relying entirely on self-attention to compute</a></li>
  <li><a href="#representations-of-its-input-and-output-without-using-sequence" id="toc-representations-of-its-input-and-output-without-using-sequence" class="nav-link" data-scroll-target="#representations-of-its-input-and-output-without-using-sequence">representations of its input and output without using sequence</a></li>
  <li><a href="#aligned-rnns-or-convolution." id="toc-aligned-rnns-or-convolution." class="nav-link" data-scroll-target="#aligned-rnns-or-convolution.">aligned RNNs or convolution.</a></li>
  <li><a href="#markdown" id="toc-markdown" class="nav-link" data-scroll-target="#markdown">%% [markdown]</a></li>
  <li><a href="#part-1-model-architecture" id="toc-part-1-model-architecture" class="nav-link" data-scroll-target="#part-1-model-architecture"># Part 1: Model Architecture</a></li>
  <li><a href="#markdown-idpfrpajeztsqb" id="toc-markdown-idpfrpajeztsqb" class="nav-link" data-scroll-target="#markdown-idpfrpajeztsqb">%% [markdown] id=“pFrPajezTsqB”</a></li>
  <li><a href="#model-architecture" id="toc-model-architecture" class="nav-link" data-scroll-target="#model-architecture"># Model Architecture</a></li>
  <li><a href="#markdown-idreuu_h-ftsqb" id="toc-markdown-idreuu_h-ftsqb" class="nav-link" data-scroll-target="#markdown-idreuu_h-ftsqb">%% [markdown] id=“ReuU_h-fTsqB”</a></li>
  <li><a href="#section-17" id="toc-section-17" class="nav-link" data-scroll-target="#section-17"></a></li>
  <li><a href="#most-competitive-neural-sequence-transduction-models-have-an" id="toc-most-competitive-neural-sequence-transduction-models-have-an" class="nav-link" data-scroll-target="#most-competitive-neural-sequence-transduction-models-have-an">Most competitive neural sequence transduction models have an</a></li>
  <li><a href="#encoder-decoder-structure" id="toc-encoder-decoder-structure" class="nav-link" data-scroll-target="#encoder-decoder-structure">encoder-decoder structure</a></li>
  <li><a href="#cite.-here-the-encoder-maps-an" id="toc-cite.-here-the-encoder-maps-an" class="nav-link" data-scroll-target="#cite.-here-the-encoder-maps-an">(cite). Here, the encoder maps an</a></li>
  <li><a href="#input-sequence-of-symbol-representations-x_1-...-x_n-to-a" id="toc-input-sequence-of-symbol-representations-x_1-...-x_n-to-a" class="nav-link" data-scroll-target="#input-sequence-of-symbol-representations-x_1-...-x_n-to-a">input sequence of symbol representations <span class="math inline">\((x_1, ..., x_n)\)</span> to a</a></li>
  <li><a href="#sequence-of-continuous-representations-mathbfz-z_1-...-z_n.-given-mathbfz-the-decoder-then-generates-an-output" id="toc-sequence-of-continuous-representations-mathbfz-z_1-...-z_n.-given-mathbfz-the-decoder-then-generates-an-output" class="nav-link" data-scroll-target="#sequence-of-continuous-representations-mathbfz-z_1-...-z_n.-given-mathbfz-the-decoder-then-generates-an-output">sequence of continuous representations <span class="math inline">\(\mathbf{z} = (z_1, ...,
  # z_n)\)</span>. Given <span class="math inline">\(\mathbf{z}\)</span>, the decoder then generates an output</a></li>
  <li><a href="#sequence-y_1...y_m-of-symbols-one-element-at-a-time.-at-each" id="toc-sequence-y_1...y_m-of-symbols-one-element-at-a-time.-at-each" class="nav-link" data-scroll-target="#sequence-y_1...y_m-of-symbols-one-element-at-a-time.-at-each">sequence <span class="math inline">\((y_1,...,y_m)\)</span> of symbols one element at a time. At each</a></li>
  <li><a href="#step-the-model-is-auto-regressive" id="toc-step-the-model-is-auto-regressive" class="nav-link" data-scroll-target="#step-the-model-is-auto-regressive">step the model is auto-regressive</a></li>
  <li><a href="#cite-consuming-the-previously" id="toc-cite-consuming-the-previously" class="nav-link" data-scroll-target="#cite-consuming-the-previously">(cite), consuming the previously</a></li>
  <li><a href="#generated-symbols-as-additional-input-when-generating-the-next." id="toc-generated-symbols-as-additional-input-when-generating-the-next." class="nav-link" data-scroll-target="#generated-symbols-as-additional-input-when-generating-the-next.">generated symbols as additional input when generating the next.</a></li>
  <li><a href="#idk0xgxhzrtsqb" id="toc-idk0xgxhzrtsqb" class="nav-link" data-scroll-target="#idk0xgxhzrtsqb">%% id=“k0XGXhzRTsqB”</a></li>
  <li><a href="#idnkgoh2rstsqc" id="toc-idnkgoh2rstsqc" class="nav-link" data-scroll-target="#idnkgoh2rstsqc">%% id=“NKGoH2RsTsqC”</a></li>
  <li><a href="#markdown-idmooenf_jtsqc" id="toc-markdown-idmooenf_jtsqc" class="nav-link" data-scroll-target="#markdown-idmooenf_jtsqc">%% [markdown] id=“mOoEnF_jTsqC”</a></li>
  <li><a href="#section-18" id="toc-section-18" class="nav-link" data-scroll-target="#section-18"></a></li>
  <li><a href="#the-transformer-follows-this-overall-architecture-using-stacked" id="toc-the-transformer-follows-this-overall-architecture-using-stacked" class="nav-link" data-scroll-target="#the-transformer-follows-this-overall-architecture-using-stacked">The Transformer follows this overall architecture using stacked</a></li>
  <li><a href="#self-attention-and-point-wise-fully-connected-layers-for-both-the" id="toc-self-attention-and-point-wise-fully-connected-layers-for-both-the" class="nav-link" data-scroll-target="#self-attention-and-point-wise-fully-connected-layers-for-both-the">self-attention and point-wise, fully connected layers for both the</a></li>
  <li><a href="#encoder-and-decoder-shown-in-the-left-and-right-halves-of-figure-1" id="toc-encoder-and-decoder-shown-in-the-left-and-right-halves-of-figure-1" class="nav-link" data-scroll-target="#encoder-and-decoder-shown-in-the-left-and-right-halves-of-figure-1">encoder and decoder, shown in the left and right halves of Figure 1,</a></li>
  <li><a href="#respectively." id="toc-respectively." class="nav-link" data-scroll-target="#respectively.">respectively.</a></li>
  <li><a href="#markdown-idoredwloytsqc" id="toc-markdown-idoredwloytsqc" class="nav-link" data-scroll-target="#markdown-idoredwloytsqc">%% [markdown] id=“oredWloYTsqC”</a></li>
  <li><a href="#section-19" id="toc-section-19" class="nav-link" data-scroll-target="#section-19"><img src="images/ModalNet-21.png" class="img-fluid"></a></li>
  <li><a href="#markdown-idbh092nzbtsqd" id="toc-markdown-idbh092nzbtsqd" class="nav-link" data-scroll-target="#markdown-idbh092nzbtsqd">%% [markdown] id=“bh092NZBTsqD”</a></li>
  <li><a href="#encoder-and-decoder-stacks" id="toc-encoder-and-decoder-stacks" class="nav-link" data-scroll-target="#encoder-and-decoder-stacks">## Encoder and Decoder Stacks</a></li>
  <li><a href="#section-20" id="toc-section-20" class="nav-link" data-scroll-target="#section-20"></a></li>
  <li><a href="#encoder" id="toc-encoder" class="nav-link" data-scroll-target="#encoder">### Encoder</a></li>
  <li><a href="#section-21" id="toc-section-21" class="nav-link" data-scroll-target="#section-21"></a></li>
  <li><a href="#the-encoder-is-composed-of-a-stack-of-n6-identical-layers." id="toc-the-encoder-is-composed-of-a-stack-of-n6-identical-layers." class="nav-link" data-scroll-target="#the-encoder-is-composed-of-a-stack-of-n6-identical-layers.">The encoder is composed of a stack of <span class="math inline">\(N=6\)</span> identical layers.</a></li>
  <li><a href="#id2gxtapuytsqd" id="toc-id2gxtapuytsqd" class="nav-link" data-scroll-target="#id2gxtapuytsqd">%% id=“2gxTApUYTsqD”</a></li>
  <li><a href="#idxqvtz9mktsqd" id="toc-idxqvtz9mktsqd" class="nav-link" data-scroll-target="#idxqvtz9mktsqd">%% id=“xqVTz9MkTsqD”</a></li>
  <li><a href="#markdown-idgjakgjgwtsqd" id="toc-markdown-idgjakgjgwtsqd" class="nav-link" data-scroll-target="#markdown-idgjakgjgwtsqd">%% [markdown] id=“GjAKgjGwTsqD”</a></li>
  <li><a href="#section-22" id="toc-section-22" class="nav-link" data-scroll-target="#section-22"></a></li>
  <li><a href="#we-employ-a-residual-connection" id="toc-we-employ-a-residual-connection" class="nav-link" data-scroll-target="#we-employ-a-residual-connection">We employ a residual connection</a></li>
  <li><a href="#cite-around-each-of-the-two" id="toc-cite-around-each-of-the-two" class="nav-link" data-scroll-target="#cite-around-each-of-the-two">(cite) around each of the two</a></li>
  <li><a href="#sub-layers-followed-by-layer-normalization" id="toc-sub-layers-followed-by-layer-normalization" class="nav-link" data-scroll-target="#sub-layers-followed-by-layer-normalization">sub-layers, followed by layer normalization</a></li>
  <li><a href="#cite." id="toc-cite." class="nav-link" data-scroll-target="#cite.">(cite).</a></li>
  <li><a href="#id3jka_prztsqe" id="toc-id3jka_prztsqe" class="nav-link" data-scroll-target="#id3jka_prztsqe">%% id=“3jKa_prZTsqE”</a></li>
  <li><a href="#markdown-idnxsj3qymtsqe" id="toc-markdown-idnxsj3qymtsqe" class="nav-link" data-scroll-target="#markdown-idnxsj3qymtsqe">%% [markdown] id=“nXSJ3QYmTsqE”</a></li>
  <li><a href="#section-23" id="toc-section-23" class="nav-link" data-scroll-target="#section-23"></a></li>
  <li><a href="#that-is-the-output-of-each-sub-layer-is-mathrmlayernormx-mathrmsublayerx-where-mathrmsublayerx-is-the-function" id="toc-that-is-the-output-of-each-sub-layer-is-mathrmlayernormx-mathrmsublayerx-where-mathrmsublayerx-is-the-function" class="nav-link" data-scroll-target="#that-is-the-output-of-each-sub-layer-is-mathrmlayernormx-mathrmsublayerx-where-mathrmsublayerx-is-the-function">That is, the output of each sub-layer is <span class="math inline">\(\mathrm{LayerNorm}(x +
  # \mathrm{Sublayer}(x))\)</span>, where <span class="math inline">\(\mathrm{Sublayer}(x)\)</span> is the function</a></li>
  <li><a href="#implemented-by-the-sub-layer-itself.-we-apply-dropout" id="toc-implemented-by-the-sub-layer-itself.-we-apply-dropout" class="nav-link" data-scroll-target="#implemented-by-the-sub-layer-itself.-we-apply-dropout">implemented by the sub-layer itself. We apply dropout</a></li>
  <li><a href="#cite-to-the" id="toc-cite-to-the" class="nav-link" data-scroll-target="#cite-to-the">(cite) to the</a></li>
  <li><a href="#output-of-each-sub-layer-before-it-is-added-to-the-sub-layer-input" id="toc-output-of-each-sub-layer-before-it-is-added-to-the-sub-layer-input" class="nav-link" data-scroll-target="#output-of-each-sub-layer-before-it-is-added-to-the-sub-layer-input">output of each sub-layer, before it is added to the sub-layer input</a></li>
  <li><a href="#and-normalized." id="toc-and-normalized." class="nav-link" data-scroll-target="#and-normalized.">and normalized.</a></li>
  <li><a href="#section-24" id="toc-section-24" class="nav-link" data-scroll-target="#section-24"></a></li>
  <li><a href="#to-facilitate-these-residual-connections-all-sub-layers-in-the" id="toc-to-facilitate-these-residual-connections-all-sub-layers-in-the" class="nav-link" data-scroll-target="#to-facilitate-these-residual-connections-all-sub-layers-in-the">To facilitate these residual connections, all sub-layers in the</a></li>
  <li><a href="#model-as-well-as-the-embedding-layers-produce-outputs-of-dimension" id="toc-model-as-well-as-the-embedding-layers-produce-outputs-of-dimension" class="nav-link" data-scroll-target="#model-as-well-as-the-embedding-layers-produce-outputs-of-dimension">model, as well as the embedding layers, produce outputs of dimension</a></li>
  <li><a href="#d_textmodel512." id="toc-d_textmodel512." class="nav-link" data-scroll-target="#d_textmodel512."><span class="math inline">\(d_{\text{model}}=512\)</span>.</a></li>
  <li><a href="#idu1p7zi0etsqe" id="toc-idu1p7zi0etsqe" class="nav-link" data-scroll-target="#idu1p7zi0etsqe">%% id=“U1P7zI0eTsqE”</a></li>
  <li><a href="#markdown-idml6odleqtsqe" id="toc-markdown-idml6odleqtsqe" class="nav-link" data-scroll-target="#markdown-idml6odleqtsqe">%% [markdown] id=“ML6oDlEqTsqE”</a></li>
  <li><a href="#section-25" id="toc-section-25" class="nav-link" data-scroll-target="#section-25"></a></li>
  <li><a href="#each-layer-has-two-sub-layers.-the-first-is-a-multi-head" id="toc-each-layer-has-two-sub-layers.-the-first-is-a-multi-head" class="nav-link" data-scroll-target="#each-layer-has-two-sub-layers.-the-first-is-a-multi-head">Each layer has two sub-layers. The first is a multi-head</a></li>
  <li><a href="#self-attention-mechanism-and-the-second-is-a-simple-position-wise" id="toc-self-attention-mechanism-and-the-second-is-a-simple-position-wise" class="nav-link" data-scroll-target="#self-attention-mechanism-and-the-second-is-a-simple-position-wise">self-attention mechanism, and the second is a simple, position-wise</a></li>
  <li><a href="#fully-connected-feed-forward-network." id="toc-fully-connected-feed-forward-network." class="nav-link" data-scroll-target="#fully-connected-feed-forward-network.">fully connected feed-forward network.</a></li>
  <li><a href="#idqykufr6gtsqe" id="toc-idqykufr6gtsqe" class="nav-link" data-scroll-target="#idqykufr6gtsqe">%% id=“qYkUFr6GTsqE”</a></li>
  <li><a href="#markdown-id7ecoqihktsqf" id="toc-markdown-id7ecoqihktsqf" class="nav-link" data-scroll-target="#markdown-id7ecoqihktsqf">%% [markdown] id=“7ecOQIhkTsqF”</a></li>
  <li><a href="#decoder" id="toc-decoder" class="nav-link" data-scroll-target="#decoder">### Decoder</a></li>
  <li><a href="#section-26" id="toc-section-26" class="nav-link" data-scroll-target="#section-26"></a></li>
  <li><a href="#the-decoder-is-also-composed-of-a-stack-of-n6-identical-layers." id="toc-the-decoder-is-also-composed-of-a-stack-of-n6-identical-layers." class="nav-link" data-scroll-target="#the-decoder-is-also-composed-of-a-stack-of-n6-identical-layers.">The decoder is also composed of a stack of <span class="math inline">\(N=6\)</span> identical layers.</a></li>
  <li><a href="#section-27" id="toc-section-27" class="nav-link" data-scroll-target="#section-27"></a></li>
  <li><a href="#section-28" id="toc-section-28" class="nav-link" data-scroll-target="#section-28">%%</a></li>
  <li><a href="#markdown-iddxlcb12ptsqf" id="toc-markdown-iddxlcb12ptsqf" class="nav-link" data-scroll-target="#markdown-iddxlcb12ptsqf">%% [markdown] id=“dXlCB12pTsqF”</a></li>
  <li><a href="#section-29" id="toc-section-29" class="nav-link" data-scroll-target="#section-29"></a></li>
  <li><a href="#in-addition-to-the-two-sub-layers-in-each-encoder-layer-the-decoder" id="toc-in-addition-to-the-two-sub-layers-in-each-encoder-layer-the-decoder" class="nav-link" data-scroll-target="#in-addition-to-the-two-sub-layers-in-each-encoder-layer-the-decoder">In addition to the two sub-layers in each encoder layer, the decoder</a></li>
  <li><a href="#inserts-a-third-sub-layer-which-performs-multi-head-attention-over" id="toc-inserts-a-third-sub-layer-which-performs-multi-head-attention-over" class="nav-link" data-scroll-target="#inserts-a-third-sub-layer-which-performs-multi-head-attention-over">inserts a third sub-layer, which performs multi-head attention over</a></li>
  <li><a href="#the-output-of-the-encoder-stack.-similar-to-the-encoder-we-employ" id="toc-the-output-of-the-encoder-stack.-similar-to-the-encoder-we-employ" class="nav-link" data-scroll-target="#the-output-of-the-encoder-stack.-similar-to-the-encoder-we-employ">the output of the encoder stack. Similar to the encoder, we employ</a></li>
  <li><a href="#residual-connections-around-each-of-the-sub-layers-followed-by" id="toc-residual-connections-around-each-of-the-sub-layers-followed-by" class="nav-link" data-scroll-target="#residual-connections-around-each-of-the-sub-layers-followed-by">residual connections around each of the sub-layers, followed by</a></li>
  <li><a href="#layer-normalization." id="toc-layer-normalization." class="nav-link" data-scroll-target="#layer-normalization.">layer normalization.</a></li>
  <li><a href="#idm2ha1xfqtsqf" id="toc-idm2ha1xfqtsqf" class="nav-link" data-scroll-target="#idm2ha1xfqtsqf">%% id=“M2hA1xFQTsqF”</a></li>
  <li><a href="#markdown-idfzz5rll4tsqf" id="toc-markdown-idfzz5rll4tsqf" class="nav-link" data-scroll-target="#markdown-idfzz5rll4tsqf">%% [markdown] id=“FZz5rLl4TsqF”</a></li>
  <li><a href="#section-30" id="toc-section-30" class="nav-link" data-scroll-target="#section-30"></a></li>
  <li><a href="#we-also-modify-the-self-attention-sub-layer-in-the-decoder-stack-to" id="toc-we-also-modify-the-self-attention-sub-layer-in-the-decoder-stack-to" class="nav-link" data-scroll-target="#we-also-modify-the-self-attention-sub-layer-in-the-decoder-stack-to">We also modify the self-attention sub-layer in the decoder stack to</a></li>
  <li><a href="#prevent-positions-from-attending-to-subsequent-positions.-this" id="toc-prevent-positions-from-attending-to-subsequent-positions.-this" class="nav-link" data-scroll-target="#prevent-positions-from-attending-to-subsequent-positions.-this">prevent positions from attending to subsequent positions. This</a></li>
  <li><a href="#masking-combined-with-fact-that-the-output-embeddings-are-offset-by" id="toc-masking-combined-with-fact-that-the-output-embeddings-are-offset-by" class="nav-link" data-scroll-target="#masking-combined-with-fact-that-the-output-embeddings-are-offset-by">masking, combined with fact that the output embeddings are offset by</a></li>
  <li><a href="#one-position-ensures-that-the-predictions-for-position-i-can" id="toc-one-position-ensures-that-the-predictions-for-position-i-can" class="nav-link" data-scroll-target="#one-position-ensures-that-the-predictions-for-position-i-can">one position, ensures that the predictions for position <span class="math inline">\(i\)</span> can</a></li>
  <li><a href="#depend-only-on-the-known-outputs-at-positions-less-than-i." id="toc-depend-only-on-the-known-outputs-at-positions-less-than-i." class="nav-link" data-scroll-target="#depend-only-on-the-known-outputs-at-positions-less-than-i.">depend only on the known outputs at positions less than <span class="math inline">\(i\)</span>.</a></li>
  <li><a href="#idqn98o2l3tsqf" id="toc-idqn98o2l3tsqf" class="nav-link" data-scroll-target="#idqn98o2l3tsqf">%% id=“QN98O2l3TsqF”</a></li>
  <li><a href="#markdown-idvg_f_w-ptsqg" id="toc-markdown-idvg_f_w-ptsqg" class="nav-link" data-scroll-target="#markdown-idvg_f_w-ptsqg">%% [markdown] id=“Vg_f_w-PTsqG”</a></li>
  <li><a href="#section-31" id="toc-section-31" class="nav-link" data-scroll-target="#section-31"></a></li>
  <li><a href="#below-the-attention-mask-shows-the-position-each-tgt-word-row-is" id="toc-below-the-attention-mask-shows-the-position-each-tgt-word-row-is" class="nav-link" data-scroll-target="#below-the-attention-mask-shows-the-position-each-tgt-word-row-is">&gt; Below the attention mask shows the position each tgt word (row) is</a></li>
  <li><a href="#allowed-to-look-at-column.-words-are-blocked-for-attending-to" id="toc-allowed-to-look-at-column.-words-are-blocked-for-attending-to" class="nav-link" data-scroll-target="#allowed-to-look-at-column.-words-are-blocked-for-attending-to">&gt; allowed to look at (column). Words are blocked for attending to</a></li>
  <li><a href="#future-words-during-training." id="toc-future-words-during-training." class="nav-link" data-scroll-target="#future-words-during-training.">&gt; future words during training.</a></li>
  <li><a href="#idht_ftgyaokc4" id="toc-idht_ftgyaokc4" class="nav-link" data-scroll-target="#idht_ftgyaokc4">%% id=“ht_FtgYAokC4”</a></li>
  <li><a href="#markdown-idqto_yg7btsqg" id="toc-markdown-idqto_yg7btsqg" class="nav-link" data-scroll-target="#markdown-idqto_yg7btsqg">%% [markdown] id=“Qto_yg7BTsqG”</a></li>
  <li><a href="#attention" id="toc-attention" class="nav-link" data-scroll-target="#attention">### Attention</a></li>
  <li><a href="#section-32" id="toc-section-32" class="nav-link" data-scroll-target="#section-32"></a></li>
  <li><a href="#an-attention-function-can-be-described-as-mapping-a-query-and-a-set" id="toc-an-attention-function-can-be-described-as-mapping-a-query-and-a-set" class="nav-link" data-scroll-target="#an-attention-function-can-be-described-as-mapping-a-query-and-a-set">An attention function can be described as mapping a query and a set</a></li>
  <li><a href="#of-key-value-pairs-to-an-output-where-the-query-keys-values-and" id="toc-of-key-value-pairs-to-an-output-where-the-query-keys-values-and" class="nav-link" data-scroll-target="#of-key-value-pairs-to-an-output-where-the-query-keys-values-and">of key-value pairs to an output, where the query, keys, values, and</a></li>
  <li><a href="#output-are-all-vectors.-the-output-is-computed-as-a-weighted-sum-of" id="toc-output-are-all-vectors.-the-output-is-computed-as-a-weighted-sum-of" class="nav-link" data-scroll-target="#output-are-all-vectors.-the-output-is-computed-as-a-weighted-sum-of">output are all vectors. The output is computed as a weighted sum of</a></li>
  <li><a href="#the-values-where-the-weight-assigned-to-each-value-is-computed-by-a" id="toc-the-values-where-the-weight-assigned-to-each-value-is-computed-by-a" class="nav-link" data-scroll-target="#the-values-where-the-weight-assigned-to-each-value-is-computed-by-a">the values, where the weight assigned to each value is computed by a</a></li>
  <li><a href="#compatibility-function-of-the-query-with-the-corresponding-key." id="toc-compatibility-function-of-the-query-with-the-corresponding-key." class="nav-link" data-scroll-target="#compatibility-function-of-the-query-with-the-corresponding-key.">compatibility function of the query with the corresponding key.</a></li>
  <li><a href="#section-33" id="toc-section-33" class="nav-link" data-scroll-target="#section-33"></a></li>
  <li><a href="#we-call-our-particular-attention-scaled-dot-product-attention." id="toc-we-call-our-particular-attention-scaled-dot-product-attention." class="nav-link" data-scroll-target="#we-call-our-particular-attention-scaled-dot-product-attention.">We call our particular attention “Scaled Dot-Product Attention”.</a></li>
  <li><a href="#the-input-consists-of-queries-and-keys-of-dimension-d_k-and" id="toc-the-input-consists-of-queries-and-keys-of-dimension-d_k-and" class="nav-link" data-scroll-target="#the-input-consists-of-queries-and-keys-of-dimension-d_k-and">The input consists of queries and keys of dimension <span class="math inline">\(d_k\)</span>, and</a></li>
  <li><a href="#values-of-dimension-d_v.-we-compute-the-dot-products-of-the-query" id="toc-values-of-dimension-d_v.-we-compute-the-dot-products-of-the-query" class="nav-link" data-scroll-target="#values-of-dimension-d_v.-we-compute-the-dot-products-of-the-query">values of dimension <span class="math inline">\(d_v\)</span>. We compute the dot products of the query</a></li>
  <li><a href="#with-all-keys-divide-each-by-sqrtd_k-and-apply-a-softmax" id="toc-with-all-keys-divide-each-by-sqrtd_k-and-apply-a-softmax" class="nav-link" data-scroll-target="#with-all-keys-divide-each-by-sqrtd_k-and-apply-a-softmax">with all keys, divide each by <span class="math inline">\(\sqrt{d_k}\)</span>, and apply a softmax</a></li>
  <li><a href="#function-to-obtain-the-weights-on-the-values." id="toc-function-to-obtain-the-weights-on-the-values." class="nav-link" data-scroll-target="#function-to-obtain-the-weights-on-the-values.">function to obtain the weights on the values.</a></li>
  <li><a href="#section-34" id="toc-section-34" class="nav-link" data-scroll-target="#section-34"></a></li>
  <li><a href="#section-35" id="toc-section-35" class="nav-link" data-scroll-target="#section-35"></a></li>
  <li><a href="#section-36" id="toc-section-36" class="nav-link" data-scroll-target="#section-36"></a></li>
  <li><a href="#section-37" id="toc-section-37" class="nav-link" data-scroll-target="#section-37"><img src="images/ModalNet-19.png" class="img-fluid"></a></li>
  <li><a href="#markdown-ideyjlwk6ctsqg" id="toc-markdown-ideyjlwk6ctsqg" class="nav-link" data-scroll-target="#markdown-ideyjlwk6ctsqg">%% [markdown] id=“EYJLWk6cTsqG”</a></li>
  <li><a href="#section-38" id="toc-section-38" class="nav-link" data-scroll-target="#section-38"></a></li>
  <li><a href="#in-practice-we-compute-the-attention-function-on-a-set-of-queries" id="toc-in-practice-we-compute-the-attention-function-on-a-set-of-queries" class="nav-link" data-scroll-target="#in-practice-we-compute-the-attention-function-on-a-set-of-queries">In practice, we compute the attention function on a set of queries</a></li>
  <li><a href="#simultaneously-packed-together-into-a-matrix-q.-the-keys-and" id="toc-simultaneously-packed-together-into-a-matrix-q.-the-keys-and" class="nav-link" data-scroll-target="#simultaneously-packed-together-into-a-matrix-q.-the-keys-and">simultaneously, packed together into a matrix <span class="math inline">\(Q\)</span>. The keys and</a></li>
  <li><a href="#values-are-also-packed-together-into-matrices-k-and-v.-we" id="toc-values-are-also-packed-together-into-matrices-k-and-v.-we" class="nav-link" data-scroll-target="#values-are-also-packed-together-into-matrices-k-and-v.-we">values are also packed together into matrices <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span>. We</a></li>
  <li><a href="#compute-the-matrix-of-outputs-as" id="toc-compute-the-matrix-of-outputs-as" class="nav-link" data-scroll-target="#compute-the-matrix-of-outputs-as">compute the matrix of outputs as:</a></li>
  <li><a href="#section-39" id="toc-section-39" class="nav-link" data-scroll-target="#section-39"></a></li>
  <li><a href="#mathrmattentionq-k-v-mathrmsoftmaxfracqktsqrtd_kv" id="toc-mathrmattentionq-k-v-mathrmsoftmaxfracqktsqrtd_kv" class="nav-link" data-scroll-target="#mathrmattentionq-k-v-mathrmsoftmaxfracqktsqrtd_kv"><span class="math display">\[
  #    \mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
  # \]</span></a></li>
  <li><a href="#idqsovxs5ytsqg" id="toc-idqsovxs5ytsqg" class="nav-link" data-scroll-target="#idqsovxs5ytsqg">%% id=“qsoVxS5yTsqG”</a></li>
  <li><a href="#markdown-idjukpwu8ktsqg" id="toc-markdown-idjukpwu8ktsqg" class="nav-link" data-scroll-target="#markdown-idjukpwu8ktsqg">%% [markdown] id=“jUkpwu8kTsqG”</a></li>
  <li><a href="#section-40" id="toc-section-40" class="nav-link" data-scroll-target="#section-40"></a></li>
  <li><a href="#the-two-most-commonly-used-attention-functions-are-additive" id="toc-the-two-most-commonly-used-attention-functions-are-additive" class="nav-link" data-scroll-target="#the-two-most-commonly-used-attention-functions-are-additive">The two most commonly used attention functions are additive</a></li>
  <li><a href="#attention-cite-and-dot-product" id="toc-attention-cite-and-dot-product" class="nav-link" data-scroll-target="#attention-cite-and-dot-product">attention (cite), and dot-product</a></li>
  <li><a href="#multiplicative-attention.-dot-product-attention-is-identical-to" id="toc-multiplicative-attention.-dot-product-attention-is-identical-to" class="nav-link" data-scroll-target="#multiplicative-attention.-dot-product-attention-is-identical-to">(multiplicative) attention. Dot-product attention is identical to</a></li>
  <li><a href="#our-algorithm-except-for-the-scaling-factor-of" id="toc-our-algorithm-except-for-the-scaling-factor-of" class="nav-link" data-scroll-target="#our-algorithm-except-for-the-scaling-factor-of">our algorithm, except for the scaling factor of</a></li>
  <li><a href="#frac1sqrtd_k.-additive-attention-computes-the" id="toc-frac1sqrtd_k.-additive-attention-computes-the" class="nav-link" data-scroll-target="#frac1sqrtd_k.-additive-attention-computes-the"><span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>. Additive attention computes the</a></li>
  <li><a href="#compatibility-function-using-a-feed-forward-network-with-a-single" id="toc-compatibility-function-using-a-feed-forward-network-with-a-single" class="nav-link" data-scroll-target="#compatibility-function-using-a-feed-forward-network-with-a-single">compatibility function using a feed-forward network with a single</a></li>
  <li><a href="#hidden-layer.-while-the-two-are-similar-in-theoretical-complexity" id="toc-hidden-layer.-while-the-two-are-similar-in-theoretical-complexity" class="nav-link" data-scroll-target="#hidden-layer.-while-the-two-are-similar-in-theoretical-complexity">hidden layer. While the two are similar in theoretical complexity,</a></li>
  <li><a href="#dot-product-attention-is-much-faster-and-more-space-efficient-in" id="toc-dot-product-attention-is-much-faster-and-more-space-efficient-in" class="nav-link" data-scroll-target="#dot-product-attention-is-much-faster-and-more-space-efficient-in">dot-product attention is much faster and more space-efficient in</a></li>
  <li><a href="#practice-since-it-can-be-implemented-using-highly-optimized-matrix" id="toc-practice-since-it-can-be-implemented-using-highly-optimized-matrix" class="nav-link" data-scroll-target="#practice-since-it-can-be-implemented-using-highly-optimized-matrix">practice, since it can be implemented using highly optimized matrix</a></li>
  <li><a href="#multiplication-code." id="toc-multiplication-code." class="nav-link" data-scroll-target="#multiplication-code.">multiplication code.</a></li>
  <li><a href="#section-41" id="toc-section-41" class="nav-link" data-scroll-target="#section-41"></a></li>
  <li><a href="#section-42" id="toc-section-42" class="nav-link" data-scroll-target="#section-42"></a></li>
  <li><a href="#while-for-small-values-of-d_k-the-two-mechanisms-perform" id="toc-while-for-small-values-of-d_k-the-two-mechanisms-perform" class="nav-link" data-scroll-target="#while-for-small-values-of-d_k-the-two-mechanisms-perform">While for small values of <span class="math inline">\(d_k\)</span> the two mechanisms perform</a></li>
  <li><a href="#similarly-additive-attention-outperforms-dot-product-attention" id="toc-similarly-additive-attention-outperforms-dot-product-attention" class="nav-link" data-scroll-target="#similarly-additive-attention-outperforms-dot-product-attention">similarly, additive attention outperforms dot product attention</a></li>
  <li><a href="#without-scaling-for-larger-values-of-d_k" id="toc-without-scaling-for-larger-values-of-d_k" class="nav-link" data-scroll-target="#without-scaling-for-larger-values-of-d_k">without scaling for larger values of <span class="math inline">\(d_k\)</span></a></li>
  <li><a href="#cite.-we-suspect-that-for" id="toc-cite.-we-suspect-that-for" class="nav-link" data-scroll-target="#cite.-we-suspect-that-for">(cite). We suspect that for</a></li>
  <li><a href="#large-values-of-d_k-the-dot-products-grow-large-in-magnitude" id="toc-large-values-of-d_k-the-dot-products-grow-large-in-magnitude" class="nav-link" data-scroll-target="#large-values-of-d_k-the-dot-products-grow-large-in-magnitude">large values of <span class="math inline">\(d_k\)</span>, the dot products grow large in magnitude,</a></li>
  <li><a href="#pushing-the-softmax-function-into-regions-where-it-has-extremely" id="toc-pushing-the-softmax-function-into-regions-where-it-has-extremely" class="nav-link" data-scroll-target="#pushing-the-softmax-function-into-regions-where-it-has-extremely">pushing the softmax function into regions where it has extremely</a></li>
  <li><a href="#small-gradients-to-illustrate-why-the-dot-products-get-large" id="toc-small-gradients-to-illustrate-why-the-dot-products-get-large" class="nav-link" data-scroll-target="#small-gradients-to-illustrate-why-the-dot-products-get-large">small gradients (To illustrate why the dot products get large,</a></li>
  <li><a href="#assume-that-the-components-of-q-and-k-are-independent-random" id="toc-assume-that-the-components-of-q-and-k-are-independent-random" class="nav-link" data-scroll-target="#assume-that-the-components-of-q-and-k-are-independent-random">assume that the components of <span class="math inline">\(q\)</span> and <span class="math inline">\(k\)</span> are independent random</a></li>
  <li><a href="#variables-with-mean-0-and-variance-1.-then-their-dot-product" id="toc-variables-with-mean-0-and-variance-1.-then-their-dot-product" class="nav-link" data-scroll-target="#variables-with-mean-0-and-variance-1.-then-their-dot-product">variables with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>. Then their dot product,</a></li>
  <li><a href="#q-cdot-k-sum_i1d_k-q_ik_i-has-mean-0-and-variance" id="toc-q-cdot-k-sum_i1d_k-q_ik_i-has-mean-0-and-variance" class="nav-link" data-scroll-target="#q-cdot-k-sum_i1d_k-q_ik_i-has-mean-0-and-variance"><span class="math inline">\(q \cdot k = \sum_{i=1}^{d_k} q_ik_i\)</span>, has mean <span class="math inline">\(0\)</span> and variance</a></li>
  <li><a href="#d_k..-to-counteract-this-effect-we-scale-the-dot-products-by" id="toc-d_k..-to-counteract-this-effect-we-scale-the-dot-products-by" class="nav-link" data-scroll-target="#d_k..-to-counteract-this-effect-we-scale-the-dot-products-by"><span class="math inline">\(d_k\)</span>.). To counteract this effect, we scale the dot products by</a></li>
  <li><a href="#frac1sqrtd_k." id="toc-frac1sqrtd_k." class="nav-link" data-scroll-target="#frac1sqrtd_k."><span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>.</a></li>
  <li><a href="#section-43" id="toc-section-43" class="nav-link" data-scroll-target="#section-43"></a></li>
  <li><a href="#section-44" id="toc-section-44" class="nav-link" data-scroll-target="#section-44"></a></li>
  <li><a href="#markdown-idbs1fszhvtsqg" id="toc-markdown-idbs1fszhvtsqg" class="nav-link" data-scroll-target="#markdown-idbs1fszhvtsqg">%% [markdown] id=“bS1FszhVTsqG”</a></li>
  <li><a href="#section-45" id="toc-section-45" class="nav-link" data-scroll-target="#section-45"><img src="images/ModalNet-20.png" class="img-fluid"></a></li>
  <li><a href="#markdown-idtntvyz-ptsqh" id="toc-markdown-idtntvyz-ptsqh" class="nav-link" data-scroll-target="#markdown-idtntvyz-ptsqh">%% [markdown] id=“TNtVyZ-pTsqH”</a></li>
  <li><a href="#section-46" id="toc-section-46" class="nav-link" data-scroll-target="#section-46"></a></li>
  <li><a href="#multi-head-attention-allows-the-model-to-jointly-attend-to" id="toc-multi-head-attention-allows-the-model-to-jointly-attend-to" class="nav-link" data-scroll-target="#multi-head-attention-allows-the-model-to-jointly-attend-to">Multi-head attention allows the model to jointly attend to</a></li>
  <li><a href="#information-from-different-representation-subspaces-at-different" id="toc-information-from-different-representation-subspaces-at-different" class="nav-link" data-scroll-target="#information-from-different-representation-subspaces-at-different">information from different representation subspaces at different</a></li>
  <li><a href="#positions.-with-a-single-attention-head-averaging-inhibits-this." id="toc-positions.-with-a-single-attention-head-averaging-inhibits-this." class="nav-link" data-scroll-target="#positions.-with-a-single-attention-head-averaging-inhibits-this.">positions. With a single attention head, averaging inhibits this.</a></li>
  <li><a href="#section-47" id="toc-section-47" class="nav-link" data-scroll-target="#section-47"></a></li>
  <li><a href="#mathrmmultiheadq-k-v-mathrmconcatmathrmhead_1-...-mathrmhead_hwo-textwheremathrmhead_i-mathrmattentionqwq_i-kwk_i-vwv_i" id="toc-mathrmmultiheadq-k-v-mathrmconcatmathrmhead_1-...-mathrmhead_hwo-textwheremathrmhead_i-mathrmattentionqwq_i-kwk_i-vwv_i" class="nav-link" data-scroll-target="#mathrmmultiheadq-k-v-mathrmconcatmathrmhead_1-...-mathrmhead_hwo-textwheremathrmhead_i-mathrmattentionqwq_i-kwk_i-vwv_i"><span class="math display">\[
  # \mathrm{MultiHead}(Q, K, V) =
  #     \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \\
  #     \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)
  # \]</span></a></li>
  <li><a href="#section-48" id="toc-section-48" class="nav-link" data-scroll-target="#section-48"></a></li>
  <li><a href="#where-the-projections-are-parameter-matrices-wq_i-in-mathbbrd_textmodel-times-d_k-wk_i-in-mathbbrd_textmodel-times-d_k-wv_i-in-mathbbrd_textmodel-times-d_v-and-wo-in-mathbbrhd_v-times-d_textmodel." id="toc-where-the-projections-are-parameter-matrices-wq_i-in-mathbbrd_textmodel-times-d_k-wk_i-in-mathbbrd_textmodel-times-d_k-wv_i-in-mathbbrd_textmodel-times-d_v-and-wo-in-mathbbrhd_v-times-d_textmodel." class="nav-link" data-scroll-target="#where-the-projections-are-parameter-matrices-wq_i-in-mathbbrd_textmodel-times-d_k-wk_i-in-mathbbrd_textmodel-times-d_k-wv_i-in-mathbbrd_textmodel-times-d_v-and-wo-in-mathbbrhd_v-times-d_textmodel.">Where the projections are parameter matrices <span class="math inline">\(W^Q_i \in
  # \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>, <span class="math inline">\(W^K_i \in
  # \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>, <span class="math inline">\(W^V_i \in
  # \mathbb{R}^{d_{\text{model}} \times d_v}\)</span> and <span class="math inline">\(W^O \in
  # \mathbb{R}^{hd_v \times d_{\text{model}}}\)</span>.</a></li>
  <li><a href="#section-49" id="toc-section-49" class="nav-link" data-scroll-target="#section-49"></a></li>
  <li><a href="#in-this-work-we-employ-h8-parallel-attention-layers-or" id="toc-in-this-work-we-employ-h8-parallel-attention-layers-or" class="nav-link" data-scroll-target="#in-this-work-we-employ-h8-parallel-attention-layers-or">In this work we employ <span class="math inline">\(h=8\)</span> parallel attention layers, or</a></li>
  <li><a href="#heads.-for-each-of-these-we-use-d_kd_vd_textmodelh64.-due" id="toc-heads.-for-each-of-these-we-use-d_kd_vd_textmodelh64.-due" class="nav-link" data-scroll-target="#heads.-for-each-of-these-we-use-d_kd_vd_textmodelh64.-due">heads. For each of these we use <span class="math inline">\(d_k=d_v=d_{\text{model}}/h=64\)</span>. Due</a></li>
  <li><a href="#to-the-reduced-dimension-of-each-head-the-total-computational-cost" id="toc-to-the-reduced-dimension-of-each-head-the-total-computational-cost" class="nav-link" data-scroll-target="#to-the-reduced-dimension-of-each-head-the-total-computational-cost">to the reduced dimension of each head, the total computational cost</a></li>
  <li><a href="#is-similar-to-that-of-single-head-attention-with-full" id="toc-is-similar-to-that-of-single-head-attention-with-full" class="nav-link" data-scroll-target="#is-similar-to-that-of-single-head-attention-with-full">is similar to that of single-head attention with full</a></li>
  <li><a href="#dimensionality." id="toc-dimensionality." class="nav-link" data-scroll-target="#dimensionality.">dimensionality.</a></li>
  <li><a href="#idd2lbmkcqtsqh" id="toc-idd2lbmkcqtsqh" class="nav-link" data-scroll-target="#idd2lbmkcqtsqh">%% id=“D2LBMKCQTsqH”</a></li>
  <li><a href="#markdown-idedrba3j3tsqh" id="toc-markdown-idedrba3j3tsqh" class="nav-link" data-scroll-target="#markdown-idedrba3j3tsqh">%% [markdown] id=“EDRba3J3TsqH”</a></li>
  <li><a href="#applications-of-attention-in-our-model" id="toc-applications-of-attention-in-our-model" class="nav-link" data-scroll-target="#applications-of-attention-in-our-model">### Applications of Attention in our Model</a></li>
  <li><a href="#section-50" id="toc-section-50" class="nav-link" data-scroll-target="#section-50"></a></li>
  <li><a href="#the-transformer-uses-multi-head-attention-in-three-different-ways" id="toc-the-transformer-uses-multi-head-attention-in-three-different-ways" class="nav-link" data-scroll-target="#the-transformer-uses-multi-head-attention-in-three-different-ways">The Transformer uses multi-head attention in three different ways:</a></li>
  <li><a href="#in-encoder-decoder-attention-layers-the-queries-come-from-the" id="toc-in-encoder-decoder-attention-layers-the-queries-come-from-the" class="nav-link" data-scroll-target="#in-encoder-decoder-attention-layers-the-queries-come-from-the">1) In “encoder-decoder attention” layers, the queries come from the</a></li>
  <li><a href="#previous-decoder-layer-and-the-memory-keys-and-values-come-from-the" id="toc-previous-decoder-layer-and-the-memory-keys-and-values-come-from-the" class="nav-link" data-scroll-target="#previous-decoder-layer-and-the-memory-keys-and-values-come-from-the">previous decoder layer, and the memory keys and values come from the</a></li>
  <li><a href="#output-of-the-encoder.-this-allows-every-position-in-the-decoder-to" id="toc-output-of-the-encoder.-this-allows-every-position-in-the-decoder-to" class="nav-link" data-scroll-target="#output-of-the-encoder.-this-allows-every-position-in-the-decoder-to">output of the encoder. This allows every position in the decoder to</a></li>
  <li><a href="#attend-over-all-positions-in-the-input-sequence.-this-mimics-the" id="toc-attend-over-all-positions-in-the-input-sequence.-this-mimics-the" class="nav-link" data-scroll-target="#attend-over-all-positions-in-the-input-sequence.-this-mimics-the">attend over all positions in the input sequence. This mimics the</a></li>
  <li><a href="#typical-encoder-decoder-attention-mechanisms-in-sequence-to-sequence" id="toc-typical-encoder-decoder-attention-mechanisms-in-sequence-to-sequence" class="nav-link" data-scroll-target="#typical-encoder-decoder-attention-mechanisms-in-sequence-to-sequence">typical encoder-decoder attention mechanisms in sequence-to-sequence</a></li>
  <li><a href="#models-such-as-cite." id="toc-models-such-as-cite." class="nav-link" data-scroll-target="#models-such-as-cite.">models such as (cite).</a></li>
  <li><a href="#section-51" id="toc-section-51" class="nav-link" data-scroll-target="#section-51"></a></li>
  <li><a href="#section-52" id="toc-section-52" class="nav-link" data-scroll-target="#section-52"></a></li>
  <li><a href="#the-encoder-contains-self-attention-layers.-in-a-self-attention" id="toc-the-encoder-contains-self-attention-layers.-in-a-self-attention" class="nav-link" data-scroll-target="#the-encoder-contains-self-attention-layers.-in-a-self-attention">2) The encoder contains self-attention layers. In a self-attention</a></li>
  <li><a href="#layer-all-of-the-keys-values-and-queries-come-from-the-same-place" id="toc-layer-all-of-the-keys-values-and-queries-come-from-the-same-place" class="nav-link" data-scroll-target="#layer-all-of-the-keys-values-and-queries-come-from-the-same-place">layer all of the keys, values and queries come from the same place,</a></li>
  <li><a href="#in-this-case-the-output-of-the-previous-layer-in-the-encoder.-each" id="toc-in-this-case-the-output-of-the-previous-layer-in-the-encoder.-each" class="nav-link" data-scroll-target="#in-this-case-the-output-of-the-previous-layer-in-the-encoder.-each">in this case, the output of the previous layer in the encoder. Each</a></li>
  <li><a href="#position-in-the-encoder-can-attend-to-all-positions-in-the-previous" id="toc-position-in-the-encoder-can-attend-to-all-positions-in-the-previous" class="nav-link" data-scroll-target="#position-in-the-encoder-can-attend-to-all-positions-in-the-previous">position in the encoder can attend to all positions in the previous</a></li>
  <li><a href="#layer-of-the-encoder." id="toc-layer-of-the-encoder." class="nav-link" data-scroll-target="#layer-of-the-encoder.">layer of the encoder.</a></li>
  <li><a href="#section-53" id="toc-section-53" class="nav-link" data-scroll-target="#section-53"></a></li>
  <li><a href="#section-54" id="toc-section-54" class="nav-link" data-scroll-target="#section-54"></a></li>
  <li><a href="#similarly-self-attention-layers-in-the-decoder-allow-each" id="toc-similarly-self-attention-layers-in-the-decoder-allow-each" class="nav-link" data-scroll-target="#similarly-self-attention-layers-in-the-decoder-allow-each">3) Similarly, self-attention layers in the decoder allow each</a></li>
  <li><a href="#position-in-the-decoder-to-attend-to-all-positions-in-the-decoder-up" id="toc-position-in-the-decoder-to-attend-to-all-positions-in-the-decoder-up" class="nav-link" data-scroll-target="#position-in-the-decoder-to-attend-to-all-positions-in-the-decoder-up">position in the decoder to attend to all positions in the decoder up</a></li>
  <li><a href="#to-and-including-that-position.-we-need-to-prevent-leftward" id="toc-to-and-including-that-position.-we-need-to-prevent-leftward" class="nav-link" data-scroll-target="#to-and-including-that-position.-we-need-to-prevent-leftward">to and including that position. We need to prevent leftward</a></li>
  <li><a href="#information-flow-in-the-decoder-to-preserve-the-auto-regressive" id="toc-information-flow-in-the-decoder-to-preserve-the-auto-regressive" class="nav-link" data-scroll-target="#information-flow-in-the-decoder-to-preserve-the-auto-regressive">information flow in the decoder to preserve the auto-regressive</a></li>
  <li><a href="#property.-we-implement-this-inside-of-scaled-dot-product-attention" id="toc-property.-we-implement-this-inside-of-scaled-dot-product-attention" class="nav-link" data-scroll-target="#property.-we-implement-this-inside-of-scaled-dot-product-attention">property. We implement this inside of scaled dot-product attention</a></li>
  <li><a href="#by-masking-out-setting-to--infty-all-values-in-the-input-of-the" id="toc-by-masking-out-setting-to--infty-all-values-in-the-input-of-the" class="nav-link" data-scroll-target="#by-masking-out-setting-to--infty-all-values-in-the-input-of-the">by masking out (setting to <span class="math inline">\(-\infty\)</span>) all values in the input of the</a></li>
  <li><a href="#softmax-which-correspond-to-illegal-connections." id="toc-softmax-which-correspond-to-illegal-connections." class="nav-link" data-scroll-target="#softmax-which-correspond-to-illegal-connections.">softmax which correspond to illegal connections.</a></li>
  <li><a href="#markdown-idm-en97_gtsqh" id="toc-markdown-idm-en97_gtsqh" class="nav-link" data-scroll-target="#markdown-idm-en97_gtsqh">%% [markdown] id=“M-en97_GTsqH”</a></li>
  <li><a href="#position-wise-feed-forward-networks" id="toc-position-wise-feed-forward-networks" class="nav-link" data-scroll-target="#position-wise-feed-forward-networks">## Position-wise Feed-Forward Networks</a></li>
  <li><a href="#section-55" id="toc-section-55" class="nav-link" data-scroll-target="#section-55"></a></li>
  <li><a href="#in-addition-to-attention-sub-layers-each-of-the-layers-in-our" id="toc-in-addition-to-attention-sub-layers-each-of-the-layers-in-our" class="nav-link" data-scroll-target="#in-addition-to-attention-sub-layers-each-of-the-layers-in-our">In addition to attention sub-layers, each of the layers in our</a></li>
  <li><a href="#encoder-and-decoder-contains-a-fully-connected-feed-forward-network" id="toc-encoder-and-decoder-contains-a-fully-connected-feed-forward-network" class="nav-link" data-scroll-target="#encoder-and-decoder-contains-a-fully-connected-feed-forward-network">encoder and decoder contains a fully connected feed-forward network,</a></li>
  <li><a href="#which-is-applied-to-each-position-separately-and-identically.-this" id="toc-which-is-applied-to-each-position-separately-and-identically.-this" class="nav-link" data-scroll-target="#which-is-applied-to-each-position-separately-and-identically.-this">which is applied to each position separately and identically. This</a></li>
  <li><a href="#consists-of-two-linear-transformations-with-a-relu-activation-in" id="toc-consists-of-two-linear-transformations-with-a-relu-activation-in" class="nav-link" data-scroll-target="#consists-of-two-linear-transformations-with-a-relu-activation-in">consists of two linear transformations with a ReLU activation in</a></li>
  <li><a href="#between." id="toc-between." class="nav-link" data-scroll-target="#between.">between.</a></li>
  <li><a href="#section-56" id="toc-section-56" class="nav-link" data-scroll-target="#section-56"></a></li>
  <li><a href="#mathrmffnxmax0-xw_1-b_1-w_2-b_2" id="toc-mathrmffnxmax0-xw_1-b_1-w_2-b_2" class="nav-link" data-scroll-target="#mathrmffnxmax0-xw_1-b_1-w_2-b_2"><span class="math display">\[\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2\]</span></a></li>
  <li><a href="#section-57" id="toc-section-57" class="nav-link" data-scroll-target="#section-57"></a></li>
  <li><a href="#while-the-linear-transformations-are-the-same-across-different" id="toc-while-the-linear-transformations-are-the-same-across-different" class="nav-link" data-scroll-target="#while-the-linear-transformations-are-the-same-across-different">While the linear transformations are the same across different</a></li>
  <li><a href="#positions-they-use-different-parameters-from-layer-to" id="toc-positions-they-use-different-parameters-from-layer-to" class="nav-link" data-scroll-target="#positions-they-use-different-parameters-from-layer-to">positions, they use different parameters from layer to</a></li>
  <li><a href="#layer.-another-way-of-describing-this-is-as-two-convolutions-with" id="toc-layer.-another-way-of-describing-this-is-as-two-convolutions-with" class="nav-link" data-scroll-target="#layer.-another-way-of-describing-this-is-as-two-convolutions-with">layer. Another way of describing this is as two convolutions with</a></li>
  <li><a href="#kernel-size-1.-the-dimensionality-of-input-and-output-is" id="toc-kernel-size-1.-the-dimensionality-of-input-and-output-is" class="nav-link" data-scroll-target="#kernel-size-1.-the-dimensionality-of-input-and-output-is">kernel size 1. The dimensionality of input and output is</a></li>
  <li><a href="#d_textmodel512-and-the-inner-layer-has-dimensionality" id="toc-d_textmodel512-and-the-inner-layer-has-dimensionality" class="nav-link" data-scroll-target="#d_textmodel512-and-the-inner-layer-has-dimensionality"><span class="math inline">\(d_{\text{model}}=512\)</span>, and the inner-layer has dimensionality</a></li>
  <li><a href="#d_ff2048." id="toc-d_ff2048." class="nav-link" data-scroll-target="#d_ff2048."><span class="math inline">\(d_{ff}=2048\)</span>.</a></li>
  <li><a href="#id6hhcemcxtsqh" id="toc-id6hhcemcxtsqh" class="nav-link" data-scroll-target="#id6hhcemcxtsqh">%% id=“6HHCemCxTsqH”</a></li>
  <li><a href="#markdown-iddr1ym520tsqh" id="toc-markdown-iddr1ym520tsqh" class="nav-link" data-scroll-target="#markdown-iddr1ym520tsqh">%% [markdown] id=“dR1YM520TsqH”</a></li>
  <li><a href="#embeddings-and-softmax" id="toc-embeddings-and-softmax" class="nav-link" data-scroll-target="#embeddings-and-softmax">## Embeddings and Softmax</a></li>
  <li><a href="#section-58" id="toc-section-58" class="nav-link" data-scroll-target="#section-58"></a></li>
  <li><a href="#similarly-to-other-sequence-transduction-models-we-use-learned" id="toc-similarly-to-other-sequence-transduction-models-we-use-learned" class="nav-link" data-scroll-target="#similarly-to-other-sequence-transduction-models-we-use-learned">Similarly to other sequence transduction models, we use learned</a></li>
  <li><a href="#embeddings-to-convert-the-input-tokens-and-output-tokens-to-vectors" id="toc-embeddings-to-convert-the-input-tokens-and-output-tokens-to-vectors" class="nav-link" data-scroll-target="#embeddings-to-convert-the-input-tokens-and-output-tokens-to-vectors">embeddings to convert the input tokens and output tokens to vectors</a></li>
  <li><a href="#of-dimension-d_textmodel.-we-also-use-the-usual-learned" id="toc-of-dimension-d_textmodel.-we-also-use-the-usual-learned" class="nav-link" data-scroll-target="#of-dimension-d_textmodel.-we-also-use-the-usual-learned">of dimension <span class="math inline">\(d_{\text{model}}\)</span>. We also use the usual learned</a></li>
  <li><a href="#linear-transformation-and-softmax-function-to-convert-the-decoder" id="toc-linear-transformation-and-softmax-function-to-convert-the-decoder" class="nav-link" data-scroll-target="#linear-transformation-and-softmax-function-to-convert-the-decoder">linear transformation and softmax function to convert the decoder</a></li>
  <li><a href="#output-to-predicted-next-token-probabilities.-in-our-model-we" id="toc-output-to-predicted-next-token-probabilities.-in-our-model-we" class="nav-link" data-scroll-target="#output-to-predicted-next-token-probabilities.-in-our-model-we">output to predicted next-token probabilities. In our model, we</a></li>
  <li><a href="#share-the-same-weight-matrix-between-the-two-embedding-layers-and" id="toc-share-the-same-weight-matrix-between-the-two-embedding-layers-and" class="nav-link" data-scroll-target="#share-the-same-weight-matrix-between-the-two-embedding-layers-and">share the same weight matrix between the two embedding layers and</a></li>
  <li><a href="#the-pre-softmax-linear-transformation-similar-to" id="toc-the-pre-softmax-linear-transformation-similar-to" class="nav-link" data-scroll-target="#the-pre-softmax-linear-transformation-similar-to">the pre-softmax linear transformation, similar to</a></li>
  <li><a href="#cite.-in-the-embedding-layers" id="toc-cite.-in-the-embedding-layers" class="nav-link" data-scroll-target="#cite.-in-the-embedding-layers">(cite). In the embedding layers,</a></li>
  <li><a href="#we-multiply-those-weights-by-sqrtd_textmodel." id="toc-we-multiply-those-weights-by-sqrtd_textmodel." class="nav-link" data-scroll-target="#we-multiply-those-weights-by-sqrtd_textmodel.">we multiply those weights by <span class="math inline">\(\sqrt{d_{\text{model}}}\)</span>.</a></li>
  <li><a href="#idpyrchq9qtsqh" id="toc-idpyrchq9qtsqh" class="nav-link" data-scroll-target="#idpyrchq9qtsqh">%% id=“pyrChq9qTsqH”</a></li>
  <li><a href="#markdown-idvokdui-ctsqh" id="toc-markdown-idvokdui-ctsqh" class="nav-link" data-scroll-target="#markdown-idvokdui-ctsqh">%% [markdown] id=“vOkdui-cTsqH”</a></li>
  <li><a href="#positional-encoding" id="toc-positional-encoding" class="nav-link" data-scroll-target="#positional-encoding">## Positional Encoding</a></li>
  <li><a href="#section-59" id="toc-section-59" class="nav-link" data-scroll-target="#section-59"></a></li>
  <li><a href="#since-our-model-contains-no-recurrence-and-no-convolution-in-order" id="toc-since-our-model-contains-no-recurrence-and-no-convolution-in-order" class="nav-link" data-scroll-target="#since-our-model-contains-no-recurrence-and-no-convolution-in-order">Since our model contains no recurrence and no convolution, in order</a></li>
  <li><a href="#for-the-model-to-make-use-of-the-order-of-the-sequence-we-must" id="toc-for-the-model-to-make-use-of-the-order-of-the-sequence-we-must" class="nav-link" data-scroll-target="#for-the-model-to-make-use-of-the-order-of-the-sequence-we-must">for the model to make use of the order of the sequence, we must</a></li>
  <li><a href="#inject-some-information-about-the-relative-or-absolute-position-of" id="toc-inject-some-information-about-the-relative-or-absolute-position-of" class="nav-link" data-scroll-target="#inject-some-information-about-the-relative-or-absolute-position-of">inject some information about the relative or absolute position of</a></li>
  <li><a href="#the-tokens-in-the-sequence.-to-this-end-we-add-positional" id="toc-the-tokens-in-the-sequence.-to-this-end-we-add-positional" class="nav-link" data-scroll-target="#the-tokens-in-the-sequence.-to-this-end-we-add-positional">the tokens in the sequence. To this end, we add “positional</a></li>
  <li><a href="#encodings-to-the-input-embeddings-at-the-bottoms-of-the-encoder-and" id="toc-encodings-to-the-input-embeddings-at-the-bottoms-of-the-encoder-and" class="nav-link" data-scroll-target="#encodings-to-the-input-embeddings-at-the-bottoms-of-the-encoder-and">encodings” to the input embeddings at the bottoms of the encoder and</a></li>
  <li><a href="#decoder-stacks.-the-positional-encodings-have-the-same-dimension" id="toc-decoder-stacks.-the-positional-encodings-have-the-same-dimension" class="nav-link" data-scroll-target="#decoder-stacks.-the-positional-encodings-have-the-same-dimension">decoder stacks. The positional encodings have the same dimension</a></li>
  <li><a href="#d_textmodel-as-the-embeddings-so-that-the-two-can-be-summed." id="toc-d_textmodel-as-the-embeddings-so-that-the-two-can-be-summed." class="nav-link" data-scroll-target="#d_textmodel-as-the-embeddings-so-that-the-two-can-be-summed."><span class="math inline">\(d_{\text{model}}\)</span> as the embeddings, so that the two can be summed.</a></li>
  <li><a href="#there-are-many-choices-of-positional-encodings-learned-and-fixed" id="toc-there-are-many-choices-of-positional-encodings-learned-and-fixed" class="nav-link" data-scroll-target="#there-are-many-choices-of-positional-encodings-learned-and-fixed">There are many choices of positional encodings, learned and fixed</a></li>
  <li><a href="#cite.-1" id="toc-cite.-1" class="nav-link" data-scroll-target="#cite.-1">(cite).</a></li>
  <li><a href="#section-60" id="toc-section-60" class="nav-link" data-scroll-target="#section-60"></a></li>
  <li><a href="#in-this-work-we-use-sine-and-cosine-functions-of-different-frequencies" id="toc-in-this-work-we-use-sine-and-cosine-functions-of-different-frequencies" class="nav-link" data-scroll-target="#in-this-work-we-use-sine-and-cosine-functions-of-different-frequencies">In this work, we use sine and cosine functions of different frequencies:</a></li>
  <li><a href="#section-61" id="toc-section-61" class="nav-link" data-scroll-target="#section-61"></a></li>
  <li><a href="#pe_pos2i-sinpos-100002id_textmodel" id="toc-pe_pos2i-sinpos-100002id_textmodel" class="nav-link" data-scroll-target="#pe_pos2i-sinpos-100002id_textmodel"><span class="math display">\[PE_{(pos,2i)} = \sin(pos / 10000^{2i/d_{\text{model}}})\]</span></a></li>
  <li><a href="#section-62" id="toc-section-62" class="nav-link" data-scroll-target="#section-62"></a></li>
  <li><a href="#pe_pos2i1-cospos-100002id_textmodel" id="toc-pe_pos2i1-cospos-100002id_textmodel" class="nav-link" data-scroll-target="#pe_pos2i1-cospos-100002id_textmodel"><span class="math display">\[PE_{(pos,2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}})\]</span></a></li>
  <li><a href="#section-63" id="toc-section-63" class="nav-link" data-scroll-target="#section-63"></a></li>
  <li><a href="#where-pos-is-the-position-and-i-is-the-dimension.-that-is-each" id="toc-where-pos-is-the-position-and-i-is-the-dimension.-that-is-each" class="nav-link" data-scroll-target="#where-pos-is-the-position-and-i-is-the-dimension.-that-is-each">where <span class="math inline">\(pos\)</span> is the position and <span class="math inline">\(i\)</span> is the dimension. That is, each</a></li>
  <li><a href="#dimension-of-the-positional-encoding-corresponds-to-a-sinusoid.-the" id="toc-dimension-of-the-positional-encoding-corresponds-to-a-sinusoid.-the" class="nav-link" data-scroll-target="#dimension-of-the-positional-encoding-corresponds-to-a-sinusoid.-the">dimension of the positional encoding corresponds to a sinusoid. The</a></li>
  <li><a href="#wavelengths-form-a-geometric-progression-from-2pi-to-10000-cdot-2pi.-we-chose-this-function-because-we-hypothesized-it-would" id="toc-wavelengths-form-a-geometric-progression-from-2pi-to-10000-cdot-2pi.-we-chose-this-function-because-we-hypothesized-it-would" class="nav-link" data-scroll-target="#wavelengths-form-a-geometric-progression-from-2pi-to-10000-cdot-2pi.-we-chose-this-function-because-we-hypothesized-it-would">wavelengths form a geometric progression from <span class="math inline">\(2\pi\)</span> to <span class="math inline">\(10000 \cdot
  # 2\pi\)</span>. We chose this function because we hypothesized it would</a></li>
  <li><a href="#allow-the-model-to-easily-learn-to-attend-by-relative-positions" id="toc-allow-the-model-to-easily-learn-to-attend-by-relative-positions" class="nav-link" data-scroll-target="#allow-the-model-to-easily-learn-to-attend-by-relative-positions">allow the model to easily learn to attend by relative positions,</a></li>
  <li><a href="#since-for-any-fixed-offset-k-pe_posk-can-be-represented-as-a" id="toc-since-for-any-fixed-offset-k-pe_posk-can-be-represented-as-a" class="nav-link" data-scroll-target="#since-for-any-fixed-offset-k-pe_posk-can-be-represented-as-a">since for any fixed offset <span class="math inline">\(k\)</span>, <span class="math inline">\(PE_{pos+k}\)</span> can be represented as a</a></li>
  <li><a href="#linear-function-of-pe_pos." id="toc-linear-function-of-pe_pos." class="nav-link" data-scroll-target="#linear-function-of-pe_pos.">linear function of <span class="math inline">\(PE_{pos}\)</span>.</a></li>
  <li><a href="#section-64" id="toc-section-64" class="nav-link" data-scroll-target="#section-64"></a></li>
  <li><a href="#in-addition-we-apply-dropout-to-the-sums-of-the-embeddings-and-the" id="toc-in-addition-we-apply-dropout-to-the-sums-of-the-embeddings-and-the" class="nav-link" data-scroll-target="#in-addition-we-apply-dropout-to-the-sums-of-the-embeddings-and-the">In addition, we apply dropout to the sums of the embeddings and the</a></li>
  <li><a href="#positional-encodings-in-both-the-encoder-and-decoder-stacks.-for" id="toc-positional-encodings-in-both-the-encoder-and-decoder-stacks.-for" class="nav-link" data-scroll-target="#positional-encodings-in-both-the-encoder-and-decoder-stacks.-for">positional encodings in both the encoder and decoder stacks. For</a></li>
  <li><a href="#the-base-model-we-use-a-rate-of-p_drop0.1." id="toc-the-base-model-we-use-a-rate-of-p_drop0.1." class="nav-link" data-scroll-target="#the-base-model-we-use-a-rate-of-p_drop0.1.">the base model, we use a rate of <span class="math inline">\(P_{drop}=0.1\)</span>.</a></li>
  <li><a href="#section-65" id="toc-section-65" class="nav-link" data-scroll-target="#section-65"></a></li>
  <li><a href="#section-66" id="toc-section-66" class="nav-link" data-scroll-target="#section-66"></a></li>
  <li><a href="#idzahgd4yjtsqh" id="toc-idzahgd4yjtsqh" class="nav-link" data-scroll-target="#idzahgd4yjtsqh">%% id=“zaHGD4yJTsqH”</a></li>
  <li><a href="#markdown-idefhactjltsqh" id="toc-markdown-idefhactjltsqh" class="nav-link" data-scroll-target="#markdown-idefhactjltsqh">%% [markdown] id=“EfHacTJLTsqH”</a></li>
  <li><a href="#section-67" id="toc-section-67" class="nav-link" data-scroll-target="#section-67"></a></li>
  <li><a href="#below-the-positional-encoding-will-add-in-a-sine-wave-based-on" id="toc-below-the-positional-encoding-will-add-in-a-sine-wave-based-on" class="nav-link" data-scroll-target="#below-the-positional-encoding-will-add-in-a-sine-wave-based-on">&gt; Below the positional encoding will add in a sine wave based on</a></li>
  <li><a href="#position.-the-frequency-and-offset-of-the-wave-is-different-for" id="toc-position.-the-frequency-and-offset-of-the-wave-is-different-for" class="nav-link" data-scroll-target="#position.-the-frequency-and-offset-of-the-wave-is-different-for">&gt; position. The frequency and offset of the wave is different for</a></li>
  <li><a href="#each-dimension." id="toc-each-dimension." class="nav-link" data-scroll-target="#each-dimension.">&gt; each dimension.</a></li>
  <li><a href="#idrnvhk_1qokc6-typeexample" id="toc-idrnvhk_1qokc6-typeexample" class="nav-link" data-scroll-target="#idrnvhk_1qokc6-typeexample">%% id=“rnvHk_1QokC6” type=“example”</a></li>
  <li><a href="#markdown-idg8rzncrztsqi" id="toc-markdown-idg8rzncrztsqi" class="nav-link" data-scroll-target="#markdown-idg8rzncrztsqi">%% [markdown] id=“g8rZNCrzTsqI”</a></li>
  <li><a href="#section-68" id="toc-section-68" class="nav-link" data-scroll-target="#section-68"></a></li>
  <li><a href="#we-also-experimented-with-using-learned-positional-embeddings" id="toc-we-also-experimented-with-using-learned-positional-embeddings" class="nav-link" data-scroll-target="#we-also-experimented-with-using-learned-positional-embeddings">We also experimented with using learned positional embeddings</a></li>
  <li><a href="#cite-instead-and-found" id="toc-cite-instead-and-found" class="nav-link" data-scroll-target="#cite-instead-and-found">(cite) instead, and found</a></li>
  <li><a href="#that-the-two-versions-produced-nearly-identical-results.-we-chose" id="toc-that-the-two-versions-produced-nearly-identical-results.-we-chose" class="nav-link" data-scroll-target="#that-the-two-versions-produced-nearly-identical-results.-we-chose">that the two versions produced nearly identical results. We chose</a></li>
  <li><a href="#the-sinusoidal-version-because-it-may-allow-the-model-to-extrapolate" id="toc-the-sinusoidal-version-because-it-may-allow-the-model-to-extrapolate" class="nav-link" data-scroll-target="#the-sinusoidal-version-because-it-may-allow-the-model-to-extrapolate">the sinusoidal version because it may allow the model to extrapolate</a></li>
  <li><a href="#to-sequence-lengths-longer-than-the-ones-encountered-during" id="toc-to-sequence-lengths-longer-than-the-ones-encountered-during" class="nav-link" data-scroll-target="#to-sequence-lengths-longer-than-the-ones-encountered-during">to sequence lengths longer than the ones encountered during</a></li>
  <li><a href="#training." id="toc-training." class="nav-link" data-scroll-target="#training.">training.</a></li>
  <li><a href="#markdown-idiwnkczlytsqi" id="toc-markdown-idiwnkczlytsqi" class="nav-link" data-scroll-target="#markdown-idiwnkczlytsqi">%% [markdown] id=“iwNKCzlyTsqI”</a></li>
  <li><a href="#full-model" id="toc-full-model" class="nav-link" data-scroll-target="#full-model">## Full Model</a></li>
  <li><a href="#section-69" id="toc-section-69" class="nav-link" data-scroll-target="#section-69"></a></li>
  <li><a href="#here-we-define-a-function-from-hyperparameters-to-a-full-model." id="toc-here-we-define-a-function-from-hyperparameters-to-a-full-model." class="nav-link" data-scroll-target="#here-we-define-a-function-from-hyperparameters-to-a-full-model.">&gt; Here we define a function from hyperparameters to a full model.</a></li>
  <li><a href="#idmpe1es0utsqi" id="toc-idmpe1es0utsqi" class="nav-link" data-scroll-target="#idmpe1es0utsqi">%% id=“mPe1ES0UTsqI”</a></li>
  <li><a href="#markdown-1" id="toc-markdown-1" class="nav-link" data-scroll-target="#markdown-1">%% [markdown]</a></li>
  <li><a href="#inference" id="toc-inference" class="nav-link" data-scroll-target="#inference">## Inference:</a></li>
  <li><a href="#section-70" id="toc-section-70" class="nav-link" data-scroll-target="#section-70"></a></li>
  <li><a href="#here-we-make-a-forward-step-to-generate-a-prediction-of-the" id="toc-here-we-make-a-forward-step-to-generate-a-prediction-of-the" class="nav-link" data-scroll-target="#here-we-make-a-forward-step-to-generate-a-prediction-of-the">&gt; Here we make a forward step to generate a prediction of the</a></li>
  <li><a href="#model.-we-try-to-use-our-transformer-to-memorize-the-input.-as-you" id="toc-model.-we-try-to-use-our-transformer-to-memorize-the-input.-as-you" class="nav-link" data-scroll-target="#model.-we-try-to-use-our-transformer-to-memorize-the-input.-as-you">model. We try to use our transformer to memorize the input. As you</a></li>
  <li><a href="#will-see-the-output-is-randomly-generated-due-to-the-fact-that-the" id="toc-will-see-the-output-is-randomly-generated-due-to-the-fact-that-the" class="nav-link" data-scroll-target="#will-see-the-output-is-randomly-generated-due-to-the-fact-that-the">will see the output is randomly generated due to the fact that the</a></li>
  <li><a href="#model-is-not-trained-yet.-in-the-next-tutorial-we-will-build-the" id="toc-model-is-not-trained-yet.-in-the-next-tutorial-we-will-build-the" class="nav-link" data-scroll-target="#model-is-not-trained-yet.-in-the-next-tutorial-we-will-build-the">model is not trained yet. In the next tutorial we will build the</a></li>
  <li><a href="#training-function-and-try-to-train-our-model-to-memorize-the-numbers" id="toc-training-function-and-try-to-train-our-model-to-memorize-the-numbers" class="nav-link" data-scroll-target="#training-function-and-try-to-train-our-model-to-memorize-the-numbers">training function and try to train our model to memorize the numbers</a></li>
  <li><a href="#from-1-to-10." id="toc-from-1-to-10." class="nav-link" data-scroll-target="#from-1-to-10.">from 1 to 10.</a></li>
  <li><a href="#section-71" id="toc-section-71" class="nav-link" data-scroll-target="#section-71">%%</a></li>
  <li><a href="#markdown-2" id="toc-markdown-2" class="nav-link" data-scroll-target="#markdown-2">%% [markdown]</a></li>
  <li><a href="#part-2-model-training" id="toc-part-2-model-training" class="nav-link" data-scroll-target="#part-2-model-training"># Part 2: Model Training</a></li>
  <li><a href="#markdown-id05s6ot9ftsqi" id="toc-markdown-id05s6ot9ftsqi" class="nav-link" data-scroll-target="#markdown-id05s6ot9ftsqi">%% [markdown] id=“05s6oT9fTsqI”</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training"># Training</a></li>
  <li><a href="#section-72" id="toc-section-72" class="nav-link" data-scroll-target="#section-72"></a></li>
  <li><a href="#this-section-describes-the-training-regime-for-our-models." id="toc-this-section-describes-the-training-regime-for-our-models." class="nav-link" data-scroll-target="#this-section-describes-the-training-regime-for-our-models.">This section describes the training regime for our models.</a></li>
  <li><a href="#markdown-idftxlofs4tsqi" id="toc-markdown-idftxlofs4tsqi" class="nav-link" data-scroll-target="#markdown-idftxlofs4tsqi">%% [markdown] id=“fTxlofs4TsqI”</a></li>
  <li><a href="#section-73" id="toc-section-73" class="nav-link" data-scroll-target="#section-73"></a></li>
  <li><a href="#we-stop-for-a-quick-interlude-to-introduce-some-of-the-tools" id="toc-we-stop-for-a-quick-interlude-to-introduce-some-of-the-tools" class="nav-link" data-scroll-target="#we-stop-for-a-quick-interlude-to-introduce-some-of-the-tools">&gt; We stop for a quick interlude to introduce some of the tools</a></li>
  <li><a href="#needed-to-train-a-standard-encoder-decoder-model.-first-we-define-a" id="toc-needed-to-train-a-standard-encoder-decoder-model.-first-we-define-a" class="nav-link" data-scroll-target="#needed-to-train-a-standard-encoder-decoder-model.-first-we-define-a">&gt; needed to train a standard encoder decoder model. First we define a</a></li>
  <li><a href="#batch-object-that-holds-the-src-and-target-sentences-for-training" id="toc-batch-object-that-holds-the-src-and-target-sentences-for-training" class="nav-link" data-scroll-target="#batch-object-that-holds-the-src-and-target-sentences-for-training">&gt; batch object that holds the src and target sentences for training,</a></li>
  <li><a href="#as-well-as-constructing-the-masks." id="toc-as-well-as-constructing-the-masks." class="nav-link" data-scroll-target="#as-well-as-constructing-the-masks.">&gt; as well as constructing the masks.</a></li>
  <li><a href="#markdown-idg7skcenxtsqi" id="toc-markdown-idg7skcenxtsqi" class="nav-link" data-scroll-target="#markdown-idg7skcenxtsqi">%% [markdown] id=“G7SkCenXTsqI”</a></li>
  <li><a href="#batches-and-masking" id="toc-batches-and-masking" class="nav-link" data-scroll-target="#batches-and-masking">## Batches and Masking</a></li>
  <li><a href="#section-74" id="toc-section-74" class="nav-link" data-scroll-target="#section-74">%%</a></li>
  <li><a href="#markdown-idckkw5gjltsqi" id="toc-markdown-idckkw5gjltsqi" class="nav-link" data-scroll-target="#markdown-idckkw5gjltsqi">%% [markdown] id=“cKkw5GjLTsqI”</a></li>
  <li><a href="#section-75" id="toc-section-75" class="nav-link" data-scroll-target="#section-75"></a></li>
  <li><a href="#next-we-create-a-generic-training-and-scoring-function-to-keep" id="toc-next-we-create-a-generic-training-and-scoring-function-to-keep" class="nav-link" data-scroll-target="#next-we-create-a-generic-training-and-scoring-function-to-keep">&gt; Next we create a generic training and scoring function to keep</a></li>
  <li><a href="#track-of-loss.-we-pass-in-a-generic-loss-compute-function-that" id="toc-track-of-loss.-we-pass-in-a-generic-loss-compute-function-that" class="nav-link" data-scroll-target="#track-of-loss.-we-pass-in-a-generic-loss-compute-function-that">&gt; track of loss. We pass in a generic loss compute function that</a></li>
  <li><a href="#also-handles-parameter-updates." id="toc-also-handles-parameter-updates." class="nav-link" data-scroll-target="#also-handles-parameter-updates.">&gt; also handles parameter updates.</a></li>
  <li><a href="#markdown-idq8zzeuc0tsqj" id="toc-markdown-idq8zzeuc0tsqj" class="nav-link" data-scroll-target="#markdown-idq8zzeuc0tsqj">%% [markdown] id=“Q8zzeUc0TsqJ”</a></li>
  <li><a href="#training-loop" id="toc-training-loop" class="nav-link" data-scroll-target="#training-loop">## Training Loop</a></li>
  <li><a href="#section-76" id="toc-section-76" class="nav-link" data-scroll-target="#section-76">%%</a></li>
  <li><a href="#id2hazd3hitsqj" id="toc-id2hazd3hitsqj" class="nav-link" data-scroll-target="#id2hazd3hitsqj">%% id=“2HAZD3hiTsqJ”</a></li>
  <li><a href="#markdown-idab1if0fotsqj" id="toc-markdown-idab1if0fotsqj" class="nav-link" data-scroll-target="#markdown-idab1if0fotsqj">%% [markdown] id=“aB1IF0foTsqJ”</a></li>
  <li><a href="#training-data-and-batching" id="toc-training-data-and-batching" class="nav-link" data-scroll-target="#training-data-and-batching">## Training Data and Batching</a></li>
  <li><a href="#section-77" id="toc-section-77" class="nav-link" data-scroll-target="#section-77"></a></li>
  <li><a href="#we-trained-on-the-standard-wmt-2014-english-german-dataset" id="toc-we-trained-on-the-standard-wmt-2014-english-german-dataset" class="nav-link" data-scroll-target="#we-trained-on-the-standard-wmt-2014-english-german-dataset">We trained on the standard WMT 2014 English-German dataset</a></li>
  <li><a href="#consisting-of-about-4.5-million-sentence-pairs.-sentences-were" id="toc-consisting-of-about-4.5-million-sentence-pairs.-sentences-were" class="nav-link" data-scroll-target="#consisting-of-about-4.5-million-sentence-pairs.-sentences-were">consisting of about 4.5 million sentence pairs. Sentences were</a></li>
  <li><a href="#encoded-using-byte-pair-encoding-which-has-a-shared-source-target" id="toc-encoded-using-byte-pair-encoding-which-has-a-shared-source-target" class="nav-link" data-scroll-target="#encoded-using-byte-pair-encoding-which-has-a-shared-source-target">encoded using byte-pair encoding, which has a shared source-target</a></li>
  <li><a href="#vocabulary-of-about-37000-tokens.-for-english-french-we-used-the" id="toc-vocabulary-of-about-37000-tokens.-for-english-french-we-used-the" class="nav-link" data-scroll-target="#vocabulary-of-about-37000-tokens.-for-english-french-we-used-the">vocabulary of about 37000 tokens. For English-French, we used the</a></li>
  <li><a href="#significantly-larger-wmt-2014-english-french-dataset-consisting-of" id="toc-significantly-larger-wmt-2014-english-french-dataset-consisting-of" class="nav-link" data-scroll-target="#significantly-larger-wmt-2014-english-french-dataset-consisting-of">significantly larger WMT 2014 English-French dataset consisting of</a></li>
  <li><a href="#m-sentences-and-split-tokens-into-a-32000-word-piece-vocabulary." id="toc-m-sentences-and-split-tokens-into-a-32000-word-piece-vocabulary." class="nav-link" data-scroll-target="#m-sentences-and-split-tokens-into-a-32000-word-piece-vocabulary.">36M sentences and split tokens into a 32000 word-piece vocabulary.</a></li>
  <li><a href="#section-78" id="toc-section-78" class="nav-link" data-scroll-target="#section-78"></a></li>
  <li><a href="#section-79" id="toc-section-79" class="nav-link" data-scroll-target="#section-79"></a></li>
  <li><a href="#sentence-pairs-were-batched-together-by-approximate-sequence-length." id="toc-sentence-pairs-were-batched-together-by-approximate-sequence-length." class="nav-link" data-scroll-target="#sentence-pairs-were-batched-together-by-approximate-sequence-length.">Sentence pairs were batched together by approximate sequence length.</a></li>
  <li><a href="#each-training-batch-contained-a-set-of-sentence-pairs-containing" id="toc-each-training-batch-contained-a-set-of-sentence-pairs-containing" class="nav-link" data-scroll-target="#each-training-batch-contained-a-set-of-sentence-pairs-containing">Each training batch contained a set of sentence pairs containing</a></li>
  <li><a href="#approximately-25000-source-tokens-and-25000-target-tokens." id="toc-approximately-25000-source-tokens-and-25000-target-tokens." class="nav-link" data-scroll-target="#approximately-25000-source-tokens-and-25000-target-tokens.">approximately 25000 source tokens and 25000 target tokens.</a></li>
  <li><a href="#markdown-idf1mtqatitsqj-jp-markdownheadingcollapsedtrue-tags" id="toc-markdown-idf1mtqatitsqj-jp-markdownheadingcollapsedtrue-tags" class="nav-link" data-scroll-target="#markdown-idf1mtqatitsqj-jp-markdownheadingcollapsedtrue-tags">%% [markdown] id=“F1mTQatiTsqJ” jp-MarkdownHeadingCollapsed=true tags=[]</a></li>
  <li><a href="#hardware-and-schedule" id="toc-hardware-and-schedule" class="nav-link" data-scroll-target="#hardware-and-schedule">## Hardware and Schedule</a></li>
  <li><a href="#section-80" id="toc-section-80" class="nav-link" data-scroll-target="#section-80"></a></li>
  <li><a href="#we-trained-our-models-on-one-machine-with-8-nvidia-p100-gpus.-for" id="toc-we-trained-our-models-on-one-machine-with-8-nvidia-p100-gpus.-for" class="nav-link" data-scroll-target="#we-trained-our-models-on-one-machine-with-8-nvidia-p100-gpus.-for">We trained our models on one machine with 8 NVIDIA P100 GPUs. For</a></li>
  <li><a href="#our-base-models-using-the-hyperparameters-described-throughout-the" id="toc-our-base-models-using-the-hyperparameters-described-throughout-the" class="nav-link" data-scroll-target="#our-base-models-using-the-hyperparameters-described-throughout-the">our base models using the hyperparameters described throughout the</a></li>
  <li><a href="#paper-each-training-step-took-about-0.4-seconds.-we-trained-the" id="toc-paper-each-training-step-took-about-0.4-seconds.-we-trained-the" class="nav-link" data-scroll-target="#paper-each-training-step-took-about-0.4-seconds.-we-trained-the">paper, each training step took about 0.4 seconds. We trained the</a></li>
  <li><a href="#base-models-for-a-total-of-100000-steps-or-12-hours.-for-our-big" id="toc-base-models-for-a-total-of-100000-steps-or-12-hours.-for-our-big" class="nav-link" data-scroll-target="#base-models-for-a-total-of-100000-steps-or-12-hours.-for-our-big">base models for a total of 100,000 steps or 12 hours. For our big</a></li>
  <li><a href="#models-step-time-was-1.0-seconds.-the-big-models-were-trained-for" id="toc-models-step-time-was-1.0-seconds.-the-big-models-were-trained-for" class="nav-link" data-scroll-target="#models-step-time-was-1.0-seconds.-the-big-models-were-trained-for">models, step time was 1.0 seconds. The big models were trained for</a></li>
  <li><a href="#steps-3.5-days." id="toc-steps-3.5-days." class="nav-link" data-scroll-target="#steps-3.5-days.">300,000 steps (3.5 days).</a></li>
  <li><a href="#markdown-id-utzeugctsqj" id="toc-markdown-id-utzeugctsqj" class="nav-link" data-scroll-target="#markdown-id-utzeugctsqj">%% [markdown] id=“-utZeuGcTsqJ”</a></li>
  <li><a href="#optimizer" id="toc-optimizer" class="nav-link" data-scroll-target="#optimizer">## Optimizer</a></li>
  <li><a href="#section-81" id="toc-section-81" class="nav-link" data-scroll-target="#section-81"></a></li>
  <li><a href="#we-used-the-adam-optimizer-cite" id="toc-we-used-the-adam-optimizer-cite" class="nav-link" data-scroll-target="#we-used-the-adam-optimizer-cite">We used the Adam optimizer (cite)</a></li>
  <li><a href="#with-beta_10.9-beta_20.98-and-epsilon10-9.-we" id="toc-with-beta_10.9-beta_20.98-and-epsilon10-9.-we" class="nav-link" data-scroll-target="#with-beta_10.9-beta_20.98-and-epsilon10-9.-we">with <span class="math inline">\(\beta_1=0.9\)</span>, <span class="math inline">\(\beta_2=0.98\)</span> and <span class="math inline">\(\epsilon=10^{-9}\)</span>. We</a></li>
  <li><a href="#varied-the-learning-rate-over-the-course-of-training-according-to" id="toc-varied-the-learning-rate-over-the-course-of-training-according-to" class="nav-link" data-scroll-target="#varied-the-learning-rate-over-the-course-of-training-according-to">varied the learning rate over the course of training, according to</a></li>
  <li><a href="#the-formula" id="toc-the-formula" class="nav-link" data-scroll-target="#the-formula">the formula:</a></li>
  <li><a href="#section-82" id="toc-section-82" class="nav-link" data-scroll-target="#section-82"></a></li>
  <li><a href="#lrate-d_textmodel-0.5-cdot-minstep_num-0.5-step_num-cdot-warmup_steps-1.5" id="toc-lrate-d_textmodel-0.5-cdot-minstep_num-0.5-step_num-cdot-warmup_steps-1.5" class="nav-link" data-scroll-target="#lrate-d_textmodel-0.5-cdot-minstep_num-0.5-step_num-cdot-warmup_steps-1.5"><span class="math display">\[
  # lrate = d_{\text{model}}^{-0.5} \cdot
  #   \min({step\_num}^{-0.5},
  #     {step\_num} \cdot {warmup\_steps}^{-1.5})
  # \]</span></a></li>
  <li><a href="#section-83" id="toc-section-83" class="nav-link" data-scroll-target="#section-83"></a></li>
  <li><a href="#this-corresponds-to-increasing-the-learning-rate-linearly-for-the" id="toc-this-corresponds-to-increasing-the-learning-rate-linearly-for-the" class="nav-link" data-scroll-target="#this-corresponds-to-increasing-the-learning-rate-linearly-for-the">This corresponds to increasing the learning rate linearly for the</a></li>
  <li><a href="#first-warmup_steps-training-steps-and-decreasing-it-thereafter" id="toc-first-warmup_steps-training-steps-and-decreasing-it-thereafter" class="nav-link" data-scroll-target="#first-warmup_steps-training-steps-and-decreasing-it-thereafter">first <span class="math inline">\(warmup\_steps\)</span> training steps, and decreasing it thereafter</a></li>
  <li><a href="#proportionally-to-the-inverse-square-root-of-the-step-number.-we" id="toc-proportionally-to-the-inverse-square-root-of-the-step-number.-we" class="nav-link" data-scroll-target="#proportionally-to-the-inverse-square-root-of-the-step-number.-we">proportionally to the inverse square root of the step number. We</a></li>
  <li><a href="#used-warmup_steps4000." id="toc-used-warmup_steps4000." class="nav-link" data-scroll-target="#used-warmup_steps4000.">used <span class="math inline">\(warmup\_steps=4000\)</span>.</a></li>
  <li><a href="#markdown-id39fbynt-tsqj" id="toc-markdown-id39fbynt-tsqj" class="nav-link" data-scroll-target="#markdown-id39fbynt-tsqj">%% [markdown] id=“39FbYnt-TsqJ”</a></li>
  <li><a href="#section-84" id="toc-section-84" class="nav-link" data-scroll-target="#section-84"></a></li>
  <li><a href="#note-this-part-is-very-important.-need-to-train-with-this-setup" id="toc-note-this-part-is-very-important.-need-to-train-with-this-setup" class="nav-link" data-scroll-target="#note-this-part-is-very-important.-need-to-train-with-this-setup">&gt; Note: This part is very important. Need to train with this setup</a></li>
  <li><a href="#of-the-model." id="toc-of-the-model." class="nav-link" data-scroll-target="#of-the-model.">&gt; of the model.</a></li>
  <li><a href="#markdown-idhlbojfkjtsqj" id="toc-markdown-idhlbojfkjtsqj" class="nav-link" data-scroll-target="#markdown-idhlbojfkjtsqj">%% [markdown] id=“hlbojFkjTsqJ”</a></li>
  <li><a href="#section-85" id="toc-section-85" class="nav-link" data-scroll-target="#section-85"></a></li>
  <li><a href="#example-of-the-curves-of-this-model-for-different-model-sizes-and" id="toc-example-of-the-curves-of-this-model-for-different-model-sizes-and" class="nav-link" data-scroll-target="#example-of-the-curves-of-this-model-for-different-model-sizes-and">&gt; Example of the curves of this model for different model sizes and</a></li>
  <li><a href="#for-optimization-hyperparameters." id="toc-for-optimization-hyperparameters." class="nav-link" data-scroll-target="#for-optimization-hyperparameters.">&gt; for optimization hyperparameters.</a></li>
  <li><a href="#idzuz3pdanvg4o" id="toc-idzuz3pdanvg4o" class="nav-link" data-scroll-target="#idzuz3pdanvg4o">%% id=“zUz3PdAnVg4o”</a></li>
  <li><a href="#idl1bnrlnsv8j5-tags" id="toc-idl1bnrlnsv8j5-tags" class="nav-link" data-scroll-target="#idl1bnrlnsv8j5-tags">%% id=“l1bnrlnSV8J5” tags=[]</a></li>
  <li><a href="#markdown-id7t1ud15vtsqk" id="toc-markdown-id7t1ud15vtsqk" class="nav-link" data-scroll-target="#markdown-id7t1ud15vtsqk">%% [markdown] id=“7T1uD15VTsqK”</a></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization">## Regularization</a></li>
  <li><a href="#section-86" id="toc-section-86" class="nav-link" data-scroll-target="#section-86"></a></li>
  <li><a href="#label-smoothing" id="toc-label-smoothing" class="nav-link" data-scroll-target="#label-smoothing">### Label Smoothing</a></li>
  <li><a href="#section-87" id="toc-section-87" class="nav-link" data-scroll-target="#section-87"></a></li>
  <li><a href="#during-training-we-employed-label-smoothing-of-value" id="toc-during-training-we-employed-label-smoothing-of-value" class="nav-link" data-scroll-target="#during-training-we-employed-label-smoothing-of-value">During training, we employed label smoothing of value</a></li>
  <li><a href="#epsilon_ls0.1-cite." id="toc-epsilon_ls0.1-cite." class="nav-link" data-scroll-target="#epsilon_ls0.1-cite."><span class="math inline">\(\epsilon_{ls}=0.1\)</span> (cite).</a></li>
  <li><a href="#this-hurts-perplexity-as-the-model-learns-to-be-more-unsure-but" id="toc-this-hurts-perplexity-as-the-model-learns-to-be-more-unsure-but" class="nav-link" data-scroll-target="#this-hurts-perplexity-as-the-model-learns-to-be-more-unsure-but">This hurts perplexity, as the model learns to be more unsure, but</a></li>
  <li><a href="#improves-accuracy-and-bleu-score." id="toc-improves-accuracy-and-bleu-score." class="nav-link" data-scroll-target="#improves-accuracy-and-bleu-score.">improves accuracy and BLEU score.</a></li>
  <li><a href="#markdown-idknoavd8btsqk" id="toc-markdown-idknoavd8btsqk" class="nav-link" data-scroll-target="#markdown-idknoavd8btsqk">%% [markdown] id=“kNoAVD8bTsqK”</a></li>
  <li><a href="#section-88" id="toc-section-88" class="nav-link" data-scroll-target="#section-88"></a></li>
  <li><a href="#we-implement-label-smoothing-using-the-kl-div-loss.-instead-of" id="toc-we-implement-label-smoothing-using-the-kl-div-loss.-instead-of" class="nav-link" data-scroll-target="#we-implement-label-smoothing-using-the-kl-div-loss.-instead-of">&gt; We implement label smoothing using the KL div loss. Instead of</a></li>
  <li><a href="#using-a-one-hot-target-distribution-we-create-a-distribution-that" id="toc-using-a-one-hot-target-distribution-we-create-a-distribution-that" class="nav-link" data-scroll-target="#using-a-one-hot-target-distribution-we-create-a-distribution-that">&gt; using a one-hot target distribution, we create a distribution that</a></li>
  <li><a href="#has-confidence-of-the-correct-word-and-the-rest-of-the" id="toc-has-confidence-of-the-correct-word-and-the-rest-of-the" class="nav-link" data-scroll-target="#has-confidence-of-the-correct-word-and-the-rest-of-the">&gt; has <code>confidence</code> of the correct word and the rest of the</a></li>
  <li><a href="#smoothing-mass-distributed-throughout-the-vocabulary." id="toc-smoothing-mass-distributed-throughout-the-vocabulary." class="nav-link" data-scroll-target="#smoothing-mass-distributed-throughout-the-vocabulary.">&gt; <code>smoothing</code> mass distributed throughout the vocabulary.</a></li>
  <li><a href="#idshu2gyietsqk" id="toc-idshu2gyietsqk" class="nav-link" data-scroll-target="#idshu2gyietsqk">%% id=“shU2GyiETsqK”</a></li>
  <li><a href="#markdown-idjcxurluytsqk" id="toc-markdown-idjcxurluytsqk" class="nav-link" data-scroll-target="#markdown-idjcxurluytsqk">%% [markdown] id=“jCxUrlUyTsqK”</a></li>
  <li><a href="#section-89" id="toc-section-89" class="nav-link" data-scroll-target="#section-89"></a></li>
  <li><a href="#here-we-can-see-an-example-of-how-the-mass-is-distributed-to-the" id="toc-here-we-can-see-an-example-of-how-the-mass-is-distributed-to-the" class="nav-link" data-scroll-target="#here-we-can-see-an-example-of-how-the-mass-is-distributed-to-the">&gt; Here we can see an example of how the mass is distributed to the</a></li>
  <li><a href="#words-based-on-confidence." id="toc-words-based-on-confidence." class="nav-link" data-scroll-target="#words-based-on-confidence.">&gt; words based on confidence.</a></li>
  <li><a href="#ideztkaaqntsqk" id="toc-ideztkaaqntsqk" class="nav-link" data-scroll-target="#ideztkaaqntsqk">%% id=“EZtKaaQNTsqK”</a></li>
  <li><a href="#example-of-label-smoothing." id="toc-example-of-label-smoothing." class="nav-link" data-scroll-target="#example-of-label-smoothing.">Example of label smoothing.</a></li>
  <li><a href="#markdown-idcgm8j1vetsqk" id="toc-markdown-idcgm8j1vetsqk" class="nav-link" data-scroll-target="#markdown-idcgm8j1vetsqk">%% [markdown] id=“CGM8J1veTsqK”</a></li>
  <li><a href="#section-90" id="toc-section-90" class="nav-link" data-scroll-target="#section-90"></a></li>
  <li><a href="#label-smoothing-actually-starts-to-penalize-the-model-if-it-gets" id="toc-label-smoothing-actually-starts-to-penalize-the-model-if-it-gets" class="nav-link" data-scroll-target="#label-smoothing-actually-starts-to-penalize-the-model-if-it-gets">&gt; Label smoothing actually starts to penalize the model if it gets</a></li>
  <li><a href="#very-confident-about-a-given-choice." id="toc-very-confident-about-a-given-choice." class="nav-link" data-scroll-target="#very-confident-about-a-given-choice.">&gt; very confident about a given choice.</a></li>
  <li><a href="#id78ehzlp7tsqk" id="toc-id78ehzlp7tsqk" class="nav-link" data-scroll-target="#id78ehzlp7tsqk">%% id=“78EHzLP7TsqK”</a></li>
  <li><a href="#markdown-id67luqelxtsqk" id="toc-markdown-id67luqelxtsqk" class="nav-link" data-scroll-target="#markdown-id67luqelxtsqk">%% [markdown] id=“67lUqeLXTsqK”</a></li>
  <li><a href="#a-first-example" id="toc-a-first-example" class="nav-link" data-scroll-target="#a-first-example"># A First Example</a></li>
  <li><a href="#section-91" id="toc-section-91" class="nav-link" data-scroll-target="#section-91"></a></li>
  <li><a href="#we-can-begin-by-trying-out-a-simple-copy-task.-given-a-random-set" id="toc-we-can-begin-by-trying-out-a-simple-copy-task.-given-a-random-set" class="nav-link" data-scroll-target="#we-can-begin-by-trying-out-a-simple-copy-task.-given-a-random-set">&gt; We can begin by trying out a simple copy-task. Given a random set</a></li>
  <li><a href="#of-input-symbols-from-a-small-vocabulary-the-goal-is-to-generate" id="toc-of-input-symbols-from-a-small-vocabulary-the-goal-is-to-generate" class="nav-link" data-scroll-target="#of-input-symbols-from-a-small-vocabulary-the-goal-is-to-generate">&gt; of input symbols from a small vocabulary, the goal is to generate</a></li>
  <li><a href="#back-those-same-symbols." id="toc-back-those-same-symbols." class="nav-link" data-scroll-target="#back-those-same-symbols.">&gt; back those same symbols.</a></li>
  <li><a href="#markdown-idjja-89_ptsqk" id="toc-markdown-idjja-89_ptsqk" class="nav-link" data-scroll-target="#markdown-idjja-89_ptsqk">%% [markdown] id=“jJa-89_pTsqK”</a></li>
  <li><a href="#synthetic-data" id="toc-synthetic-data" class="nav-link" data-scroll-target="#synthetic-data">## Synthetic Data</a></li>
  <li><a href="#idg1atxeqqtsqk" id="toc-idg1atxeqqtsqk" class="nav-link" data-scroll-target="#idg1atxeqqtsqk">%% id=“g1aTxeqqTsqK”</a></li>
  <li><a href="#markdown-idxtxwd9hutsqk" id="toc-markdown-idxtxwd9hutsqk" class="nav-link" data-scroll-target="#markdown-idxtxwd9hutsqk">%% [markdown] id=“XTXwD9hUTsqK”</a></li>
  <li><a href="#loss-computation" id="toc-loss-computation" class="nav-link" data-scroll-target="#loss-computation">## Loss Computation</a></li>
  <li><a href="#id3j8ejm87tsqk" id="toc-id3j8ejm87tsqk" class="nav-link" data-scroll-target="#id3j8ejm87tsqk">%% id=“3J8EJm87TsqK”</a></li>
  <li><a href="#markdown-idedai7elutsql" id="toc-markdown-idedai7elutsql" class="nav-link" data-scroll-target="#markdown-idedai7elutsql">%% [markdown] id=“eDAI7ELUTsqL”</a></li>
  <li><a href="#greedy-decoding" id="toc-greedy-decoding" class="nav-link" data-scroll-target="#greedy-decoding">## Greedy Decoding</a></li>
  <li><a href="#markdown-idlfkwakpltsql-tags" id="toc-markdown-idlfkwakpltsql-tags" class="nav-link" data-scroll-target="#markdown-idlfkwakpltsql-tags">%% [markdown] id=“LFkWakplTsqL” tags=[]</a></li>
  <li><a href="#this-code-predicts-a-translation-using-greedy-decoding-for-simplicity." id="toc-this-code-predicts-a-translation-using-greedy-decoding-for-simplicity." class="nav-link" data-scroll-target="#this-code-predicts-a-translation-using-greedy-decoding-for-simplicity.">&gt; This code predicts a translation using greedy decoding for simplicity.</a></li>
  <li><a href="#idn2uopnt3biyu" id="toc-idn2uopnt3biyu" class="nav-link" data-scroll-target="#idn2uopnt3biyu">%% id=“N2UOpnT3bIyU”</a></li>
  <li><a href="#idqgiz2yetdywe-tags" id="toc-idqgiz2yetdywe-tags" class="nav-link" data-scroll-target="#idqgiz2yetdywe-tags">%% id=“qgIZ2yEtdYwe” tags=[]</a></li>
  <li><a href="#train-the-simple-copy-task." id="toc-train-the-simple-copy-task." class="nav-link" data-scroll-target="#train-the-simple-copy-task.">Train the simple copy task.</a></li>
  <li><a href="#execute_exampleexample_simple_model" id="toc-execute_exampleexample_simple_model" class="nav-link" data-scroll-target="#execute_exampleexample_simple_model">execute_example(example_simple_model)</a></li>
  <li><a href="#markdown-idopuqv2gstsql" id="toc-markdown-idopuqv2gstsql" class="nav-link" data-scroll-target="#markdown-idopuqv2gstsql">%% [markdown] id=“OpuQv2GsTsqL”</a></li>
  <li><a href="#part-3-a-real-world-example" id="toc-part-3-a-real-world-example" class="nav-link" data-scroll-target="#part-3-a-real-world-example"># Part 3: A Real World Example</a></li>
  <li><a href="#section-92" id="toc-section-92" class="nav-link" data-scroll-target="#section-92"></a></li>
  <li><a href="#now-we-consider-a-real-world-example-using-the-multi30k" id="toc-now-we-consider-a-real-world-example-using-the-multi30k" class="nav-link" data-scroll-target="#now-we-consider-a-real-world-example-using-the-multi30k">&gt; Now we consider a real-world example using the Multi30k</a></li>
  <li><a href="#german-english-translation-task.-this-task-is-much-smaller-than" id="toc-german-english-translation-task.-this-task-is-much-smaller-than" class="nav-link" data-scroll-target="#german-english-translation-task.-this-task-is-much-smaller-than">&gt; German-English Translation task. This task is much smaller than</a></li>
  <li><a href="#the-wmt-task-considered-in-the-paper-but-it-illustrates-the-whole" id="toc-the-wmt-task-considered-in-the-paper-but-it-illustrates-the-whole" class="nav-link" data-scroll-target="#the-wmt-task-considered-in-the-paper-but-it-illustrates-the-whole">&gt; the WMT task considered in the paper, but it illustrates the whole</a></li>
  <li><a href="#system.-we-also-show-how-to-use-multi-gpu-processing-to-make-it" id="toc-system.-we-also-show-how-to-use-multi-gpu-processing-to-make-it" class="nav-link" data-scroll-target="#system.-we-also-show-how-to-use-multi-gpu-processing-to-make-it">&gt; system. We also show how to use multi-gpu processing to make it</a></li>
  <li><a href="#really-fast." id="toc-really-fast." class="nav-link" data-scroll-target="#really-fast.">&gt; really fast.</a></li>
  <li><a href="#markdown-id8y9dpfoltsql-tags" id="toc-markdown-id8y9dpfoltsql-tags" class="nav-link" data-scroll-target="#markdown-id8y9dpfoltsql-tags">%% [markdown] id=“8y9dpfolTsqL” tags=[]</a></li>
  <li><a href="#data-loading" id="toc-data-loading" class="nav-link" data-scroll-target="#data-loading">## Data Loading</a></li>
  <li><a href="#section-93" id="toc-section-93" class="nav-link" data-scroll-target="#section-93"></a></li>
  <li><a href="#we-will-load-the-dataset-using-torchtext-and-spacy-for" id="toc-we-will-load-the-dataset-using-torchtext-and-spacy-for" class="nav-link" data-scroll-target="#we-will-load-the-dataset-using-torchtext-and-spacy-for">&gt; We will load the dataset using torchtext and spacy for</a></li>
  <li><a href="#tokenization." id="toc-tokenization." class="nav-link" data-scroll-target="#tokenization.">&gt; tokenization.</a></li>
  <li><a href="#section-94" id="toc-section-94" class="nav-link" data-scroll-target="#section-94">%%</a></li>
  <li><a href="#load-spacy-tokenizer-models-download-them-if-they-havent-been" id="toc-load-spacy-tokenizer-models-download-them-if-they-havent-been" class="nav-link" data-scroll-target="#load-spacy-tokenizer-models-download-them-if-they-havent-been">Load spacy tokenizer models, download them if they haven’t been</a></li>
  <li><a href="#downloaded-already" id="toc-downloaded-already" class="nav-link" data-scroll-target="#downloaded-already">downloaded already</a></li>
  <li><a href="#idt4bszxxjtsql-tags" id="toc-idt4bszxxjtsql-tags" class="nav-link" data-scroll-target="#idt4bszxxjtsql-tags">%% id=“t4BszXXJTsqL” tags=[]</a></li>
  <li><a href="#idju3kvlv5okc--tags" id="toc-idju3kvlv5okc--tags" class="nav-link" data-scroll-target="#idju3kvlv5okc--tags">%% id=“jU3kVlV5okC-” tags=[]</a></li>
  <li><a href="#markdown-id-l-tfwzftsql" id="toc-markdown-id-l-tfwzftsql" class="nav-link" data-scroll-target="#markdown-id-l-tfwzftsql">%% [markdown] id=“-l-TFwzfTsqL”</a></li>
  <li><a href="#section-95" id="toc-section-95" class="nav-link" data-scroll-target="#section-95"></a></li>
  <li><a href="#batching-matters-a-ton-for-speed.-we-want-to-have-very-evenly" id="toc-batching-matters-a-ton-for-speed.-we-want-to-have-very-evenly" class="nav-link" data-scroll-target="#batching-matters-a-ton-for-speed.-we-want-to-have-very-evenly">&gt; Batching matters a ton for speed. We want to have very evenly</a></li>
  <li><a href="#divided-batches-with-absolutely-minimal-padding.-to-do-this-we" id="toc-divided-batches-with-absolutely-minimal-padding.-to-do-this-we" class="nav-link" data-scroll-target="#divided-batches-with-absolutely-minimal-padding.-to-do-this-we">&gt; divided batches, with absolutely minimal padding. To do this we</a></li>
  <li><a href="#have-to-hack-a-bit-around-the-default-torchtext-batching.-this" id="toc-have-to-hack-a-bit-around-the-default-torchtext-batching.-this" class="nav-link" data-scroll-target="#have-to-hack-a-bit-around-the-default-torchtext-batching.-this">&gt; have to hack a bit around the default torchtext batching. This</a></li>
  <li><a href="#code-patches-their-default-batching-to-make-sure-we-search-over" id="toc-code-patches-their-default-batching-to-make-sure-we-search-over" class="nav-link" data-scroll-target="#code-patches-their-default-batching-to-make-sure-we-search-over">&gt; code patches their default batching to make sure we search over</a></li>
  <li><a href="#enough-sentences-to-find-tight-batches." id="toc-enough-sentences-to-find-tight-batches." class="nav-link" data-scroll-target="#enough-sentences-to-find-tight-batches.">&gt; enough sentences to find tight batches.</a></li>
  <li><a href="#markdown-idkdej-hcgokc--tags-jp-markdownheadingcollapsedtrue" id="toc-markdown-idkdej-hcgokc--tags-jp-markdownheadingcollapsedtrue" class="nav-link" data-scroll-target="#markdown-idkdej-hcgokc--tags-jp-markdownheadingcollapsedtrue">%% [markdown] id=“kDEj-hCgokC-” tags=[] jp-MarkdownHeadingCollapsed=true</a></li>
  <li><a href="#iterators" id="toc-iterators" class="nav-link" data-scroll-target="#iterators">## Iterators</a></li>
  <li><a href="#idwgsihfgookc_-tags" id="toc-idwgsihfgookc_-tags" class="nav-link" data-scroll-target="#idwgsihfgookc_-tags">%% id=“wGsIHFgOokC_” tags=[]</a></li>
  <li><a href="#idka2ce_wiokc_-tags" id="toc-idka2ce_wiokc_-tags" class="nav-link" data-scroll-target="#idka2ce_wiokc_-tags">%% id=“ka2Ce_WIokC_” tags=[]</a></li>
  <li><a href="#markdown-id90qm8rzctsqm" id="toc-markdown-id90qm8rzctsqm" class="nav-link" data-scroll-target="#markdown-id90qm8rzctsqm">%% [markdown] id=“90qM8RzCTsqM”</a></li>
  <li><a href="#training-the-system" id="toc-training-the-system" class="nav-link" data-scroll-target="#training-the-system">## Training the System</a></li>
  <li><a href="#section-96" id="toc-section-96" class="nav-link" data-scroll-target="#section-96">%%</a></li>
  <li><a href="#tags" id="toc-tags" class="nav-link" data-scroll-target="#tags">%% tags=[]</a></li>
  <li><a href="#markdown-idrzk_vjdptsqn" id="toc-markdown-idrzk_vjdptsqn" class="nav-link" data-scroll-target="#markdown-idrzk_vjdptsqn">%% [markdown] id=“RZK_VjDPTsqN”</a></li>
  <li><a href="#section-97" id="toc-section-97" class="nav-link" data-scroll-target="#section-97"></a></li>
  <li><a href="#once-trained-we-can-decode-the-model-to-produce-a-set-of" id="toc-once-trained-we-can-decode-the-model-to-produce-a-set-of" class="nav-link" data-scroll-target="#once-trained-we-can-decode-the-model-to-produce-a-set-of">&gt; Once trained we can decode the model to produce a set of</a></li>
  <li><a href="#translations.-here-we-simply-translate-the-first-sentence-in-the" id="toc-translations.-here-we-simply-translate-the-first-sentence-in-the" class="nav-link" data-scroll-target="#translations.-here-we-simply-translate-the-first-sentence-in-the">&gt; translations. Here we simply translate the first sentence in the</a></li>
  <li><a href="#validation-set.-this-dataset-is-pretty-small-so-the-translations" id="toc-validation-set.-this-dataset-is-pretty-small-so-the-translations" class="nav-link" data-scroll-target="#validation-set.-this-dataset-is-pretty-small-so-the-translations">&gt; validation set. This dataset is pretty small so the translations</a></li>
  <li><a href="#with-greedy-search-are-reasonably-accurate." id="toc-with-greedy-search-are-reasonably-accurate." class="nav-link" data-scroll-target="#with-greedy-search-are-reasonably-accurate.">&gt; with greedy search are reasonably accurate.</a></li>
  <li><a href="#markdown-idl50i0iextsqn" id="toc-markdown-idl50i0iextsqn" class="nav-link" data-scroll-target="#markdown-idl50i0iextsqn">%% [markdown] id=“L50i0iEXTsqN”</a></li>
  <li><a href="#additional-components-bpe-search-averaging" id="toc-additional-components-bpe-search-averaging" class="nav-link" data-scroll-target="#additional-components-bpe-search-averaging"># Additional Components: BPE, Search, Averaging</a></li>
  <li><a href="#markdown-idnbx1c2_ntsqn" id="toc-markdown-idnbx1c2_ntsqn" class="nav-link" data-scroll-target="#markdown-idnbx1c2_ntsqn">%% [markdown] id=“NBx1C2_NTsqN”</a></li>
  <li><a href="#section-98" id="toc-section-98" class="nav-link" data-scroll-target="#section-98"></a></li>
  <li><a href="#so-this-mostly-covers-the-transformer-model-itself.-there-are-four" id="toc-so-this-mostly-covers-the-transformer-model-itself.-there-are-four" class="nav-link" data-scroll-target="#so-this-mostly-covers-the-transformer-model-itself.-there-are-four">&gt; So this mostly covers the transformer model itself. There are four</a></li>
  <li><a href="#aspects-that-we-didnt-cover-explicitly.-we-also-have-all-these" id="toc-aspects-that-we-didnt-cover-explicitly.-we-also-have-all-these" class="nav-link" data-scroll-target="#aspects-that-we-didnt-cover-explicitly.-we-also-have-all-these">&gt; aspects that we didn’t cover explicitly. We also have all these</a></li>
  <li><a href="#additional-features-implemented-in" id="toc-additional-features-implemented-in" class="nav-link" data-scroll-target="#additional-features-implemented-in">&gt; additional features implemented in</a></li>
  <li><a href="#opennmt-py." id="toc-opennmt-py." class="nav-link" data-scroll-target="#opennmt-py.">&gt; OpenNMT-py.</a></li>
  <li><a href="#section-99" id="toc-section-99" class="nav-link" data-scroll-target="#section-99"></a></li>
  <li><a href="#section-100" id="toc-section-100" class="nav-link" data-scroll-target="#section-100"></a></li>
  <li><a href="#markdown-idupqv1mwntsqn" id="toc-markdown-idupqv1mwntsqn" class="nav-link" data-scroll-target="#markdown-idupqv1mwntsqn">%% [markdown] id=“UpqV1mWnTsqN”</a></li>
  <li><a href="#section-101" id="toc-section-101" class="nav-link" data-scroll-target="#section-101"></a></li>
  <li><a href="#bpe-word-piece-we-can-use-a-library-to-first-preprocess-the" id="toc-bpe-word-piece-we-can-use-a-library-to-first-preprocess-the" class="nav-link" data-scroll-target="#bpe-word-piece-we-can-use-a-library-to-first-preprocess-the">&gt; 1) BPE/ Word-piece: We can use a library to first preprocess the</a></li>
  <li><a href="#data-into-subword-units.-see-rico-sennrichs" id="toc-data-into-subword-units.-see-rico-sennrichs" class="nav-link" data-scroll-target="#data-into-subword-units.-see-rico-sennrichs">&gt; data into subword units. See Rico Sennrich’s</a></li>
  <li><a href="#subword-nmt" id="toc-subword-nmt" class="nav-link" data-scroll-target="#subword-nmt">&gt; subword-nmt</a></li>
  <li><a href="#implementation.-these-models-will-transform-the-training-data-to" id="toc-implementation.-these-models-will-transform-the-training-data-to" class="nav-link" data-scroll-target="#implementation.-these-models-will-transform-the-training-data-to">&gt; implementation. These models will transform the training data to</a></li>
  <li><a href="#look-like-this" id="toc-look-like-this" class="nav-link" data-scroll-target="#look-like-this">&gt; look like this:</a></li>
  <li><a href="#markdown-idhwj_9j0btsqn" id="toc-markdown-idhwj_9j0btsqn" class="nav-link" data-scroll-target="#markdown-idhwj_9j0btsqn">%% [markdown] id=“hwJ_9J0BTsqN”</a></li>
  <li><a href="#die-protokoll-datei-kann-heimlich-per-e---mail-oder-ftp" id="toc-die-protokoll-datei-kann-heimlich-per-e---mail-oder-ftp" class="nav-link" data-scroll-target="#die-protokoll-datei-kann-heimlich-per-e---mail-oder-ftp">▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP</a></li>
  <li><a href="#an-einen-bestimmte-n-empfänger-gesendet-werden-." id="toc-an-einen-bestimmte-n-empfänger-gesendet-werden-." class="nav-link" data-scroll-target="#an-einen-bestimmte-n-empfänger-gesendet-werden-.">▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .</a></li>
  <li><a href="#markdown-id9hwejykptsqn" id="toc-markdown-id9hwejykptsqn" class="nav-link" data-scroll-target="#markdown-id9hwejykptsqn">%% [markdown] id=“9HwejYkpTsqN”</a></li>
  <li><a href="#section-102" id="toc-section-102" class="nav-link" data-scroll-target="#section-102"></a></li>
  <li><a href="#shared-embeddings-when-using-bpe-with-shared-vocabulary-we-can" id="toc-shared-embeddings-when-using-bpe-with-shared-vocabulary-we-can" class="nav-link" data-scroll-target="#shared-embeddings-when-using-bpe-with-shared-vocabulary-we-can">&gt; 2) Shared Embeddings: When using BPE with shared vocabulary we can</a></li>
  <li><a href="#share-the-same-weight-vectors-between-the-source-target" id="toc-share-the-same-weight-vectors-between-the-source-target" class="nav-link" data-scroll-target="#share-the-same-weight-vectors-between-the-source-target">&gt; share the same weight vectors between the source / target /</a></li>
  <li><a href="#generator.-see-the-cite-for" id="toc-generator.-see-the-cite-for" class="nav-link" data-scroll-target="#generator.-see-the-cite-for">&gt; generator. See the (cite) for</a></li>
  <li><a href="#details.-to-add-this-to-the-model-simply-do-this" id="toc-details.-to-add-this-to-the-model-simply-do-this" class="nav-link" data-scroll-target="#details.-to-add-this-to-the-model-simply-do-this">&gt; details. To add this to the model simply do this:</a></li>
  <li><a href="#idtb3j3cyltsqn-tags" id="toc-idtb3j3cyltsqn-tags" class="nav-link" data-scroll-target="#idtb3j3cyltsqn-tags">%% id=“tb3j3CYLTsqN” tags=[]</a></li>
  <li><a href="#markdown-idxdkjsswrtsqn" id="toc-markdown-idxdkjsswrtsqn" class="nav-link" data-scroll-target="#markdown-idxdkjsswrtsqn">%% [markdown] id=“xDKJsSwRTsqN”</a></li>
  <li><a href="#section-103" id="toc-section-103" class="nav-link" data-scroll-target="#section-103"></a></li>
  <li><a href="#beam-search-this-is-a-bit-too-complicated-to-cover-here.-see-the" id="toc-beam-search-this-is-a-bit-too-complicated-to-cover-here.-see-the" class="nav-link" data-scroll-target="#beam-search-this-is-a-bit-too-complicated-to-cover-here.-see-the">&gt; 3) Beam Search: This is a bit too complicated to cover here. See the</a></li>
  <li><a href="#opennmt-py" id="toc-opennmt-py" class="nav-link" data-scroll-target="#opennmt-py">&gt; OpenNMT-py</a></li>
  <li><a href="#for-a-pytorch-implementation." id="toc-for-a-pytorch-implementation." class="nav-link" data-scroll-target="#for-a-pytorch-implementation.">&gt; for a pytorch implementation.</a></li>
  <li><a href="#section-104" id="toc-section-104" class="nav-link" data-scroll-target="#section-104">&gt;</a></li>
  <li><a href="#section-105" id="toc-section-105" class="nav-link" data-scroll-target="#section-105"></a></li>
  <li><a href="#markdown-idwf3vvygztsqn" id="toc-markdown-idwf3vvygztsqn" class="nav-link" data-scroll-target="#markdown-idwf3vvygztsqn">%% [markdown] id=“wf3vVYGZTsqN”</a></li>
  <li><a href="#section-106" id="toc-section-106" class="nav-link" data-scroll-target="#section-106"></a></li>
  <li><a href="#model-averaging-the-paper-averages-the-last-k-checkpoints-to" id="toc-model-averaging-the-paper-averages-the-last-k-checkpoints-to" class="nav-link" data-scroll-target="#model-averaging-the-paper-averages-the-last-k-checkpoints-to">&gt; 4) Model Averaging: The paper averages the last k checkpoints to</a></li>
  <li><a href="#create-an-ensembling-effect.-we-can-do-this-after-the-fact-if-we" id="toc-create-an-ensembling-effect.-we-can-do-this-after-the-fact-if-we" class="nav-link" data-scroll-target="#create-an-ensembling-effect.-we-can-do-this-after-the-fact-if-we">&gt; create an ensembling effect. We can do this after the fact if we</a></li>
  <li><a href="#have-a-bunch-of-models" id="toc-have-a-bunch-of-models" class="nav-link" data-scroll-target="#have-a-bunch-of-models">&gt; have a bunch of models:</a></li>
  <li><a href="#idhafea78jokdb" id="toc-idhafea78jokdb" class="nav-link" data-scroll-target="#idhafea78jokdb">%% id=“hAFEa78JokDB”</a></li>
  <li><a href="#markdown-idkz5byj9stsqo" id="toc-markdown-idkz5byj9stsqo" class="nav-link" data-scroll-target="#markdown-idkz5byj9stsqo">%% [markdown] id=“Kz5BYJ9sTsqO”</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"># Results</a></li>
  <li><a href="#section-107" id="toc-section-107" class="nav-link" data-scroll-target="#section-107"></a></li>
  <li><a href="#on-the-wmt-2014-english-to-german-translation-task-the-big" id="toc-on-the-wmt-2014-english-to-german-translation-task-the-big" class="nav-link" data-scroll-target="#on-the-wmt-2014-english-to-german-translation-task-the-big">On the WMT 2014 English-to-German translation task, the big</a></li>
  <li><a href="#transformer-model-transformer-big-in-table-2-outperforms-the" id="toc-transformer-model-transformer-big-in-table-2-outperforms-the" class="nav-link" data-scroll-target="#transformer-model-transformer-big-in-table-2-outperforms-the">transformer model (Transformer (big) in Table 2) outperforms the</a></li>
  <li><a href="#best-previously-reported-models-including-ensembles-by-more-than" id="toc-best-previously-reported-models-including-ensembles-by-more-than" class="nav-link" data-scroll-target="#best-previously-reported-models-including-ensembles-by-more-than">best previously reported models (including ensembles) by more than</a></li>
  <li><a href="#bleu-establishing-a-new-state-of-the-art-bleu-score-of" id="toc-bleu-establishing-a-new-state-of-the-art-bleu-score-of" class="nav-link" data-scroll-target="#bleu-establishing-a-new-state-of-the-art-bleu-score-of">2.0 BLEU, establishing a new state-of-the-art BLEU score of</a></li>
  <li><a href="#the-configuration-of-this-model-is-listed-in-the-bottom-line" id="toc-the-configuration-of-this-model-is-listed-in-the-bottom-line" class="nav-link" data-scroll-target="#the-configuration-of-this-model-is-listed-in-the-bottom-line">28.4. The configuration of this model is listed in the bottom line</a></li>
  <li><a href="#of-table-3.-training-took-3.5-days-on-8-p100-gpus.-even-our-base" id="toc-of-table-3.-training-took-3.5-days-on-8-p100-gpus.-even-our-base" class="nav-link" data-scroll-target="#of-table-3.-training-took-3.5-days-on-8-p100-gpus.-even-our-base">of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base</a></li>
  <li><a href="#model-surpasses-all-previously-published-models-and-ensembles-at-a" id="toc-model-surpasses-all-previously-published-models-and-ensembles-at-a" class="nav-link" data-scroll-target="#model-surpasses-all-previously-published-models-and-ensembles-at-a">model surpasses all previously published models and ensembles, at a</a></li>
  <li><a href="#fraction-of-the-training-cost-of-any-of-the-competitive-models." id="toc-fraction-of-the-training-cost-of-any-of-the-competitive-models." class="nav-link" data-scroll-target="#fraction-of-the-training-cost-of-any-of-the-competitive-models.">fraction of the training cost of any of the competitive models.</a></li>
  <li><a href="#section-108" id="toc-section-108" class="nav-link" data-scroll-target="#section-108"></a></li>
  <li><a href="#on-the-wmt-2014-english-to-french-translation-task-our-big-model" id="toc-on-the-wmt-2014-english-to-french-translation-task-our-big-model" class="nav-link" data-scroll-target="#on-the-wmt-2014-english-to-french-translation-task-our-big-model">On the WMT 2014 English-to-French translation task, our big model</a></li>
  <li><a href="#achieves-a-bleu-score-of-41.0-outperforming-all-of-the-previously" id="toc-achieves-a-bleu-score-of-41.0-outperforming-all-of-the-previously" class="nav-link" data-scroll-target="#achieves-a-bleu-score-of-41.0-outperforming-all-of-the-previously">achieves a BLEU score of 41.0, outperforming all of the previously</a></li>
  <li><a href="#published-single-models-at-less-than-14-the-training-cost-of-the" id="toc-published-single-models-at-less-than-14-the-training-cost-of-the" class="nav-link" data-scroll-target="#published-single-models-at-less-than-14-the-training-cost-of-the">published single models, at less than 1/4 the training cost of the</a></li>
  <li><a href="#previous-state-of-the-art-model.-the-transformer-big-model-trained" id="toc-previous-state-of-the-art-model.-the-transformer-big-model-trained" class="nav-link" data-scroll-target="#previous-state-of-the-art-model.-the-transformer-big-model-trained">previous state-of-the-art model. The Transformer (big) model trained</a></li>
  <li><a href="#for-english-to-french-used-dropout-rate-pdrop-0.1-instead-of-0.3." id="toc-for-english-to-french-used-dropout-rate-pdrop-0.1-instead-of-0.3." class="nav-link" data-scroll-target="#for-english-to-french-used-dropout-rate-pdrop-0.1-instead-of-0.3.">for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.</a></li>
  <li><a href="#section-109" id="toc-section-109" class="nav-link" data-scroll-target="#section-109"></a></li>
  <li><a href="#markdown-3" id="toc-markdown-3" class="nav-link" data-scroll-target="#markdown-3">%% [markdown]</a></li>
  <li><a href="#section-110" id="toc-section-110" class="nav-link" data-scroll-target="#section-110"><img src="images/results.png" class="img-fluid"></a></li>
  <li><a href="#markdown-idcpcnshvqtsqo" id="toc-markdown-idcpcnshvqtsqo" class="nav-link" data-scroll-target="#markdown-idcpcnshvqtsqo">%% [markdown] id=“cPcnsHvQTsqO”</a></li>
  <li><a href="#section-111" id="toc-section-111" class="nav-link" data-scroll-target="#section-111"></a></li>
  <li><a href="#section-112" id="toc-section-112" class="nav-link" data-scroll-target="#section-112"></a></li>
  <li><a href="#with-the-addtional-extensions-in-the-last-section-the-opennmt-py" id="toc-with-the-addtional-extensions-in-the-last-section-the-opennmt-py" class="nav-link" data-scroll-target="#with-the-addtional-extensions-in-the-last-section-the-opennmt-py">&gt; With the addtional extensions in the last section, the OpenNMT-py</a></li>
  <li><a href="#replication-gets-to-26.9-on-en-de-wmt.-here-i-have-loaded-in-those" id="toc-replication-gets-to-26.9-on-en-de-wmt.-here-i-have-loaded-in-those" class="nav-link" data-scroll-target="#replication-gets-to-26.9-on-en-de-wmt.-here-i-have-loaded-in-those">&gt; replication gets to 26.9 on EN-DE WMT. Here I have loaded in those</a></li>
  <li><a href="#parameters-to-our-reimplemenation." id="toc-parameters-to-our-reimplemenation." class="nav-link" data-scroll-target="#parameters-to-our-reimplemenation.">&gt; parameters to our reimplemenation.</a></li>
  <li><a href="#section-113" id="toc-section-113" class="nav-link" data-scroll-target="#section-113">%%</a></li>
  <li><a href="#load-data-and-model-for-output-checks" id="toc-load-data-and-model-for-output-checks" class="nav-link" data-scroll-target="#load-data-and-model-for-output-checks">Load data and model for output checks</a></li>
  <li><a href="#section-114" id="toc-section-114" class="nav-link" data-scroll-target="#section-114">%%</a></li>
  <li><a href="#execute_examplerun_model_example" id="toc-execute_examplerun_model_example" class="nav-link" data-scroll-target="#execute_examplerun_model_example">execute_example(run_model_example)</a></li>
  <li><a href="#markdown-id0zkkntkltsqo" id="toc-markdown-id0zkkntkltsqo" class="nav-link" data-scroll-target="#markdown-id0zkkntkltsqo">%% [markdown] id=“0ZkkNTKLTsqO”</a></li>
  <li><a href="#attention-visualization" id="toc-attention-visualization" class="nav-link" data-scroll-target="#attention-visualization">## Attention Visualization</a></li>
  <li><a href="#section-115" id="toc-section-115" class="nav-link" data-scroll-target="#section-115"></a></li>
  <li><a href="#even-with-a-greedy-decoder-the-translation-looks-pretty-good.-we" id="toc-even-with-a-greedy-decoder-the-translation-looks-pretty-good.-we" class="nav-link" data-scroll-target="#even-with-a-greedy-decoder-the-translation-looks-pretty-good.-we">&gt; Even with a greedy decoder the translation looks pretty good. We</a></li>
  <li><a href="#can-further-visualize-it-to-see-what-is-happening-at-each-layer-of" id="toc-can-further-visualize-it-to-see-what-is-happening-at-each-layer-of" class="nav-link" data-scroll-target="#can-further-visualize-it-to-see-what-is-happening-at-each-layer-of">&gt; can further visualize it to see what is happening at each layer of</a></li>
  <li><a href="#the-attention" id="toc-the-attention" class="nav-link" data-scroll-target="#the-attention">&gt; the attention</a></li>
  <li><a href="#section-116" id="toc-section-116" class="nav-link" data-scroll-target="#section-116">%%</a></li>
  <li><a href="#tags-1" id="toc-tags-1" class="nav-link" data-scroll-target="#tags-1">%% tags=[]</a></li>
  <li><a href="#markdown-4" id="toc-markdown-4" class="nav-link" data-scroll-target="#markdown-4">%% [markdown]</a></li>
  <li><a href="#encoder-self-attention" id="toc-encoder-self-attention" class="nav-link" data-scroll-target="#encoder-self-attention">## Encoder Self Attention</a></li>
  <li><a href="#tags-2" id="toc-tags-2" class="nav-link" data-scroll-target="#tags-2">%% tags=[]</a></li>
  <li><a href="#markdown-5" id="toc-markdown-5" class="nav-link" data-scroll-target="#markdown-5">%% [markdown]</a></li>
  <li><a href="#decoder-self-attention" id="toc-decoder-self-attention" class="nav-link" data-scroll-target="#decoder-self-attention">## Decoder Self Attention</a></li>
  <li><a href="#tags-3" id="toc-tags-3" class="nav-link" data-scroll-target="#tags-3">%% tags=[]</a></li>
  <li><a href="#markdown-6" id="toc-markdown-6" class="nav-link" data-scroll-target="#markdown-6">%% [markdown]</a></li>
  <li><a href="#decoder-src-attention" id="toc-decoder-src-attention" class="nav-link" data-scroll-target="#decoder-src-attention">## Decoder Src Attention</a></li>
  <li><a href="#tags-4" id="toc-tags-4" class="nav-link" data-scroll-target="#tags-4">%% tags=[]</a></li>
  <li><a href="#markdown-idnsseuccatsqo" id="toc-markdown-idnsseuccatsqo" class="nav-link" data-scroll-target="#markdown-idnsseuccatsqo">%% [markdown] id=“nSseuCcATsqO”</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"># Conclusion</a></li>
  <li><a href="#section-117" id="toc-section-117" class="nav-link" data-scroll-target="#section-117"></a></li>
  <li><a href="#hopefully-this-code-is-useful-for-future-research.-please-reach" id="toc-hopefully-this-code-is-useful-for-future-research.-please-reach" class="nav-link" data-scroll-target="#hopefully-this-code-is-useful-for-future-research.-please-reach">Hopefully this code is useful for future research. Please reach</a></li>
  <li><a href="#out-if-you-have-any-issues." id="toc-out-if-you-have-any-issues." class="nav-link" data-scroll-target="#out-if-you-have-any-issues.">out if you have any issues.</a></li>
  <li><a href="#section-118" id="toc-section-118" class="nav-link" data-scroll-target="#section-118"></a></li>
  <li><a href="#section-119" id="toc-section-119" class="nav-link" data-scroll-target="#section-119"></a></li>
  <li><a href="#cheers" id="toc-cheers" class="nav-link" data-scroll-target="#cheers">Cheers,</a></li>
  <li><a href="#sasha-rush-austin-huang-suraj-subramanian-jonathan-sum-khalid-almubarak" id="toc-sasha-rush-austin-huang-suraj-subramanian-jonathan-sum-khalid-almubarak" class="nav-link" data-scroll-target="#sasha-rush-austin-huang-suraj-subramanian-jonathan-sum-khalid-almubarak">Sasha Rush, Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak,</a></li>
  <li><a href="#stella-biderman" id="toc-stella-biderman" class="nav-link" data-scroll-target="#stella-biderman">Stella Biderman</a>
  <ul class="collapse">
  <li><a href="#transformer-architecture" id="toc-transformer-architecture" class="nav-link" data-scroll-target="#transformer-architecture">Transformer Architecture</a>
  <ul class="collapse">
  <li><a href="#architecture-details" id="toc-architecture-details" class="nav-link" data-scroll-target="#architecture-details">Architecture Details</a></li>
  </ul></li>
  <li><a href="#attention-1" id="toc-attention-1" class="nav-link" data-scroll-target="#attention-1">Attention</a>
  <ul class="collapse">
  <li><a href="#final-attention-function" id="toc-final-attention-function" class="nav-link" data-scroll-target="#final-attention-function">Final Attention Function</a></li>
  <li><a href="#attention-2" id="toc-attention-2" class="nav-link" data-scroll-target="#attention-2">attention</a></li>
  </ul></li>
  <li><a href="#reading-paper" id="toc-reading-paper" class="nav-link" data-scroll-target="#reading-paper">Reading Paper</a></li>
  <li><a href="#multiheadedattention" id="toc-multiheadedattention" class="nav-link" data-scroll-target="#multiheadedattention">MultiHeadedAttention</a>
  <ul class="collapse">
  <li><a href="#final-multiheaded-attention-class" id="toc-final-multiheaded-attention-class" class="nav-link" data-scroll-target="#final-multiheaded-attention-class">Final MultiHeaded Attention Class</a></li>
  <li><a href="#multiheadedattention-1" id="toc-multiheadedattention-1" class="nav-link" data-scroll-target="#multiheadedattention-1">MultiHeadedAttention</a></li>
  </ul></li>
  <li><a href="#positionwisefeedforward" id="toc-positionwisefeedforward" class="nav-link" data-scroll-target="#positionwisefeedforward">PositionwiseFeedForward</a>
  <ul class="collapse">
  <li><a href="#final-positionwisefeedforward-class" id="toc-final-positionwisefeedforward-class" class="nav-link" data-scroll-target="#final-positionwisefeedforward-class">Final PositionwiseFeedForward Class</a></li>
  <li><a href="#positionwisefeedforward-1" id="toc-positionwisefeedforward-1" class="nav-link" data-scroll-target="#positionwisefeedforward-1">PositionwiseFeedForward</a></li>
  </ul></li>
  <li><a href="#positional-encoding-1" id="toc-positional-encoding-1" class="nav-link" data-scroll-target="#positional-encoding-1">Positional Encoding</a>
  <ul class="collapse">
  <li><a href="#final-positionalencoding" id="toc-final-positionalencoding" class="nav-link" data-scroll-target="#final-positionalencoding">Final PositionalEncoding</a></li>
  <li><a href="#positionalencoding" id="toc-positionalencoding" class="nav-link" data-scroll-target="#positionalencoding">PositionalEncoding</a></li>
  </ul></li>
  <li><a href="#encoder-layer" id="toc-encoder-layer" class="nav-link" data-scroll-target="#encoder-layer">Encoder Layer</a>
  <ul class="collapse">
  <li><a href="#sublayer-connection" id="toc-sublayer-connection" class="nav-link" data-scroll-target="#sublayer-connection">Sublayer Connection</a></li>
  <li><a href="#final-sublayerconnection-function" id="toc-final-sublayerconnection-function" class="nav-link" data-scroll-target="#final-sublayerconnection-function">Final SublayerConnection Function</a></li>
  <li><a href="#sublayerconnection" id="toc-sublayerconnection" class="nav-link" data-scroll-target="#sublayerconnection">SublayerConnection</a></li>
  <li><a href="#final-encoderlayer-function" id="toc-final-encoderlayer-function" class="nav-link" data-scroll-target="#final-encoderlayer-function">Final EncoderLayer Function</a></li>
  <li><a href="#encoderlayer" id="toc-encoderlayer" class="nav-link" data-scroll-target="#encoderlayer">EncoderLayer</a></li>
  <li><a href="#final-clones-function" id="toc-final-clones-function" class="nav-link" data-scroll-target="#final-clones-function">Final clones function</a></li>
  <li><a href="#clones" id="toc-clones" class="nav-link" data-scroll-target="#clones">clones</a></li>
  </ul></li>
  <li><a href="#encoder-class" id="toc-encoder-class" class="nav-link" data-scroll-target="#encoder-class">Encoder Class</a>
  <ul class="collapse">
  <li><a href="#final-encoder-function." id="toc-final-encoder-function." class="nav-link" data-scroll-target="#final-encoder-function.">Final encoder function.</a></li>
  <li><a href="#encoder-1" id="toc-encoder-1" class="nav-link" data-scroll-target="#encoder-1">Encoder</a></li>
  </ul></li>
  <li><a href="#decoder-layer" id="toc-decoder-layer" class="nav-link" data-scroll-target="#decoder-layer">Decoder Layer</a>
  <ul class="collapse">
  <li><a href="#final-decoder-layer-function." id="toc-final-decoder-layer-function." class="nav-link" data-scroll-target="#final-decoder-layer-function.">Final decoder layer Function.</a></li>
  <li><a href="#decoderlayer" id="toc-decoderlayer" class="nav-link" data-scroll-target="#decoderlayer">DecoderLayer</a></li>
  </ul></li>
  <li><a href="#decoder-1" id="toc-decoder-1" class="nav-link" data-scroll-target="#decoder-1">Decoder</a>
  <ul class="collapse">
  <li><a href="#final-decoder-function." id="toc-final-decoder-function." class="nav-link" data-scroll-target="#final-decoder-function.">Final decoder function.</a></li>
  <li><a href="#decoder-2" id="toc-decoder-2" class="nav-link" data-scroll-target="#decoder-2">Decoder</a></li>
  <li><a href="#unit-test-7" id="toc-unit-test-7" class="nav-link" data-scroll-target="#unit-test-7">Unit test</a></li>
  </ul></li>
  <li><a href="#embeddings" id="toc-embeddings" class="nav-link" data-scroll-target="#embeddings">Embeddings</a>
  <ul class="collapse">
  <li><a href="#final-embeddings-model-function" id="toc-final-embeddings-model-function" class="nav-link" data-scroll-target="#final-embeddings-model-function">Final Embeddings Model Function</a></li>
  <li><a href="#embeddings-1" id="toc-embeddings-1" class="nav-link" data-scroll-target="#embeddings-1">Embeddings</a></li>
  <li><a href="#unit-test-8" id="toc-unit-test-8" class="nav-link" data-scroll-target="#unit-test-8">Unit test</a></li>
  </ul></li>
  <li><a href="#generator-class" id="toc-generator-class" class="nav-link" data-scroll-target="#generator-class">Generator class</a>
  <ul class="collapse">
  <li><a href="#final-generator-function." id="toc-final-generator-function." class="nav-link" data-scroll-target="#final-generator-function.">Final generator function.</a></li>
  <li><a href="#generator" id="toc-generator" class="nav-link" data-scroll-target="#generator">Generator</a></li>
  <li><a href="#final-encoder-decoder-function." id="toc-final-encoder-decoder-function." class="nav-link" data-scroll-target="#final-encoder-decoder-function.">Final Encoder Decoder Function.</a></li>
  <li><a href="#encoderdecoder" id="toc-encoderdecoder" class="nav-link" data-scroll-target="#encoderdecoder">EncoderDecoder</a></li>
  </ul></li>
  <li><a href="#make-model" id="toc-make-model" class="nav-link" data-scroll-target="#make-model">Make Model</a>
  <ul class="collapse">
  <li><a href="#make_model" id="toc-make_model" class="nav-link" data-scroll-target="#make_model">make_model</a></li>
  <li><a href="#final-make-model-function" id="toc-final-make-model-function" class="nav-link" data-scroll-target="#final-make-model-function">Final Make Model Function</a></li>
  <li><a href="#make_model-1" id="toc-make_model-1" class="nav-link" data-scroll-target="#make_model-1">make_model</a></li>
  </ul></li>
  <li><a href="#module-hierarchy" id="toc-module-hierarchy" class="nav-link" data-scroll-target="#module-hierarchy">Module Hierarchy</a></li>
  <li><a href="#copy-code-task" id="toc-copy-code-task" class="nav-link" data-scroll-target="#copy-code-task">Copy Code Task</a>
  <ul class="collapse">
  <li><a href="#final-subsequent-mask-function" id="toc-final-subsequent-mask-function" class="nav-link" data-scroll-target="#final-subsequent-mask-function">Final subsequent mask function</a></li>
  <li><a href="#subsequent_mask" id="toc-subsequent_mask" class="nav-link" data-scroll-target="#subsequent_mask">subsequent_mask</a></li>
  <li><a href="#final-batch-class-for-copy-example" id="toc-final-batch-class-for-copy-example" class="nav-link" data-scroll-target="#final-batch-class-for-copy-example">Final Batch Class for copy example</a></li>
  <li><a href="#batch" id="toc-batch" class="nav-link" data-scroll-target="#batch">Batch</a></li>
  <li><a href="#final-data-gen-function" id="toc-final-data-gen-function" class="nav-link" data-scroll-target="#final-data-gen-function">Final Data Gen Function</a></li>
  <li><a href="#data_gen" id="toc-data_gen" class="nav-link" data-scroll-target="#data_gen">data_gen</a></li>
  <li><a href="#final-simple-loss-compute-function" id="toc-final-simple-loss-compute-function" class="nav-link" data-scroll-target="#final-simple-loss-compute-function">Final Simple Loss Compute Function</a></li>
  <li><a href="#simplelosscompute" id="toc-simplelosscompute" class="nav-link" data-scroll-target="#simplelosscompute">SimpleLossCompute</a></li>
  <li><a href="#final-run-epoch-function-for-copy" id="toc-final-run-epoch-function-for-copy" class="nav-link" data-scroll-target="#final-run-epoch-function-for-copy">Final Run Epoch Function for Copy</a></li>
  <li><a href="#run_epoch" id="toc-run_epoch" class="nav-link" data-scroll-target="#run_epoch">run_epoch</a></li>
  <li><a href="#creating-small-copy-model" id="toc-creating-small-copy-model" class="nav-link" data-scroll-target="#creating-small-copy-model">Creating small copy model</a></li>
  <li><a href="#greedy_decode" id="toc-greedy_decode" class="nav-link" data-scroll-target="#greedy_decode">greedy_decode</a></li>
  </ul></li>
  <li><a href="#congratulations" id="toc-congratulations" class="nav-link" data-scroll-target="#congratulations">🎉 Congratulations! 🎉</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mettafore/annotated-transformer/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="attention_is_all_you_need.html.md"><i class="bi bi-file-code"></i>CommonMark</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Simple 2x2 example</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="imports" class="level2">
<h2 class="anchored" data-anchor-id="imports">Imports</h2>
<p>https://arxiv.org/pdf/1706.03762</p>
<p>I will be doing an implementation of the seminal paper “Attention Is All You Need.”</p>
</section>
<section id="coding-utf-8---" class="level1">
<h1>-<em>- coding: utf-8 -</em>-</h1>
</section>
<section id="section" class="level1">
<h1>—</h1>
</section>
<section id="jupyter" class="level1">
<h1>jupyter:</h1>
</section>
<section id="jupytext" class="level1">
<h1>jupytext:</h1>
</section>
<section id="formats-ipynbpypercent" class="level1">
<h1>formats: ipynb,py:percent</h1>
</section>
<section id="text_representation" class="level1">
<h1>text_representation:</h1>
</section>
<section id="extension-.py" class="level1">
<h1>extension: .py</h1>
</section>
<section id="format_name-percent" class="level1">
<h1>format_name: percent</h1>
</section>
<section id="format_version-1.3" class="level1">
<h1>format_version: ‘1.3’</h1>
</section>
<section id="jupytext_version-1.13.0" class="level1">
<h1>jupytext_version: 1.13.0</h1>
</section>
<section id="kernelspec" class="level1">
<h1>kernelspec:</h1>
</section>
<section id="display_name-python-3-ipykernel" class="level1">
<h1>display_name: Python 3 (ipykernel)</h1>
</section>
<section id="language-python" class="level1">
<h1>language: python</h1>
</section>
<section id="name-python3" class="level1">
<h1>name: python3</h1>
</section>
<section id="section-1" class="level1">
<h1>—</h1>
</section>
<section id="markdown-idsx7uc-8jtsp7-tags" class="level1">
<h1>%% [markdown] id=“SX7UC-8jTsp7” tags=[]</h1>
</section>
<section id="section-2" class="level1">
<h1></h1>
#
<center>
<h1>
The Annotated Transformer
</h1>
</center>
</section>
<section id="section-3" class="level1">
<h1></h1>
</section>
<section id="section-4" class="level1">
<h1></h1>
#
<center>
#
<p>
<a href="https://arxiv.org/abs/1706.03762">Attention is All You Need # </a>
</p>
#
</center>
</section>
<section id="section-5" class="level1">
<h1></h1>
</section>
<section id="section-6" class="level1">
<h1><img src="images/aiayn.png" width="70%"></h1>
</section>
<section id="section-7" class="level1">
<h1></h1>
</section>
<section id="v2022-austin-huang-suraj-subramanian-jonathan-sum-khalid-almubarak" class="level1">
<h1>* *v2022: Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak,</h1>
</section>
<section id="and-stella-biderman." class="level1">
<h1>and Stella Biderman.*</h1>
</section>
<section id="original" class="level1">
<h1>* *<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">Original</a>:</h1>
</section>
<section id="sasha-rush." class="level1">
<h1><a href="http://rush-nlp.com/">Sasha Rush</a>.*</h1>
</section>
<section id="section-8" class="level1">
<h1></h1>
</section>
<section id="section-9" class="level1">
<h1></h1>
</section>
<section id="the-transformer-has-been-on-a-lot-of" class="level1">
<h1>The Transformer has been on a lot of</h1>
</section>
<section id="peoples-minds-over-the-last-year-five-years." class="level1">
<h1>people’s minds over the last <s>year</s> five years.</h1>
</section>
<section id="this-post-presents-an-annotated-version-of-the-paper-in-the" class="level1">
<h1>This post presents an annotated version of the paper in the</h1>
</section>
<section id="form-of-a-line-by-line-implementation.-it-reorders-and-deletes" class="level1">
<h1>form of a line-by-line implementation. It reorders and deletes</h1>
</section>
<section id="some-sections-from-the-original-paper-and-adds-comments" class="level1">
<h1>some sections from the original paper and adds comments</h1>
</section>
<section id="throughout.-this-document-itself-is-a-working-notebook-and-should" class="level1">
<h1>throughout. This document itself is a working notebook, and should</h1>
</section>
<section id="be-a-completely-usable-implementation." class="level1">
<h1>be a completely usable implementation.</h1>
</section>
<section id="code-is-available" class="level1">
<h1>Code is available</h1>
</section>
<section id="here." class="level1">
<h1><a href="https://github.com/harvardnlp/annotated-transformer/">here</a>.</h1>
</section>
<section id="section-10" class="level1">
<h1></h1>
</section>
<section id="markdown-idrsntdwkhtsp-" class="level1">
<h1>%% [markdown] id=“RSntDwKhTsp-”</h1>
#
<h3 class="anchored" data-anchor-id="markdown-idrsntdwkhtsp-">
Table of Contents
</h3>
#
<ul>
#
<li>
<a href="#prelims">Prelims</a>
</li>
#
<li>
<a href="#background">Background</a>
</li>
#
<li>
<a href="#part-1-model-architecture">Part 1: Model Architecture</a>
</li>
#
<li>
<a href="#model-architecture">Model Architecture</a>
<ul>
#
<li>
<a href="#encoder-and-decoder-stacks">Encoder and Decoder Stacks</a>
</li>
#
<li>
<a href="#position-wise-feed-forward-networks">Position-wise Feed-Forward # Networks</a>
</li>
#
<li>
<a href="#embeddings-and-softmax">Embeddings and Softmax</a>
</li>
#
<li>
<a href="#positional-encoding">Positional Encoding</a>
</li>
#
<li>
<a href="#full-model">Full Model</a>
</li>
#
<li>
<a href="#inference">Inference:</a>
</li>
#
</ul>
</li>
#
<li>
<a href="#part-2-model-training">Part 2: Model Training</a>
</li>
#
<li>
<a href="#training">Training</a>
<ul>
#
<li>
<a href="#batches-and-masking">Batches and Masking</a>
</li>
#
<li>
<a href="#training-loop">Training Loop</a>
</li>
#
<li>
<a href="#training-data-and-batching">Training Data and Batching</a>
</li>
#
<li>
<a href="#hardware-and-schedule">Hardware and Schedule</a>
</li>
#
<li>
<a href="#optimizer">Optimizer</a>
</li>
#
<li>
<a href="#regularization">Regularization</a>
</li>
#
</ul>
</li>
#
<li>
<a href="#a-first-example">A First Example</a>
<ul>
#
<li>
<a href="#synthetic-data">Synthetic Data</a>
</li>
#
<li>
<a href="#loss-computation">Loss Computation</a>
</li>
#
<li>
<a href="#greedy-decoding">Greedy Decoding</a>
</li>
#
</ul>
</li>
#
<li>
<a href="#part-3-a-real-world-example">Part 3: A Real World Example</a> #
<ul>
#
<li>
<a href="#data-loading">Data Loading</a>
</li>
#
<li>
<a href="#iterators">Iterators</a>
</li>
#
<li>
<a href="#training-the-system">Training the System</a>
</li>
#
</ul>
</li>
#
<li>
<a href="#additional-components-bpe-search-averaging">Additional # Components: BPE, Search, Averaging</a>
</li>
#
<li>
<a href="#results">Results</a>
<ul>
#
<li>
<a href="#attention-visualization">Attention Visualization</a>
</li>
#
<li>
<a href="#encoder-self-attention">Encoder Self Attention</a>
</li>
#
<li>
<a href="#decoder-self-attention">Decoder Self Attention</a>
</li>
#
<li>
<a href="#decoder-src-attention">Decoder Src Attention</a>
</li>
#
</ul>
</li>
#
<li>
<a href="#conclusion">Conclusion</a>
</li>
#
</ul>
</section>
<section id="markdown-idbhmohn9ltsp8" class="level1">
<h1>%% [markdown] id=“BhmOhn9lTsp8”</h1>
</section>
<section id="prelims" class="level1">
<h1># Prelims</h1>
</section>
<section id="section-11" class="level1">
<h1></h1>
</section>
<section id="skip" class="level1">
<h1><a href="#background">Skip</a></h1>
</section>
<section id="idnwclcbh6tsp8" class="level1">
<h1>%% id=“NwClcbH6Tsp8”</h1>
</section>
<section id="pip-install--r-requirements.txt" class="level1">
<h1># !pip install -r requirements.txt</h1>
</section>
<section id="idnwclcbh6tsp8-1" class="level1">
<h1>%% id=“NwClcbH6Tsp8”</h1>
</section>
<section id="uncomment-for-colab" class="level1">
<h1># Uncomment for colab</h1>
</section>
<section id="section-12" class="level1">
<h1></h1>
</section>
<section id="pip-install--q-torchdata0.3.0-torchtext0.12-spacy3.2-altair-gputil" class="level1">
<h1># !pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil</h1>
</section>
<section id="python--m-spacy-download-de_core_news_sm" class="level1">
<h1># !python -m spacy download de_core_news_sm</h1>
</section>
<section id="python--m-spacy-download-en_core_web_sm" class="level1">
<h1># !python -m spacy download en_core_web_sm</h1>
</section>
<section id="idv1-1mx6otsp9" class="level1">
<h1>%% id=“v1-1MX6oTsp9”</h1>
<p>import os from os.path import exists import torch import torch.nn as nn from torch.nn.functional import log_softmax, pad import math import copy import time from torch.optim.lr_scheduler import LambdaLR import pandas as pd import altair as alt from torchtext.data.functional import to_map_style_dataset from torch.utils.data import DataLoader from torchtext.vocab import build_vocab_from_iterator import torchtext.datasets as datasets import spacy import GPUtil import warnings from torch.utils.data.distributed import DistributedSampler import torch.distributed as dist import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP</p>
</section>
<section id="set-to-false-to-skip-notebook-execution-e.g.-for-debugging" class="level1">
<h1>Set to False to skip notebook execution (e.g.&nbsp;for debugging)</h1>
<p>warnings.filterwarnings(“ignore”) RUN_EXAMPLES = True</p>
</section>
<section id="section-13" class="level1">
<h1>%%</h1>
</section>
<section id="some-convenience-helper-functions-used-throughout-the-notebook" class="level1">
<h1>Some convenience helper functions used throughout the notebook</h1>
<p>def is_interactive_notebook(): return <strong>name</strong> == “<strong>main</strong>”</p>
<p>def show_example(fn, args=[]): if <strong>name</strong> == “<strong>main</strong>” and RUN_EXAMPLES: return fn(*args)</p>
<p>def execute_example(fn, args=[]): if <strong>name</strong> == “<strong>main</strong>” and RUN_EXAMPLES: fn(*args)</p>
<p>class DummyOptimizer(torch.optim.Optimizer): def <strong>init</strong>(self): self.param_groups = [{“lr”: 0}] None</p>
<pre><code>def step(self):
    None

def zero_grad(self, set_to_none=False):
    None</code></pre>
<p>class DummyScheduler: def step(self): None</p>
</section>
<section id="markdown-idjx49wryftsp-" class="level1">
<h1>%% [markdown] id=“jx49WRyfTsp-”</h1>
</section>
<section id="my-comments-are-blockquoted.-the-main-text-is-all-from-the-paper-itself." class="level1">
<h1>&gt; My comments are blockquoted. The main text is all from the paper itself.</h1>
</section>
<section id="markdown-id7phvewghtsp_" class="level1">
<h1>%% [markdown] id=“7phVeWghTsp_”</h1>
</section>
<section id="background" class="level1">
<h1># Background</h1>
</section>
<section id="markdown-id83zds91dtsqa" class="level1">
<h1>%% [markdown] id=“83ZDS91dTsqA”</h1>
</section>
<section id="section-14" class="level1">
<h1></h1>
</section>
<section id="the-goal-of-reducing-sequential-computation-also-forms-the" class="level1">
<h1>The goal of reducing sequential computation also forms the</h1>
</section>
<section id="foundation-of-the-extended-neural-gpu-bytenet-and-convs2s-all-of" class="level1">
<h1>foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of</h1>
</section>
<section id="which-use-convolutional-neural-networks-as-basic-building-block" class="level1">
<h1>which use convolutional neural networks as basic building block,</h1>
</section>
<section id="computing-hidden-representations-in-parallel-for-all-input-and" class="level1">
<h1>computing hidden representations in parallel for all input and</h1>
</section>
<section id="output-positions.-in-these-models-the-number-of-operations-required" class="level1">
<h1>output positions. In these models, the number of operations required</h1>
</section>
<section id="to-relate-signals-from-two-arbitrary-input-or-output-positions-grows" class="level1">
<h1>to relate signals from two arbitrary input or output positions grows</h1>
</section>
<section id="in-the-distance-between-positions-linearly-for-convs2s-and" class="level1">
<h1>in the distance between positions, linearly for ConvS2S and</h1>
</section>
<section id="logarithmically-for-bytenet.-this-makes-it-more-difficult-to-learn" class="level1">
<h1>logarithmically for ByteNet. This makes it more difficult to learn</h1>
</section>
<section id="dependencies-between-distant-positions.-in-the-transformer-this-is" class="level1">
<h1>dependencies between distant positions. In the Transformer this is</h1>
</section>
<section id="reduced-to-a-constant-number-of-operations-albeit-at-the-cost-of" class="level1">
<h1>reduced to a constant number of operations, albeit at the cost of</h1>
</section>
<section id="reduced-effective-resolution-due-to-averaging-attention-weighted" class="level1">
<h1>reduced effective resolution due to averaging attention-weighted</h1>
</section>
<section id="positions-an-effect-we-counteract-with-multi-head-attention." class="level1">
<h1>positions, an effect we counteract with Multi-Head Attention.</h1>
</section>
<section id="section-15" class="level1">
<h1></h1>
</section>
<section id="self-attention-sometimes-called-intra-attention-is-an-attention" class="level1">
<h1>Self-attention, sometimes called intra-attention is an attention</h1>
</section>
<section id="mechanism-relating-different-positions-of-a-single-sequence-in-order" class="level1">
<h1>mechanism relating different positions of a single sequence in order</h1>
</section>
<section id="to-compute-a-representation-of-the-sequence.-self-attention-has-been" class="level1">
<h1>to compute a representation of the sequence. Self-attention has been</h1>
</section>
<section id="used-successfully-in-a-variety-of-tasks-including-reading" class="level1">
<h1>used successfully in a variety of tasks including reading</h1>
</section>
<section id="comprehension-abstractive-summarization-textual-entailment-and" class="level1">
<h1>comprehension, abstractive summarization, textual entailment and</h1>
</section>
<section id="learning-task-independent-sentence-representations.-end-to-end" class="level1">
<h1>learning task-independent sentence representations. End-to-end</h1>
</section>
<section id="memory-networks-are-based-on-a-recurrent-attention-mechanism-instead" class="level1">
<h1>memory networks are based on a recurrent attention mechanism instead</h1>
</section>
<section id="of-sequencealigned-recurrence-and-have-been-shown-to-perform-well-on" class="level1">
<h1>of sequencealigned recurrence and have been shown to perform well on</h1>
</section>
<section id="simple-language-question-answering-and-language-modeling-tasks." class="level1">
<h1>simple-language question answering and language modeling tasks.</h1>
</section>
<section id="section-16" class="level1">
<h1></h1>
</section>
<section id="to-the-best-of-our-knowledge-however-the-transformer-is-the-first" class="level1">
<h1>To the best of our knowledge, however, the Transformer is the first</h1>
</section>
<section id="transduction-model-relying-entirely-on-self-attention-to-compute" class="level1">
<h1>transduction model relying entirely on self-attention to compute</h1>
</section>
<section id="representations-of-its-input-and-output-without-using-sequence" class="level1">
<h1>representations of its input and output without using sequence</h1>
</section>
<section id="aligned-rnns-or-convolution." class="level1">
<h1>aligned RNNs or convolution.</h1>
</section>
<section id="markdown" class="level1">
<h1>%% [markdown]</h1>
</section>
<section id="part-1-model-architecture" class="level1">
<h1># Part 1: Model Architecture</h1>
</section>
<section id="markdown-idpfrpajeztsqb" class="level1">
<h1>%% [markdown] id=“pFrPajezTsqB”</h1>
</section>
<section id="model-architecture" class="level1">
<h1># Model Architecture</h1>
</section>
<section id="markdown-idreuu_h-ftsqb" class="level1">
<h1>%% [markdown] id=“ReuU_h-fTsqB”</h1>
</section>
<section id="section-17" class="level1">
<h1></h1>
</section>
<section id="most-competitive-neural-sequence-transduction-models-have-an" class="level1">
<h1>Most competitive neural sequence transduction models have an</h1>
</section>
<section id="encoder-decoder-structure" class="level1">
<h1>encoder-decoder structure</h1>
</section>
<section id="cite.-here-the-encoder-maps-an" class="level1">
<h1><a href="https://arxiv.org/abs/1409.0473">(cite)</a>. Here, the encoder maps an</h1>
</section>
<section id="input-sequence-of-symbol-representations-x_1-...-x_n-to-a" class="level1">
<h1>input sequence of symbol representations <span class="math inline">\((x_1, ..., x_n)\)</span> to a</h1>
</section>
<section id="sequence-of-continuous-representations-mathbfz-z_1-...-z_n.-given-mathbfz-the-decoder-then-generates-an-output" class="level1">
<h1>sequence of continuous representations <span class="math inline">\(\mathbf{z} = (z_1, ...,
# z_n)\)</span>. Given <span class="math inline">\(\mathbf{z}\)</span>, the decoder then generates an output</h1>
</section>
<section id="sequence-y_1...y_m-of-symbols-one-element-at-a-time.-at-each" class="level1">
<h1>sequence <span class="math inline">\((y_1,...,y_m)\)</span> of symbols one element at a time. At each</h1>
</section>
<section id="step-the-model-is-auto-regressive" class="level1">
<h1>step the model is auto-regressive</h1>
</section>
<section id="cite-consuming-the-previously" class="level1">
<h1><a href="https://arxiv.org/abs/1308.0850">(cite)</a>, consuming the previously</h1>
</section>
<section id="generated-symbols-as-additional-input-when-generating-the-next." class="level1">
<h1>generated symbols as additional input when generating the next.</h1>
</section>
<section id="idk0xgxhzrtsqb" class="level1">
<h1>%% id=“k0XGXhzRTsqB”</h1>
<p>class EncoderDecoder(nn.Module): ““” A standard Encoder-Decoder architecture. Base for this and many other models. ““”</p>
<pre><code>def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
    super(EncoderDecoder, self).__init__()
    self.encoder = encoder
    self.decoder = decoder
    self.src_embed = src_embed
    self.tgt_embed = tgt_embed
    self.generator = generator

def forward(self, src, tgt, src_mask, tgt_mask):
    "Take in and process masked src and target sequences."
    return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)

def encode(self, src, src_mask):
    return self.encoder(self.src_embed(src), src_mask)

def decode(self, memory, src_mask, tgt, tgt_mask):
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</code></pre>
</section>
<section id="idnkgoh2rstsqc" class="level1">
<h1>%% id=“NKGoH2RsTsqC”</h1>
<p>class Generator(nn.Module): “Define standard linear + softmax generation step.”</p>
<pre><code>def __init__(self, d_model, vocab):
    super(Generator, self).__init__()
    self.proj = nn.Linear(d_model, vocab)

def forward(self, x):
    return log_softmax(self.proj(x), dim=-1)</code></pre>
</section>
<section id="markdown-idmooenf_jtsqc" class="level1">
<h1>%% [markdown] id=“mOoEnF_jTsqC”</h1>
</section>
<section id="section-18" class="level1">
<h1></h1>
</section>
<section id="the-transformer-follows-this-overall-architecture-using-stacked" class="level1">
<h1>The Transformer follows this overall architecture using stacked</h1>
</section>
<section id="self-attention-and-point-wise-fully-connected-layers-for-both-the" class="level1">
<h1>self-attention and point-wise, fully connected layers for both the</h1>
</section>
<section id="encoder-and-decoder-shown-in-the-left-and-right-halves-of-figure-1" class="level1">
<h1>encoder and decoder, shown in the left and right halves of Figure 1,</h1>
</section>
<section id="respectively." class="level1">
<h1>respectively.</h1>
</section>
<section id="markdown-idoredwloytsqc" class="level1">
<h1>%% [markdown] id=“oredWloYTsqC”</h1>
</section>
<section id="section-19" class="level1">
<h1><img src="images/ModalNet-21.png" class="img-fluid"></h1>
</section>
<section id="markdown-idbh092nzbtsqd" class="level1">
<h1>%% [markdown] id=“bh092NZBTsqD”</h1>
</section>
<section id="encoder-and-decoder-stacks" class="level1">
<h1>## Encoder and Decoder Stacks</h1>
</section>
<section id="section-20" class="level1">
<h1></h1>
</section>
<section id="encoder" class="level1">
<h1>### Encoder</h1>
</section>
<section id="section-21" class="level1">
<h1></h1>
</section>
<section id="the-encoder-is-composed-of-a-stack-of-n6-identical-layers." class="level1">
<h1>The encoder is composed of a stack of <span class="math inline">\(N=6\)</span> identical layers.</h1>
</section>
<section id="id2gxtapuytsqd" class="level1">
<h1>%% id=“2gxTApUYTsqD”</h1>
<p>def clones(module, N): “Produce N identical layers.” return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])</p>
</section>
<section id="idxqvtz9mktsqd" class="level1">
<h1>%% id=“xqVTz9MkTsqD”</h1>
<p>class Encoder(nn.Module): “Core encoder is a stack of N layers”</p>
<pre><code>def __init__(self, layer, N):
    super(Encoder, self).__init__()
    self.layers = clones(layer, N)
    self.norm = LayerNorm(layer.size)

def forward(self, x, mask):
    "Pass the input (and mask) through each layer in turn."
    for layer in self.layers:
        x = layer(x, mask)
    return self.norm(x)</code></pre>
</section>
<section id="markdown-idgjakgjgwtsqd" class="level1">
<h1>%% [markdown] id=“GjAKgjGwTsqD”</h1>
</section>
<section id="section-22" class="level1">
<h1></h1>
</section>
<section id="we-employ-a-residual-connection" class="level1">
<h1>We employ a residual connection</h1>
</section>
<section id="cite-around-each-of-the-two" class="level1">
<h1><a href="https://arxiv.org/abs/1512.03385">(cite)</a> around each of the two</h1>
</section>
<section id="sub-layers-followed-by-layer-normalization" class="level1">
<h1>sub-layers, followed by layer normalization</h1>
</section>
<section id="cite." class="level1">
<h1><a href="https://arxiv.org/abs/1607.06450">(cite)</a>.</h1>
</section>
<section id="id3jka_prztsqe" class="level1">
<h1>%% id=“3jKa_prZTsqE”</h1>
<p>class LayerNorm(nn.Module): “Construct a layernorm module (See citation for details).”</p>
<pre><code>def __init__(self, features, eps=1e-6):
    super(LayerNorm, self).__init__()
    self.a_2 = nn.Parameter(torch.ones(features))
    self.b_2 = nn.Parameter(torch.zeros(features))
    self.eps = eps

def forward(self, x):
    mean = x.mean(-1, keepdim=True)
    std = x.std(-1, keepdim=True)
    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2</code></pre>
</section>
<section id="markdown-idnxsj3qymtsqe" class="level1">
<h1>%% [markdown] id=“nXSJ3QYmTsqE”</h1>
</section>
<section id="section-23" class="level1">
<h1></h1>
</section>
<section id="that-is-the-output-of-each-sub-layer-is-mathrmlayernormx-mathrmsublayerx-where-mathrmsublayerx-is-the-function" class="level1">
<h1>That is, the output of each sub-layer is <span class="math inline">\(\mathrm{LayerNorm}(x +
# \mathrm{Sublayer}(x))\)</span>, where <span class="math inline">\(\mathrm{Sublayer}(x)\)</span> is the function</h1>
</section>
<section id="implemented-by-the-sub-layer-itself.-we-apply-dropout" class="level1">
<h1>implemented by the sub-layer itself. We apply dropout</h1>
</section>
<section id="cite-to-the" class="level1">
<h1><a href="http://jmlr.org/papers/v15/srivastava14a.html">(cite)</a> to the</h1>
</section>
<section id="output-of-each-sub-layer-before-it-is-added-to-the-sub-layer-input" class="level1">
<h1>output of each sub-layer, before it is added to the sub-layer input</h1>
</section>
<section id="and-normalized." class="level1">
<h1>and normalized.</h1>
</section>
<section id="section-24" class="level1">
<h1></h1>
</section>
<section id="to-facilitate-these-residual-connections-all-sub-layers-in-the" class="level1">
<h1>To facilitate these residual connections, all sub-layers in the</h1>
</section>
<section id="model-as-well-as-the-embedding-layers-produce-outputs-of-dimension" class="level1">
<h1>model, as well as the embedding layers, produce outputs of dimension</h1>
</section>
<section id="d_textmodel512." class="level1">
<h1><span class="math inline">\(d_{\text{model}}=512\)</span>.</h1>
</section>
<section id="idu1p7zi0etsqe" class="level1">
<h1>%% id=“U1P7zI0eTsqE”</h1>
<p>class SublayerConnection(nn.Module): ““” A residual connection followed by a layer norm. Note for code simplicity the norm is first as opposed to last. ““”</p>
<pre><code>def __init__(self, size, dropout):
    super(SublayerConnection, self).__init__()
    self.norm = LayerNorm(size)
    self.dropout = nn.Dropout(dropout)

def forward(self, x, sublayer):
    "Apply residual connection to any sublayer with the same size."
    return x + self.dropout(sublayer(self.norm(x)))</code></pre>
</section>
<section id="markdown-idml6odleqtsqe" class="level1">
<h1>%% [markdown] id=“ML6oDlEqTsqE”</h1>
</section>
<section id="section-25" class="level1">
<h1></h1>
</section>
<section id="each-layer-has-two-sub-layers.-the-first-is-a-multi-head" class="level1">
<h1>Each layer has two sub-layers. The first is a multi-head</h1>
</section>
<section id="self-attention-mechanism-and-the-second-is-a-simple-position-wise" class="level1">
<h1>self-attention mechanism, and the second is a simple, position-wise</h1>
</section>
<section id="fully-connected-feed-forward-network." class="level1">
<h1>fully connected feed-forward network.</h1>
</section>
<section id="idqykufr6gtsqe" class="level1">
<h1>%% id=“qYkUFr6GTsqE”</h1>
<p>class EncoderLayer(nn.Module): “Encoder is made up of self-attn and feed forward (defined below)”</p>
<pre><code>def __init__(self, size, self_attn, feed_forward, dropout):
    super(EncoderLayer, self).__init__()
    self.self_attn = self_attn
    self.feed_forward = feed_forward
    self.sublayer = clones(SublayerConnection(size, dropout), 2)
    self.size = size

def forward(self, x, mask):
    "Follow Figure 1 (left) for connections."
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
    return self.sublayer[1](x, self.feed_forward)</code></pre>
</section>
<section id="markdown-id7ecoqihktsqf" class="level1">
<h1>%% [markdown] id=“7ecOQIhkTsqF”</h1>
</section>
<section id="decoder" class="level1">
<h1>### Decoder</h1>
</section>
<section id="section-26" class="level1">
<h1></h1>
</section>
<section id="the-decoder-is-also-composed-of-a-stack-of-n6-identical-layers." class="level1">
<h1>The decoder is also composed of a stack of <span class="math inline">\(N=6\)</span> identical layers.</h1>
</section>
<section id="section-27" class="level1">
<h1></h1>
</section>
<section id="section-28" class="level1">
<h1>%%</h1>
<p>class Decoder(nn.Module): “Generic N layer decoder with masking.”</p>
<pre><code>def __init__(self, layer, N):
    super(Decoder, self).__init__()
    self.layers = clones(layer, N)
    self.norm = LayerNorm(layer.size)

def forward(self, x, memory, src_mask, tgt_mask):
    for layer in self.layers:
        x = layer(x, memory, src_mask, tgt_mask)
    return self.norm(x)</code></pre>
</section>
<section id="markdown-iddxlcb12ptsqf" class="level1">
<h1>%% [markdown] id=“dXlCB12pTsqF”</h1>
</section>
<section id="section-29" class="level1">
<h1></h1>
</section>
<section id="in-addition-to-the-two-sub-layers-in-each-encoder-layer-the-decoder" class="level1">
<h1>In addition to the two sub-layers in each encoder layer, the decoder</h1>
</section>
<section id="inserts-a-third-sub-layer-which-performs-multi-head-attention-over" class="level1">
<h1>inserts a third sub-layer, which performs multi-head attention over</h1>
</section>
<section id="the-output-of-the-encoder-stack.-similar-to-the-encoder-we-employ" class="level1">
<h1>the output of the encoder stack. Similar to the encoder, we employ</h1>
</section>
<section id="residual-connections-around-each-of-the-sub-layers-followed-by" class="level1">
<h1>residual connections around each of the sub-layers, followed by</h1>
</section>
<section id="layer-normalization." class="level1">
<h1>layer normalization.</h1>
</section>
<section id="idm2ha1xfqtsqf" class="level1">
<h1>%% id=“M2hA1xFQTsqF”</h1>
<p>class DecoderLayer(nn.Module): “Decoder is made of self-attn, src-attn, and feed forward (defined below)”</p>
<pre><code>def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
    super(DecoderLayer, self).__init__()
    self.size = size
    self.self_attn = self_attn
    self.src_attn = src_attn
    self.feed_forward = feed_forward
    self.sublayer = clones(SublayerConnection(size, dropout), 3)

def forward(self, x, memory, src_mask, tgt_mask):
    "Follow Figure 1 (right) for connections."
    m = memory
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
    x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
    return self.sublayer[2](x, self.feed_forward)</code></pre>
</section>
<section id="markdown-idfzz5rll4tsqf" class="level1">
<h1>%% [markdown] id=“FZz5rLl4TsqF”</h1>
</section>
<section id="section-30" class="level1">
<h1></h1>
</section>
<section id="we-also-modify-the-self-attention-sub-layer-in-the-decoder-stack-to" class="level1">
<h1>We also modify the self-attention sub-layer in the decoder stack to</h1>
</section>
<section id="prevent-positions-from-attending-to-subsequent-positions.-this" class="level1">
<h1>prevent positions from attending to subsequent positions. This</h1>
</section>
<section id="masking-combined-with-fact-that-the-output-embeddings-are-offset-by" class="level1">
<h1>masking, combined with fact that the output embeddings are offset by</h1>
</section>
<section id="one-position-ensures-that-the-predictions-for-position-i-can" class="level1">
<h1>one position, ensures that the predictions for position <span class="math inline">\(i\)</span> can</h1>
</section>
<section id="depend-only-on-the-known-outputs-at-positions-less-than-i." class="level1">
<h1>depend only on the known outputs at positions less than <span class="math inline">\(i\)</span>.</h1>
</section>
<section id="idqn98o2l3tsqf" class="level1">
<h1>%% id=“QN98O2l3TsqF”</h1>
<p>def subsequent_mask(size): “Mask out subsequent positions.” attn_shape = (1, size, size) subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type( torch.uint8 ) return subsequent_mask == 0</p>
</section>
<section id="markdown-idvg_f_w-ptsqg" class="level1">
<h1>%% [markdown] id=“Vg_f_w-PTsqG”</h1>
</section>
<section id="section-31" class="level1">
<h1></h1>
</section>
<section id="below-the-attention-mask-shows-the-position-each-tgt-word-row-is" class="level1">
<h1>&gt; Below the attention mask shows the position each tgt word (row) is</h1>
</section>
<section id="allowed-to-look-at-column.-words-are-blocked-for-attending-to" class="level1">
<h1>&gt; allowed to look at (column). Words are blocked for attending to</h1>
</section>
<section id="future-words-during-training." class="level1">
<h1>&gt; future words during training.</h1>
</section>
<section id="idht_ftgyaokc4" class="level1">
<h1>%% id=“ht_FtgYAokC4”</h1>
<p>def example_mask(): LS_data = pd.concat( [ pd.DataFrame( { “Subsequent Mask”: subsequent_mask(20)[0][x, y].flatten(), “Window”: y, “Masking”: x, } ) for y in range(20) for x in range(20) ] )</p>
<pre><code>return (
    alt.Chart(LS_data)
    .mark_rect()
    .properties(height=250, width=250)
    .encode(
        alt.X("Window:O"),
        alt.Y("Masking:O"),
        alt.Color("Subsequent Mask:Q", scale=alt.Scale(scheme="viridis")),
    )
    .interactive()
)</code></pre>
<p>show_example(example_mask)</p>
</section>
<section id="markdown-idqto_yg7btsqg" class="level1">
<h1>%% [markdown] id=“Qto_yg7BTsqG”</h1>
</section>
<section id="attention" class="level1">
<h1>### Attention</h1>
</section>
<section id="section-32" class="level1">
<h1></h1>
</section>
<section id="an-attention-function-can-be-described-as-mapping-a-query-and-a-set" class="level1">
<h1>An attention function can be described as mapping a query and a set</h1>
</section>
<section id="of-key-value-pairs-to-an-output-where-the-query-keys-values-and" class="level1">
<h1>of key-value pairs to an output, where the query, keys, values, and</h1>
</section>
<section id="output-are-all-vectors.-the-output-is-computed-as-a-weighted-sum-of" class="level1">
<h1>output are all vectors. The output is computed as a weighted sum of</h1>
</section>
<section id="the-values-where-the-weight-assigned-to-each-value-is-computed-by-a" class="level1">
<h1>the values, where the weight assigned to each value is computed by a</h1>
</section>
<section id="compatibility-function-of-the-query-with-the-corresponding-key." class="level1">
<h1>compatibility function of the query with the corresponding key.</h1>
</section>
<section id="section-33" class="level1">
<h1></h1>
</section>
<section id="we-call-our-particular-attention-scaled-dot-product-attention." class="level1">
<h1>We call our particular attention “Scaled Dot-Product Attention”.</h1>
</section>
<section id="the-input-consists-of-queries-and-keys-of-dimension-d_k-and" class="level1">
<h1>The input consists of queries and keys of dimension <span class="math inline">\(d_k\)</span>, and</h1>
</section>
<section id="values-of-dimension-d_v.-we-compute-the-dot-products-of-the-query" class="level1">
<h1>values of dimension <span class="math inline">\(d_v\)</span>. We compute the dot products of the query</h1>
</section>
<section id="with-all-keys-divide-each-by-sqrtd_k-and-apply-a-softmax" class="level1">
<h1>with all keys, divide each by <span class="math inline">\(\sqrt{d_k}\)</span>, and apply a softmax</h1>
</section>
<section id="function-to-obtain-the-weights-on-the-values." class="level1">
<h1>function to obtain the weights on the values.</h1>
</section>
<section id="section-34" class="level1">
<h1></h1>
</section>
<section id="section-35" class="level1">
<h1></h1>
</section>
<section id="section-36" class="level1">
<h1></h1>
</section>
<section id="section-37" class="level1">
<h1><img src="images/ModalNet-19.png" class="img-fluid"></h1>
</section>
<section id="markdown-ideyjlwk6ctsqg" class="level1">
<h1>%% [markdown] id=“EYJLWk6cTsqG”</h1>
</section>
<section id="section-38" class="level1">
<h1></h1>
</section>
<section id="in-practice-we-compute-the-attention-function-on-a-set-of-queries" class="level1">
<h1>In practice, we compute the attention function on a set of queries</h1>
</section>
<section id="simultaneously-packed-together-into-a-matrix-q.-the-keys-and" class="level1">
<h1>simultaneously, packed together into a matrix <span class="math inline">\(Q\)</span>. The keys and</h1>
</section>
<section id="values-are-also-packed-together-into-matrices-k-and-v.-we" class="level1">
<h1>values are also packed together into matrices <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span>. We</h1>
</section>
<section id="compute-the-matrix-of-outputs-as" class="level1">
<h1>compute the matrix of outputs as:</h1>
</section>
<section id="section-39" class="level1">
<h1></h1>
</section>
<section id="mathrmattentionq-k-v-mathrmsoftmaxfracqktsqrtd_kv" class="level1">
<h1><span class="math display">\[
#    \mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
# \]</span></h1>
</section>
<section id="idqsovxs5ytsqg" class="level1">
<h1>%% id=“qsoVxS5yTsqG”</h1>
<p>def attention(query, key, value, mask=None, dropout=None): “Compute ‘Scaled Dot Product Attention’” d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = scores.softmax(dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn</p>
</section>
<section id="markdown-idjukpwu8ktsqg" class="level1">
<h1>%% [markdown] id=“jUkpwu8kTsqG”</h1>
</section>
<section id="section-40" class="level1">
<h1></h1>
</section>
<section id="the-two-most-commonly-used-attention-functions-are-additive" class="level1">
<h1>The two most commonly used attention functions are additive</h1>
</section>
<section id="attention-cite-and-dot-product" class="level1">
<h1>attention <a href="https://arxiv.org/abs/1409.0473">(cite)</a>, and dot-product</h1>
</section>
<section id="multiplicative-attention.-dot-product-attention-is-identical-to" class="level1">
<h1>(multiplicative) attention. Dot-product attention is identical to</h1>
</section>
<section id="our-algorithm-except-for-the-scaling-factor-of" class="level1">
<h1>our algorithm, except for the scaling factor of</h1>
</section>
<section id="frac1sqrtd_k.-additive-attention-computes-the" class="level1">
<h1><span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>. Additive attention computes the</h1>
</section>
<section id="compatibility-function-using-a-feed-forward-network-with-a-single" class="level1">
<h1>compatibility function using a feed-forward network with a single</h1>
</section>
<section id="hidden-layer.-while-the-two-are-similar-in-theoretical-complexity" class="level1">
<h1>hidden layer. While the two are similar in theoretical complexity,</h1>
</section>
<section id="dot-product-attention-is-much-faster-and-more-space-efficient-in" class="level1">
<h1>dot-product attention is much faster and more space-efficient in</h1>
</section>
<section id="practice-since-it-can-be-implemented-using-highly-optimized-matrix" class="level1">
<h1>practice, since it can be implemented using highly optimized matrix</h1>
</section>
<section id="multiplication-code." class="level1">
<h1>multiplication code.</h1>
</section>
<section id="section-41" class="level1">
<h1></h1>
</section>
<section id="section-42" class="level1">
<h1></h1>
</section>
<section id="while-for-small-values-of-d_k-the-two-mechanisms-perform" class="level1">
<h1>While for small values of <span class="math inline">\(d_k\)</span> the two mechanisms perform</h1>
</section>
<section id="similarly-additive-attention-outperforms-dot-product-attention" class="level1">
<h1>similarly, additive attention outperforms dot product attention</h1>
</section>
<section id="without-scaling-for-larger-values-of-d_k" class="level1">
<h1>without scaling for larger values of <span class="math inline">\(d_k\)</span></h1>
</section>
<section id="cite.-we-suspect-that-for" class="level1">
<h1><a href="https://arxiv.org/abs/1703.03906">(cite)</a>. We suspect that for</h1>
</section>
<section id="large-values-of-d_k-the-dot-products-grow-large-in-magnitude" class="level1">
<h1>large values of <span class="math inline">\(d_k\)</span>, the dot products grow large in magnitude,</h1>
</section>
<section id="pushing-the-softmax-function-into-regions-where-it-has-extremely" class="level1">
<h1>pushing the softmax function into regions where it has extremely</h1>
</section>
<section id="small-gradients-to-illustrate-why-the-dot-products-get-large" class="level1">
<h1>small gradients (To illustrate why the dot products get large,</h1>
</section>
<section id="assume-that-the-components-of-q-and-k-are-independent-random" class="level1">
<h1>assume that the components of <span class="math inline">\(q\)</span> and <span class="math inline">\(k\)</span> are independent random</h1>
</section>
<section id="variables-with-mean-0-and-variance-1.-then-their-dot-product" class="level1">
<h1>variables with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>. Then their dot product,</h1>
</section>
<section id="q-cdot-k-sum_i1d_k-q_ik_i-has-mean-0-and-variance" class="level1">
<h1><span class="math inline">\(q \cdot k = \sum_{i=1}^{d_k} q_ik_i\)</span>, has mean <span class="math inline">\(0\)</span> and variance</h1>
</section>
<section id="d_k..-to-counteract-this-effect-we-scale-the-dot-products-by" class="level1">
<h1><span class="math inline">\(d_k\)</span>.). To counteract this effect, we scale the dot products by</h1>
</section>
<section id="frac1sqrtd_k." class="level1">
<h1><span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>.</h1>
</section>
<section id="section-43" class="level1">
<h1></h1>
</section>
<section id="section-44" class="level1">
<h1></h1>
</section>
<section id="markdown-idbs1fszhvtsqg" class="level1">
<h1>%% [markdown] id=“bS1FszhVTsqG”</h1>
</section>
<section id="section-45" class="level1">
<h1><img src="images/ModalNet-20.png" class="img-fluid"></h1>
</section>
<section id="markdown-idtntvyz-ptsqh" class="level1">
<h1>%% [markdown] id=“TNtVyZ-pTsqH”</h1>
</section>
<section id="section-46" class="level1">
<h1></h1>
</section>
<section id="multi-head-attention-allows-the-model-to-jointly-attend-to" class="level1">
<h1>Multi-head attention allows the model to jointly attend to</h1>
</section>
<section id="information-from-different-representation-subspaces-at-different" class="level1">
<h1>information from different representation subspaces at different</h1>
</section>
<section id="positions.-with-a-single-attention-head-averaging-inhibits-this." class="level1">
<h1>positions. With a single attention head, averaging inhibits this.</h1>
</section>
<section id="section-47" class="level1">
<h1></h1>
</section>
<section id="mathrmmultiheadq-k-v-mathrmconcatmathrmhead_1-...-mathrmhead_hwo-textwheremathrmhead_i-mathrmattentionqwq_i-kwk_i-vwv_i" class="level1">
<h1><span class="math display">\[
# \mathrm{MultiHead}(Q, K, V) =
#     \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \\
#     \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)
# \]</span></h1>
</section>
<section id="section-48" class="level1">
<h1></h1>
</section>
<section id="where-the-projections-are-parameter-matrices-wq_i-in-mathbbrd_textmodel-times-d_k-wk_i-in-mathbbrd_textmodel-times-d_k-wv_i-in-mathbbrd_textmodel-times-d_v-and-wo-in-mathbbrhd_v-times-d_textmodel." class="level1">
<h1>Where the projections are parameter matrices <span class="math inline">\(W^Q_i \in
# \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>, <span class="math inline">\(W^K_i \in
# \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>, <span class="math inline">\(W^V_i \in
# \mathbb{R}^{d_{\text{model}} \times d_v}\)</span> and <span class="math inline">\(W^O \in
# \mathbb{R}^{hd_v \times d_{\text{model}}}\)</span>.</h1>
</section>
<section id="section-49" class="level1">
<h1></h1>
</section>
<section id="in-this-work-we-employ-h8-parallel-attention-layers-or" class="level1">
<h1>In this work we employ <span class="math inline">\(h=8\)</span> parallel attention layers, or</h1>
</section>
<section id="heads.-for-each-of-these-we-use-d_kd_vd_textmodelh64.-due" class="level1">
<h1>heads. For each of these we use <span class="math inline">\(d_k=d_v=d_{\text{model}}/h=64\)</span>. Due</h1>
</section>
<section id="to-the-reduced-dimension-of-each-head-the-total-computational-cost" class="level1">
<h1>to the reduced dimension of each head, the total computational cost</h1>
</section>
<section id="is-similar-to-that-of-single-head-attention-with-full" class="level1">
<h1>is similar to that of single-head attention with full</h1>
</section>
<section id="dimensionality." class="level1">
<h1>dimensionality.</h1>
</section>
<section id="idd2lbmkcqtsqh" class="level1">
<h1>%% id=“D2LBMKCQTsqH”</h1>
<p>class MultiHeadedAttention(nn.Module): def <strong>init</strong>(self, h, d_model, dropout=0.1): “Take in model size and number of heads.” super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h self.h = h self.linears = clones(nn.Linear(d_model, d_model), 4) self.attn = None self.dropout = nn.Dropout(p=dropout)</p>
<pre><code>def forward(self, query, key, value, mask=None):
    "Implements Figure 2"
    if mask is not None:
        # Same mask applied to all h heads.
        mask = mask.unsqueeze(1)
    nbatches = query.size(0)

    # 1) Do all the linear projections in batch from d_model =&gt; h x d_k
    query, key, value = [
        lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
        for lin, x in zip(self.linears, (query, key, value))
    ]

    # 2) Apply attention on all the projected vectors in batch.
    x, self.attn = attention(
        query, key, value, mask=mask, dropout=self.dropout
    )

    # 3) "Concat" using a view and apply a final linear.
    x = (
        x.transpose(1, 2)
        .contiguous()
        .view(nbatches, -1, self.h * self.d_k)
    )
    del query
    del key
    del value
    return self.linears[-1](x)</code></pre>
</section>
<section id="markdown-idedrba3j3tsqh" class="level1">
<h1>%% [markdown] id=“EDRba3J3TsqH”</h1>
</section>
<section id="applications-of-attention-in-our-model" class="level1">
<h1>### Applications of Attention in our Model</h1>
</section>
<section id="section-50" class="level1">
<h1></h1>
</section>
<section id="the-transformer-uses-multi-head-attention-in-three-different-ways" class="level1">
<h1>The Transformer uses multi-head attention in three different ways:</h1>
</section>
<section id="in-encoder-decoder-attention-layers-the-queries-come-from-the" class="level1">
<h1>1) In “encoder-decoder attention” layers, the queries come from the</h1>
</section>
<section id="previous-decoder-layer-and-the-memory-keys-and-values-come-from-the" class="level1">
<h1>previous decoder layer, and the memory keys and values come from the</h1>
</section>
<section id="output-of-the-encoder.-this-allows-every-position-in-the-decoder-to" class="level1">
<h1>output of the encoder. This allows every position in the decoder to</h1>
</section>
<section id="attend-over-all-positions-in-the-input-sequence.-this-mimics-the" class="level1">
<h1>attend over all positions in the input sequence. This mimics the</h1>
</section>
<section id="typical-encoder-decoder-attention-mechanisms-in-sequence-to-sequence" class="level1">
<h1>typical encoder-decoder attention mechanisms in sequence-to-sequence</h1>
</section>
<section id="models-such-as-cite." class="level1">
<h1>models such as <a href="https://arxiv.org/abs/1609.08144">(cite)</a>.</h1>
</section>
<section id="section-51" class="level1">
<h1></h1>
</section>
<section id="section-52" class="level1">
<h1></h1>
</section>
<section id="the-encoder-contains-self-attention-layers.-in-a-self-attention" class="level1">
<h1>2) The encoder contains self-attention layers. In a self-attention</h1>
</section>
<section id="layer-all-of-the-keys-values-and-queries-come-from-the-same-place" class="level1">
<h1>layer all of the keys, values and queries come from the same place,</h1>
</section>
<section id="in-this-case-the-output-of-the-previous-layer-in-the-encoder.-each" class="level1">
<h1>in this case, the output of the previous layer in the encoder. Each</h1>
</section>
<section id="position-in-the-encoder-can-attend-to-all-positions-in-the-previous" class="level1">
<h1>position in the encoder can attend to all positions in the previous</h1>
</section>
<section id="layer-of-the-encoder." class="level1">
<h1>layer of the encoder.</h1>
</section>
<section id="section-53" class="level1">
<h1></h1>
</section>
<section id="section-54" class="level1">
<h1></h1>
</section>
<section id="similarly-self-attention-layers-in-the-decoder-allow-each" class="level1">
<h1>3) Similarly, self-attention layers in the decoder allow each</h1>
</section>
<section id="position-in-the-decoder-to-attend-to-all-positions-in-the-decoder-up" class="level1">
<h1>position in the decoder to attend to all positions in the decoder up</h1>
</section>
<section id="to-and-including-that-position.-we-need-to-prevent-leftward" class="level1">
<h1>to and including that position. We need to prevent leftward</h1>
</section>
<section id="information-flow-in-the-decoder-to-preserve-the-auto-regressive" class="level1">
<h1>information flow in the decoder to preserve the auto-regressive</h1>
</section>
<section id="property.-we-implement-this-inside-of-scaled-dot-product-attention" class="level1">
<h1>property. We implement this inside of scaled dot-product attention</h1>
</section>
<section id="by-masking-out-setting-to--infty-all-values-in-the-input-of-the" class="level1">
<h1>by masking out (setting to <span class="math inline">\(-\infty\)</span>) all values in the input of the</h1>
</section>
<section id="softmax-which-correspond-to-illegal-connections." class="level1">
<h1>softmax which correspond to illegal connections.</h1>
</section>
<section id="markdown-idm-en97_gtsqh" class="level1">
<h1>%% [markdown] id=“M-en97_GTsqH”</h1>
</section>
<section id="position-wise-feed-forward-networks" class="level1">
<h1>## Position-wise Feed-Forward Networks</h1>
</section>
<section id="section-55" class="level1">
<h1></h1>
</section>
<section id="in-addition-to-attention-sub-layers-each-of-the-layers-in-our" class="level1">
<h1>In addition to attention sub-layers, each of the layers in our</h1>
</section>
<section id="encoder-and-decoder-contains-a-fully-connected-feed-forward-network" class="level1">
<h1>encoder and decoder contains a fully connected feed-forward network,</h1>
</section>
<section id="which-is-applied-to-each-position-separately-and-identically.-this" class="level1">
<h1>which is applied to each position separately and identically. This</h1>
</section>
<section id="consists-of-two-linear-transformations-with-a-relu-activation-in" class="level1">
<h1>consists of two linear transformations with a ReLU activation in</h1>
</section>
<section id="between." class="level1">
<h1>between.</h1>
</section>
<section id="section-56" class="level1">
<h1></h1>
</section>
<section id="mathrmffnxmax0-xw_1-b_1-w_2-b_2" class="level1">
<h1><span class="math display">\[\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2\]</span></h1>
</section>
<section id="section-57" class="level1">
<h1></h1>
</section>
<section id="while-the-linear-transformations-are-the-same-across-different" class="level1">
<h1>While the linear transformations are the same across different</h1>
</section>
<section id="positions-they-use-different-parameters-from-layer-to" class="level1">
<h1>positions, they use different parameters from layer to</h1>
</section>
<section id="layer.-another-way-of-describing-this-is-as-two-convolutions-with" class="level1">
<h1>layer. Another way of describing this is as two convolutions with</h1>
</section>
<section id="kernel-size-1.-the-dimensionality-of-input-and-output-is" class="level1">
<h1>kernel size 1. The dimensionality of input and output is</h1>
</section>
<section id="d_textmodel512-and-the-inner-layer-has-dimensionality" class="level1">
<h1><span class="math inline">\(d_{\text{model}}=512\)</span>, and the inner-layer has dimensionality</h1>
</section>
<section id="d_ff2048." class="level1">
<h1><span class="math inline">\(d_{ff}=2048\)</span>.</h1>
</section>
<section id="id6hhcemcxtsqh" class="level1">
<h1>%% id=“6HHCemCxTsqH”</h1>
<p>class PositionwiseFeedForward(nn.Module): “Implements FFN equation.”</p>
<pre><code>def __init__(self, d_model, d_ff, dropout=0.1):
    super(PositionwiseFeedForward, self).__init__()
    self.w_1 = nn.Linear(d_model, d_ff)
    self.w_2 = nn.Linear(d_ff, d_model)
    self.dropout = nn.Dropout(dropout)

def forward(self, x):
    return self.w_2(self.dropout(self.w_1(x).relu()))</code></pre>
</section>
<section id="markdown-iddr1ym520tsqh" class="level1">
<h1>%% [markdown] id=“dR1YM520TsqH”</h1>
</section>
<section id="embeddings-and-softmax" class="level1">
<h1>## Embeddings and Softmax</h1>
</section>
<section id="section-58" class="level1">
<h1></h1>
</section>
<section id="similarly-to-other-sequence-transduction-models-we-use-learned" class="level1">
<h1>Similarly to other sequence transduction models, we use learned</h1>
</section>
<section id="embeddings-to-convert-the-input-tokens-and-output-tokens-to-vectors" class="level1">
<h1>embeddings to convert the input tokens and output tokens to vectors</h1>
</section>
<section id="of-dimension-d_textmodel.-we-also-use-the-usual-learned" class="level1">
<h1>of dimension <span class="math inline">\(d_{\text{model}}\)</span>. We also use the usual learned</h1>
</section>
<section id="linear-transformation-and-softmax-function-to-convert-the-decoder" class="level1">
<h1>linear transformation and softmax function to convert the decoder</h1>
</section>
<section id="output-to-predicted-next-token-probabilities.-in-our-model-we" class="level1">
<h1>output to predicted next-token probabilities. In our model, we</h1>
</section>
<section id="share-the-same-weight-matrix-between-the-two-embedding-layers-and" class="level1">
<h1>share the same weight matrix between the two embedding layers and</h1>
</section>
<section id="the-pre-softmax-linear-transformation-similar-to" class="level1">
<h1>the pre-softmax linear transformation, similar to</h1>
</section>
<section id="cite.-in-the-embedding-layers" class="level1">
<h1><a href="https://arxiv.org/abs/1608.05859">(cite)</a>. In the embedding layers,</h1>
</section>
<section id="we-multiply-those-weights-by-sqrtd_textmodel." class="level1">
<h1>we multiply those weights by <span class="math inline">\(\sqrt{d_{\text{model}}}\)</span>.</h1>
</section>
<section id="idpyrchq9qtsqh" class="level1">
<h1>%% id=“pyrChq9qTsqH”</h1>
<p>class Embeddings(nn.Module): def <strong>init</strong>(self, d_model, vocab): super(Embeddings, self).__init__() self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model</p>
<pre><code>def forward(self, x):
    return self.lut(x) * math.sqrt(self.d_model)</code></pre>
</section>
<section id="markdown-idvokdui-ctsqh" class="level1">
<h1>%% [markdown] id=“vOkdui-cTsqH”</h1>
</section>
<section id="positional-encoding" class="level1">
<h1>## Positional Encoding</h1>
</section>
<section id="section-59" class="level1">
<h1></h1>
</section>
<section id="since-our-model-contains-no-recurrence-and-no-convolution-in-order" class="level1">
<h1>Since our model contains no recurrence and no convolution, in order</h1>
</section>
<section id="for-the-model-to-make-use-of-the-order-of-the-sequence-we-must" class="level1">
<h1>for the model to make use of the order of the sequence, we must</h1>
</section>
<section id="inject-some-information-about-the-relative-or-absolute-position-of" class="level1">
<h1>inject some information about the relative or absolute position of</h1>
</section>
<section id="the-tokens-in-the-sequence.-to-this-end-we-add-positional" class="level1">
<h1>the tokens in the sequence. To this end, we add “positional</h1>
</section>
<section id="encodings-to-the-input-embeddings-at-the-bottoms-of-the-encoder-and" class="level1">
<h1>encodings” to the input embeddings at the bottoms of the encoder and</h1>
</section>
<section id="decoder-stacks.-the-positional-encodings-have-the-same-dimension" class="level1">
<h1>decoder stacks. The positional encodings have the same dimension</h1>
</section>
<section id="d_textmodel-as-the-embeddings-so-that-the-two-can-be-summed." class="level1">
<h1><span class="math inline">\(d_{\text{model}}\)</span> as the embeddings, so that the two can be summed.</h1>
</section>
<section id="there-are-many-choices-of-positional-encodings-learned-and-fixed" class="level1">
<h1>There are many choices of positional encodings, learned and fixed</h1>
</section>
<section id="cite.-1" class="level1">
<h1><a href="https://arxiv.org/pdf/1705.03122.pdf">(cite)</a>.</h1>
</section>
<section id="section-60" class="level1">
<h1></h1>
</section>
<section id="in-this-work-we-use-sine-and-cosine-functions-of-different-frequencies" class="level1">
<h1>In this work, we use sine and cosine functions of different frequencies:</h1>
</section>
<section id="section-61" class="level1">
<h1></h1>
</section>
<section id="pe_pos2i-sinpos-100002id_textmodel" class="level1">
<h1><span class="math display">\[PE_{(pos,2i)} = \sin(pos / 10000^{2i/d_{\text{model}}})\]</span></h1>
</section>
<section id="section-62" class="level1">
<h1></h1>
</section>
<section id="pe_pos2i1-cospos-100002id_textmodel" class="level1">
<h1><span class="math display">\[PE_{(pos,2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}})\]</span></h1>
</section>
<section id="section-63" class="level1">
<h1></h1>
</section>
<section id="where-pos-is-the-position-and-i-is-the-dimension.-that-is-each" class="level1">
<h1>where <span class="math inline">\(pos\)</span> is the position and <span class="math inline">\(i\)</span> is the dimension. That is, each</h1>
</section>
<section id="dimension-of-the-positional-encoding-corresponds-to-a-sinusoid.-the" class="level1">
<h1>dimension of the positional encoding corresponds to a sinusoid. The</h1>
</section>
<section id="wavelengths-form-a-geometric-progression-from-2pi-to-10000-cdot-2pi.-we-chose-this-function-because-we-hypothesized-it-would" class="level1">
<h1>wavelengths form a geometric progression from <span class="math inline">\(2\pi\)</span> to <span class="math inline">\(10000 \cdot
# 2\pi\)</span>. We chose this function because we hypothesized it would</h1>
</section>
<section id="allow-the-model-to-easily-learn-to-attend-by-relative-positions" class="level1">
<h1>allow the model to easily learn to attend by relative positions,</h1>
</section>
<section id="since-for-any-fixed-offset-k-pe_posk-can-be-represented-as-a" class="level1">
<h1>since for any fixed offset <span class="math inline">\(k\)</span>, <span class="math inline">\(PE_{pos+k}\)</span> can be represented as a</h1>
</section>
<section id="linear-function-of-pe_pos." class="level1">
<h1>linear function of <span class="math inline">\(PE_{pos}\)</span>.</h1>
</section>
<section id="section-64" class="level1">
<h1></h1>
</section>
<section id="in-addition-we-apply-dropout-to-the-sums-of-the-embeddings-and-the" class="level1">
<h1>In addition, we apply dropout to the sums of the embeddings and the</h1>
</section>
<section id="positional-encodings-in-both-the-encoder-and-decoder-stacks.-for" class="level1">
<h1>positional encodings in both the encoder and decoder stacks. For</h1>
</section>
<section id="the-base-model-we-use-a-rate-of-p_drop0.1." class="level1">
<h1>the base model, we use a rate of <span class="math inline">\(P_{drop}=0.1\)</span>.</h1>
</section>
<section id="section-65" class="level1">
<h1></h1>
</section>
<section id="section-66" class="level1">
<h1></h1>
</section>
<section id="idzahgd4yjtsqh" class="level1">
<h1>%% id=“zaHGD4yJTsqH”</h1>
<p>class PositionalEncoding(nn.Module): “Implement the PE function.”</p>
<pre><code>def __init__(self, d_model, dropout, max_len=5000):
    super(PositionalEncoding, self).__init__()
    self.dropout = nn.Dropout(p=dropout)

    # Compute the positional encodings once in log space.
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len).unsqueeze(1)
    div_term = torch.exp(
        torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)
    )
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    pe = pe.unsqueeze(0)
    self.register_buffer("pe", pe)

def forward(self, x):
    x = x + self.pe[:, : x.size(1)].requires_grad_(False)
    return self.dropout(x)</code></pre>
</section>
<section id="markdown-idefhactjltsqh" class="level1">
<h1>%% [markdown] id=“EfHacTJLTsqH”</h1>
</section>
<section id="section-67" class="level1">
<h1></h1>
</section>
<section id="below-the-positional-encoding-will-add-in-a-sine-wave-based-on" class="level1">
<h1>&gt; Below the positional encoding will add in a sine wave based on</h1>
</section>
<section id="position.-the-frequency-and-offset-of-the-wave-is-different-for" class="level1">
<h1>&gt; position. The frequency and offset of the wave is different for</h1>
</section>
<section id="each-dimension." class="level1">
<h1>&gt; each dimension.</h1>
</section>
<section id="idrnvhk_1qokc6-typeexample" class="level1">
<h1>%% id=“rnvHk_1QokC6” type=“example”</h1>
<p>def example_positional(): pe = PositionalEncoding(20, 0) y = pe.forward(torch.zeros(1, 100, 20))</p>
<pre><code>data = pd.concat(
    [
        pd.DataFrame(
            {
                "embedding": y[0, :, dim],
                "dimension": dim,
                "position": list(range(100)),
            }
        )
        for dim in [4, 5, 6, 7]
    ]
)

return (
    alt.Chart(data)
    .mark_line()
    .properties(width=800)
    .encode(x="position", y="embedding", color="dimension:N")
    .interactive()
)</code></pre>
<p>show_example(example_positional)</p>
</section>
<section id="markdown-idg8rzncrztsqi" class="level1">
<h1>%% [markdown] id=“g8rZNCrzTsqI”</h1>
</section>
<section id="section-68" class="level1">
<h1></h1>
</section>
<section id="we-also-experimented-with-using-learned-positional-embeddings" class="level1">
<h1>We also experimented with using learned positional embeddings</h1>
</section>
<section id="cite-instead-and-found" class="level1">
<h1><a href="https://arxiv.org/pdf/1705.03122.pdf">(cite)</a> instead, and found</h1>
</section>
<section id="that-the-two-versions-produced-nearly-identical-results.-we-chose" class="level1">
<h1>that the two versions produced nearly identical results. We chose</h1>
</section>
<section id="the-sinusoidal-version-because-it-may-allow-the-model-to-extrapolate" class="level1">
<h1>the sinusoidal version because it may allow the model to extrapolate</h1>
</section>
<section id="to-sequence-lengths-longer-than-the-ones-encountered-during" class="level1">
<h1>to sequence lengths longer than the ones encountered during</h1>
</section>
<section id="training." class="level1">
<h1>training.</h1>
</section>
<section id="markdown-idiwnkczlytsqi" class="level1">
<h1>%% [markdown] id=“iwNKCzlyTsqI”</h1>
</section>
<section id="full-model" class="level1">
<h1>## Full Model</h1>
</section>
<section id="section-69" class="level1">
<h1></h1>
</section>
<section id="here-we-define-a-function-from-hyperparameters-to-a-full-model." class="level1">
<h1>&gt; Here we define a function from hyperparameters to a full model.</h1>
</section>
<section id="idmpe1es0utsqi" class="level1">
<h1>%% id=“mPe1ES0UTsqI”</h1>
<p>def make_model( src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1 ): “Helper: Construct a model from hyperparameters.” c = copy.deepcopy attn = MultiHeadedAttention(h, d_model) ff = PositionwiseFeedForward(d_model, d_ff, dropout) position = PositionalEncoding(d_model, dropout) model = EncoderDecoder( Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), nn.Sequential(Embeddings(d_model, src_vocab), c(position)), nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), Generator(d_model, tgt_vocab), )</p>
<pre><code># This was important from their code.
# Initialize parameters with Glorot / fan_avg.
for p in model.parameters():
    if p.dim() &gt; 1:
        nn.init.xavier_uniform_(p)
return model</code></pre>
</section>
<section id="markdown-1" class="level1">
<h1>%% [markdown]</h1>
</section>
<section id="inference" class="level1">
<h1>## Inference:</h1>
</section>
<section id="section-70" class="level1">
<h1></h1>
</section>
<section id="here-we-make-a-forward-step-to-generate-a-prediction-of-the" class="level1">
<h1>&gt; Here we make a forward step to generate a prediction of the</h1>
</section>
<section id="model.-we-try-to-use-our-transformer-to-memorize-the-input.-as-you" class="level1">
<h1>model. We try to use our transformer to memorize the input. As you</h1>
</section>
<section id="will-see-the-output-is-randomly-generated-due-to-the-fact-that-the" class="level1">
<h1>will see the output is randomly generated due to the fact that the</h1>
</section>
<section id="model-is-not-trained-yet.-in-the-next-tutorial-we-will-build-the" class="level1">
<h1>model is not trained yet. In the next tutorial we will build the</h1>
</section>
<section id="training-function-and-try-to-train-our-model-to-memorize-the-numbers" class="level1">
<h1>training function and try to train our model to memorize the numbers</h1>
</section>
<section id="from-1-to-10." class="level1">
<h1>from 1 to 10.</h1>
</section>
<section id="section-71" class="level1">
<h1>%%</h1>
<p>def inference_test(): test_model = make_model(11, 11, 2) test_model.eval() src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]) src_mask = torch.ones(1, 1, 10)</p>
<pre><code>memory = test_model.encode(src, src_mask)
ys = torch.zeros(1, 1).type_as(src)

for i in range(9):
    out = test_model.decode(
        memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)
    )
    prob = test_model.generator(out[:, -1])
    _, next_word = torch.max(prob, dim=1)
    next_word = next_word.data[0]
    ys = torch.cat(
        [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1
    )

print("Example Untrained Model Prediction:", ys)</code></pre>
<p>def run_tests(): for _ in range(10): inference_test()</p>
<p>show_example(run_tests)</p>
</section>
<section id="markdown-2" class="level1">
<h1>%% [markdown]</h1>
</section>
<section id="part-2-model-training" class="level1">
<h1># Part 2: Model Training</h1>
</section>
<section id="markdown-id05s6ot9ftsqi" class="level1">
<h1>%% [markdown] id=“05s6oT9fTsqI”</h1>
</section>
<section id="training" class="level1">
<h1># Training</h1>
</section>
<section id="section-72" class="level1">
<h1></h1>
</section>
<section id="this-section-describes-the-training-regime-for-our-models." class="level1">
<h1>This section describes the training regime for our models.</h1>
</section>
<section id="markdown-idftxlofs4tsqi" class="level1">
<h1>%% [markdown] id=“fTxlofs4TsqI”</h1>
</section>
<section id="section-73" class="level1">
<h1></h1>
</section>
<section id="we-stop-for-a-quick-interlude-to-introduce-some-of-the-tools" class="level1">
<h1>&gt; We stop for a quick interlude to introduce some of the tools</h1>
</section>
<section id="needed-to-train-a-standard-encoder-decoder-model.-first-we-define-a" class="level1">
<h1>&gt; needed to train a standard encoder decoder model. First we define a</h1>
</section>
<section id="batch-object-that-holds-the-src-and-target-sentences-for-training" class="level1">
<h1>&gt; batch object that holds the src and target sentences for training,</h1>
</section>
<section id="as-well-as-constructing-the-masks." class="level1">
<h1>&gt; as well as constructing the masks.</h1>
</section>
<section id="markdown-idg7skcenxtsqi" class="level1">
<h1>%% [markdown] id=“G7SkCenXTsqI”</h1>
</section>
<section id="batches-and-masking" class="level1">
<h1>## Batches and Masking</h1>
</section>
<section id="section-74" class="level1">
<h1>%%</h1>
<p>class Batch: “““Object for holding a batch of data with mask during training.”“”</p>
<pre><code>def __init__(self, src, tgt=None, pad=2):  # 2 = &lt;blank&gt;
    self.src = src
    self.src_mask = (src != pad).unsqueeze(-2)
    if tgt is not None:
        self.tgt = tgt[:, :-1]
        self.tgt_y = tgt[:, 1:]
        self.tgt_mask = self.make_std_mask(self.tgt, pad)
        self.ntokens = (self.tgt_y != pad).data.sum()

@staticmethod
def make_std_mask(tgt, pad):
    "Create a mask to hide padding and future words."
    tgt_mask = (tgt != pad).unsqueeze(-2)
    tgt_mask = tgt_mask &amp; subsequent_mask(tgt.size(-1)).type_as(
        tgt_mask.data
    )
    return tgt_mask</code></pre>
</section>
<section id="markdown-idckkw5gjltsqi" class="level1">
<h1>%% [markdown] id=“cKkw5GjLTsqI”</h1>
</section>
<section id="section-75" class="level1">
<h1></h1>
</section>
<section id="next-we-create-a-generic-training-and-scoring-function-to-keep" class="level1">
<h1>&gt; Next we create a generic training and scoring function to keep</h1>
</section>
<section id="track-of-loss.-we-pass-in-a-generic-loss-compute-function-that" class="level1">
<h1>&gt; track of loss. We pass in a generic loss compute function that</h1>
</section>
<section id="also-handles-parameter-updates." class="level1">
<h1>&gt; also handles parameter updates.</h1>
</section>
<section id="markdown-idq8zzeuc0tsqj" class="level1">
<h1>%% [markdown] id=“Q8zzeUc0TsqJ”</h1>
</section>
<section id="training-loop" class="level1">
<h1>## Training Loop</h1>
</section>
<section id="section-76" class="level1">
<h1>%%</h1>
<p>class TrainState: “““Track number of steps, examples, and tokens processed”“”</p>
<pre><code>step: int = 0  # Steps in the current epoch
accum_step: int = 0  # Number of gradient accumulation steps
samples: int = 0  # total # of examples used
tokens: int = 0  # total # of tokens processed</code></pre>
</section>
<section id="id2hazd3hitsqj" class="level1">
<h1>%% id=“2HAZD3hiTsqJ”</h1>
<p>def run_epoch( data_iter, model, loss_compute, optimizer, scheduler, mode=“train”, accum_iter=1, train_state=TrainState(), ): “““Train a single epoch”“” start = time.time() total_tokens = 0 total_loss = 0 tokens = 0 n_accum = 0 for i, batch in enumerate(data_iter): out = model.forward( batch.src, batch.tgt, batch.src_mask, batch.tgt_mask ) loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens) # loss_node = loss_node / accum_iter if mode == “train” or mode == “train+log”: loss_node.backward() train_state.step += 1 train_state.samples += batch.src.shape[0] train_state.tokens += batch.ntokens if i % accum_iter == 0: optimizer.step() optimizer.zero_grad(set_to_none=True) n_accum += 1 train_state.accum_step += 1 scheduler.step()</p>
<pre><code>    total_loss += loss
    total_tokens += batch.ntokens
    tokens += batch.ntokens
    if i % 40 == 1 and (mode == "train" or mode == "train+log"):
        lr = optimizer.param_groups[0]["lr"]
        elapsed = time.time() - start
        print(
            (
                "Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f "
                + "| Tokens / Sec: %7.1f | Learning Rate: %6.1e"
            )
            % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)
        )
        start = time.time()
        tokens = 0
    del loss
    del loss_node
return total_loss / total_tokens, train_state</code></pre>
</section>
<section id="markdown-idab1if0fotsqj" class="level1">
<h1>%% [markdown] id=“aB1IF0foTsqJ”</h1>
</section>
<section id="training-data-and-batching" class="level1">
<h1>## Training Data and Batching</h1>
</section>
<section id="section-77" class="level1">
<h1></h1>
</section>
<section id="we-trained-on-the-standard-wmt-2014-english-german-dataset" class="level1">
<h1>We trained on the standard WMT 2014 English-German dataset</h1>
</section>
<section id="consisting-of-about-4.5-million-sentence-pairs.-sentences-were" class="level1">
<h1>consisting of about 4.5 million sentence pairs. Sentences were</h1>
</section>
<section id="encoded-using-byte-pair-encoding-which-has-a-shared-source-target" class="level1">
<h1>encoded using byte-pair encoding, which has a shared source-target</h1>
</section>
<section id="vocabulary-of-about-37000-tokens.-for-english-french-we-used-the" class="level1">
<h1>vocabulary of about 37000 tokens. For English-French, we used the</h1>
</section>
<section id="significantly-larger-wmt-2014-english-french-dataset-consisting-of" class="level1">
<h1>significantly larger WMT 2014 English-French dataset consisting of</h1>
</section>
<section id="m-sentences-and-split-tokens-into-a-32000-word-piece-vocabulary." class="level1">
<h1>36M sentences and split tokens into a 32000 word-piece vocabulary.</h1>
</section>
<section id="section-78" class="level1">
<h1></h1>
</section>
<section id="section-79" class="level1">
<h1></h1>
</section>
<section id="sentence-pairs-were-batched-together-by-approximate-sequence-length." class="level1">
<h1>Sentence pairs were batched together by approximate sequence length.</h1>
</section>
<section id="each-training-batch-contained-a-set-of-sentence-pairs-containing" class="level1">
<h1>Each training batch contained a set of sentence pairs containing</h1>
</section>
<section id="approximately-25000-source-tokens-and-25000-target-tokens." class="level1">
<h1>approximately 25000 source tokens and 25000 target tokens.</h1>
</section>
<section id="markdown-idf1mtqatitsqj-jp-markdownheadingcollapsedtrue-tags" class="level1">
<h1>%% [markdown] id=“F1mTQatiTsqJ” jp-MarkdownHeadingCollapsed=true tags=[]</h1>
</section>
<section id="hardware-and-schedule" class="level1">
<h1>## Hardware and Schedule</h1>
</section>
<section id="section-80" class="level1">
<h1></h1>
</section>
<section id="we-trained-our-models-on-one-machine-with-8-nvidia-p100-gpus.-for" class="level1">
<h1>We trained our models on one machine with 8 NVIDIA P100 GPUs. For</h1>
</section>
<section id="our-base-models-using-the-hyperparameters-described-throughout-the" class="level1">
<h1>our base models using the hyperparameters described throughout the</h1>
</section>
<section id="paper-each-training-step-took-about-0.4-seconds.-we-trained-the" class="level1">
<h1>paper, each training step took about 0.4 seconds. We trained the</h1>
</section>
<section id="base-models-for-a-total-of-100000-steps-or-12-hours.-for-our-big" class="level1">
<h1>base models for a total of 100,000 steps or 12 hours. For our big</h1>
</section>
<section id="models-step-time-was-1.0-seconds.-the-big-models-were-trained-for" class="level1">
<h1>models, step time was 1.0 seconds. The big models were trained for</h1>
</section>
<section id="steps-3.5-days." class="level1">
<h1>300,000 steps (3.5 days).</h1>
</section>
<section id="markdown-id-utzeugctsqj" class="level1">
<h1>%% [markdown] id=“-utZeuGcTsqJ”</h1>
</section>
<section id="optimizer" class="level1">
<h1>## Optimizer</h1>
</section>
<section id="section-81" class="level1">
<h1></h1>
</section>
<section id="we-used-the-adam-optimizer-cite" class="level1">
<h1>We used the Adam optimizer <a href="https://arxiv.org/abs/1412.6980">(cite)</a></h1>
</section>
<section id="with-beta_10.9-beta_20.98-and-epsilon10-9.-we" class="level1">
<h1>with <span class="math inline">\(\beta_1=0.9\)</span>, <span class="math inline">\(\beta_2=0.98\)</span> and <span class="math inline">\(\epsilon=10^{-9}\)</span>. We</h1>
</section>
<section id="varied-the-learning-rate-over-the-course-of-training-according-to" class="level1">
<h1>varied the learning rate over the course of training, according to</h1>
</section>
<section id="the-formula" class="level1">
<h1>the formula:</h1>
</section>
<section id="section-82" class="level1">
<h1></h1>
</section>
<section id="lrate-d_textmodel-0.5-cdot-minstep_num-0.5-step_num-cdot-warmup_steps-1.5" class="level1">
<h1><span class="math display">\[
# lrate = d_{\text{model}}^{-0.5} \cdot
#   \min({step\_num}^{-0.5},
#     {step\_num} \cdot {warmup\_steps}^{-1.5})
# \]</span></h1>
</section>
<section id="section-83" class="level1">
<h1></h1>
</section>
<section id="this-corresponds-to-increasing-the-learning-rate-linearly-for-the" class="level1">
<h1>This corresponds to increasing the learning rate linearly for the</h1>
</section>
<section id="first-warmup_steps-training-steps-and-decreasing-it-thereafter" class="level1">
<h1>first <span class="math inline">\(warmup\_steps\)</span> training steps, and decreasing it thereafter</h1>
</section>
<section id="proportionally-to-the-inverse-square-root-of-the-step-number.-we" class="level1">
<h1>proportionally to the inverse square root of the step number. We</h1>
</section>
<section id="used-warmup_steps4000." class="level1">
<h1>used <span class="math inline">\(warmup\_steps=4000\)</span>.</h1>
</section>
<section id="markdown-id39fbynt-tsqj" class="level1">
<h1>%% [markdown] id=“39FbYnt-TsqJ”</h1>
</section>
<section id="section-84" class="level1">
<h1></h1>
</section>
<section id="note-this-part-is-very-important.-need-to-train-with-this-setup" class="level1">
<h1>&gt; Note: This part is very important. Need to train with this setup</h1>
</section>
<section id="of-the-model." class="level1">
<h1>&gt; of the model.</h1>
</section>
<section id="markdown-idhlbojfkjtsqj" class="level1">
<h1>%% [markdown] id=“hlbojFkjTsqJ”</h1>
</section>
<section id="section-85" class="level1">
<h1></h1>
</section>
<section id="example-of-the-curves-of-this-model-for-different-model-sizes-and" class="level1">
<h1>&gt; Example of the curves of this model for different model sizes and</h1>
</section>
<section id="for-optimization-hyperparameters." class="level1">
<h1>&gt; for optimization hyperparameters.</h1>
</section>
<section id="idzuz3pdanvg4o" class="level1">
<h1>%% id=“zUz3PdAnVg4o”</h1>
<p>def rate(step, model_size, factor, warmup): ““” we have to default the step to 1 for LambdaLR function to avoid zero raising to negative power. ““” if step == 0: step = 1 return factor * ( model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5)) )</p>
</section>
<section id="idl1bnrlnsv8j5-tags" class="level1">
<h1>%% id=“l1bnrlnSV8J5” tags=[]</h1>
<p>def example_learning_schedule(): opts = [ [512, 1, 4000], # example 1 [512, 1, 8000], # example 2 [256, 1, 4000], # example 3 ]</p>
<pre><code>dummy_model = torch.nn.Linear(1, 1)
learning_rates = []

# we have 3 examples in opts list.
for idx, example in enumerate(opts):
    # run 20000 epoch for each example
    optimizer = torch.optim.Adam(
        dummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9
    )
    lr_scheduler = LambdaLR(
        optimizer=optimizer, lr_lambda=lambda step: rate(step, *example)
    )
    tmp = []
    # take 20K dummy training steps, save the learning rate at each step
    for step in range(20000):
        tmp.append(optimizer.param_groups[0]["lr"])
        optimizer.step()
        lr_scheduler.step()
    learning_rates.append(tmp)

learning_rates = torch.tensor(learning_rates)

# Enable altair to handle more than 5000 rows
alt.data_transformers.disable_max_rows()

opts_data = pd.concat(
    [
        pd.DataFrame(
            {
                "Learning Rate": learning_rates[warmup_idx, :],
                "model_size:warmup": ["512:4000", "512:8000", "256:4000"][
                    warmup_idx
                ],
                "step": range(20000),
            }
        )
        for warmup_idx in [0, 1, 2]
    ]
)

return (
    alt.Chart(opts_data)
    .mark_line()
    .properties(width=600)
    .encode(x="step", y="Learning Rate", color="model_size:warmup:N")
    .interactive()
)</code></pre>
<p>example_learning_schedule()</p>
</section>
<section id="markdown-id7t1ud15vtsqk" class="level1">
<h1>%% [markdown] id=“7T1uD15VTsqK”</h1>
</section>
<section id="regularization" class="level1">
<h1>## Regularization</h1>
</section>
<section id="section-86" class="level1">
<h1></h1>
</section>
<section id="label-smoothing" class="level1">
<h1>### Label Smoothing</h1>
</section>
<section id="section-87" class="level1">
<h1></h1>
</section>
<section id="during-training-we-employed-label-smoothing-of-value" class="level1">
<h1>During training, we employed label smoothing of value</h1>
</section>
<section id="epsilon_ls0.1-cite." class="level1">
<h1><span class="math inline">\(\epsilon_{ls}=0.1\)</span> <a href="https://arxiv.org/abs/1512.00567">(cite)</a>.</h1>
</section>
<section id="this-hurts-perplexity-as-the-model-learns-to-be-more-unsure-but" class="level1">
<h1>This hurts perplexity, as the model learns to be more unsure, but</h1>
</section>
<section id="improves-accuracy-and-bleu-score." class="level1">
<h1>improves accuracy and BLEU score.</h1>
</section>
<section id="markdown-idknoavd8btsqk" class="level1">
<h1>%% [markdown] id=“kNoAVD8bTsqK”</h1>
</section>
<section id="section-88" class="level1">
<h1></h1>
</section>
<section id="we-implement-label-smoothing-using-the-kl-div-loss.-instead-of" class="level1">
<h1>&gt; We implement label smoothing using the KL div loss. Instead of</h1>
</section>
<section id="using-a-one-hot-target-distribution-we-create-a-distribution-that" class="level1">
<h1>&gt; using a one-hot target distribution, we create a distribution that</h1>
</section>
<section id="has-confidence-of-the-correct-word-and-the-rest-of-the" class="level1">
<h1>&gt; has <code>confidence</code> of the correct word and the rest of the</h1>
</section>
<section id="smoothing-mass-distributed-throughout-the-vocabulary." class="level1">
<h1>&gt; <code>smoothing</code> mass distributed throughout the vocabulary.</h1>
</section>
<section id="idshu2gyietsqk" class="level1">
<h1>%% id=“shU2GyiETsqK”</h1>
<p>class LabelSmoothing(nn.Module): “Implement label smoothing.”</p>
<pre><code>def __init__(self, size, padding_idx, smoothing=0.0):
    super(LabelSmoothing, self).__init__()
    self.criterion = nn.KLDivLoss(reduction="sum")
    self.padding_idx = padding_idx
    self.confidence = 1.0 - smoothing
    self.smoothing = smoothing
    self.size = size
    self.true_dist = None

def forward(self, x, target):
    assert x.size(1) == self.size
    true_dist = x.data.clone()
    true_dist.fill_(self.smoothing / (self.size - 2))
    true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
    true_dist[:, self.padding_idx] = 0
    mask = torch.nonzero(target.data == self.padding_idx)
    if mask.dim() &gt; 0:
        true_dist.index_fill_(0, mask.squeeze(), 0.0)
    self.true_dist = true_dist
    return self.criterion(x, true_dist.clone().detach())</code></pre>
</section>
<section id="markdown-idjcxurluytsqk" class="level1">
<h1>%% [markdown] id=“jCxUrlUyTsqK”</h1>
</section>
<section id="section-89" class="level1">
<h1></h1>
</section>
<section id="here-we-can-see-an-example-of-how-the-mass-is-distributed-to-the" class="level1">
<h1>&gt; Here we can see an example of how the mass is distributed to the</h1>
</section>
<section id="words-based-on-confidence." class="level1">
<h1>&gt; words based on confidence.</h1>
</section>
<section id="ideztkaaqntsqk" class="level1">
<h1>%% id=“EZtKaaQNTsqK”</h1>
</section>
<section id="example-of-label-smoothing." class="level1">
<h1>Example of label smoothing.</h1>
<p>def example_label_smoothing(): crit = LabelSmoothing(5, 0, 0.4) predict = torch.FloatTensor( [ [0, 0.2, 0.7, 0.1, 0], [0, 0.2, 0.7, 0.1, 0], [0, 0.2, 0.7, 0.1, 0], [0, 0.2, 0.7, 0.1, 0], [0, 0.2, 0.7, 0.1, 0], ] ) crit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3])) LS_data = pd.concat( [ pd.DataFrame( { “target distribution”: crit.true_dist[x, y].flatten(), “columns”: y, “rows”: x, } ) for y in range(5) for x in range(5) ] )</p>
<pre><code>return (
    alt.Chart(LS_data)
    .mark_rect(color="Blue", opacity=1)
    .properties(height=200, width=200)
    .encode(
        alt.X("columns:O", title=None),
        alt.Y("rows:O", title=None),
        alt.Color(
            "target distribution:Q", scale=alt.Scale(scheme="viridis")
        ),
    )
    .interactive()
)</code></pre>
<p>show_example(example_label_smoothing)</p>
</section>
<section id="markdown-idcgm8j1vetsqk" class="level1">
<h1>%% [markdown] id=“CGM8J1veTsqK”</h1>
</section>
<section id="section-90" class="level1">
<h1></h1>
</section>
<section id="label-smoothing-actually-starts-to-penalize-the-model-if-it-gets" class="level1">
<h1>&gt; Label smoothing actually starts to penalize the model if it gets</h1>
</section>
<section id="very-confident-about-a-given-choice." class="level1">
<h1>&gt; very confident about a given choice.</h1>
</section>
<section id="id78ehzlp7tsqk" class="level1">
<h1>%% id=“78EHzLP7TsqK”</h1>
<p>def loss(x, crit): d = x + 3 * 1 predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]]) return crit(predict.log(), torch.LongTensor([1])).data</p>
<p>def penalization_visualization(): crit = LabelSmoothing(5, 0, 0.1) loss_data = pd.DataFrame( { “Loss”: [loss(x, crit) for x in range(1, 100)], “Steps”: list(range(99)), } ).astype(“float”)</p>
<pre><code>return (
    alt.Chart(loss_data)
    .mark_line()
    .properties(width=350)
    .encode(
        x="Steps",
        y="Loss",
    )
    .interactive()
)</code></pre>
<p>show_example(penalization_visualization)</p>
</section>
<section id="markdown-id67luqelxtsqk" class="level1">
<h1>%% [markdown] id=“67lUqeLXTsqK”</h1>
</section>
<section id="a-first-example" class="level1">
<h1># A First Example</h1>
</section>
<section id="section-91" class="level1">
<h1></h1>
</section>
<section id="we-can-begin-by-trying-out-a-simple-copy-task.-given-a-random-set" class="level1">
<h1>&gt; We can begin by trying out a simple copy-task. Given a random set</h1>
</section>
<section id="of-input-symbols-from-a-small-vocabulary-the-goal-is-to-generate" class="level1">
<h1>&gt; of input symbols from a small vocabulary, the goal is to generate</h1>
</section>
<section id="back-those-same-symbols." class="level1">
<h1>&gt; back those same symbols.</h1>
</section>
<section id="markdown-idjja-89_ptsqk" class="level1">
<h1>%% [markdown] id=“jJa-89_pTsqK”</h1>
</section>
<section id="synthetic-data" class="level1">
<h1>## Synthetic Data</h1>
</section>
<section id="idg1atxeqqtsqk" class="level1">
<h1>%% id=“g1aTxeqqTsqK”</h1>
<p>def data_gen(V, batch_size, nbatches): “Generate random data for a src-tgt copy task.” for i in range(nbatches): data = torch.randint(1, V, size=(batch_size, 10)) data[:, 0] = 1 src = data.requires_grad_(False).clone().detach() tgt = data.requires_grad_(False).clone().detach() yield Batch(src, tgt, 0)</p>
</section>
<section id="markdown-idxtxwd9hutsqk" class="level1">
<h1>%% [markdown] id=“XTXwD9hUTsqK”</h1>
</section>
<section id="loss-computation" class="level1">
<h1>## Loss Computation</h1>
</section>
<section id="id3j8ejm87tsqk" class="level1">
<h1>%% id=“3J8EJm87TsqK”</h1>
<p>class SimpleLossCompute: “A simple loss compute and train function.”</p>
<pre><code>def __init__(self, generator, criterion):
    self.generator = generator
    self.criterion = criterion

def __call__(self, x, y, norm):
    x = self.generator(x)
    sloss = (
        self.criterion(
            x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)
        )
        / norm
    )
    return sloss.data * norm, sloss</code></pre>
</section>
<section id="markdown-idedai7elutsql" class="level1">
<h1>%% [markdown] id=“eDAI7ELUTsqL”</h1>
</section>
<section id="greedy-decoding" class="level1">
<h1>## Greedy Decoding</h1>
</section>
<section id="markdown-idlfkwakpltsql-tags" class="level1">
<h1>%% [markdown] id=“LFkWakplTsqL” tags=[]</h1>
</section>
<section id="this-code-predicts-a-translation-using-greedy-decoding-for-simplicity." class="level1">
<h1>&gt; This code predicts a translation using greedy decoding for simplicity.</h1>
</section>
<section id="idn2uopnt3biyu" class="level1">
<h1>%% id=“N2UOpnT3bIyU”</h1>
<p>def greedy_decode(model, src, src_mask, max_len, start_symbol): memory = model.encode(src, src_mask) ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data) for i in range(max_len - 1): out = model.decode( memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data) ) prob = model.generator(out[:, -1]) _, next_word = torch.max(prob, dim=1) next_word = next_word.data[0] ys = torch.cat( [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1 ) return ys</p>
</section>
<section id="idqgiz2yetdywe-tags" class="level1">
<h1>%% id=“qgIZ2yEtdYwe” tags=[]</h1>
</section>
<section id="train-the-simple-copy-task." class="level1">
<h1>Train the simple copy task.</h1>
<p>def example_simple_model(): V = 11 criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0) model = make_model(V, V, N=2)</p>
<pre><code>optimizer = torch.optim.Adam(
    model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9
)
lr_scheduler = LambdaLR(
    optimizer=optimizer,
    lr_lambda=lambda step: rate(
        step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400
    ),
)

batch_size = 80
for epoch in range(20):
    model.train()
    run_epoch(
        data_gen(V, batch_size, 20),
        model,
        SimpleLossCompute(model.generator, criterion),
        optimizer,
        lr_scheduler,
        mode="train",
    )
    model.eval()
    run_epoch(
        data_gen(V, batch_size, 5),
        model,
        SimpleLossCompute(model.generator, criterion),
        DummyOptimizer(),
        DummyScheduler(),
        mode="eval",
    )[0]

model.eval()
src = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])
max_len = src.shape[1]
src_mask = torch.ones(1, 1, max_len)
print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0))</code></pre>
</section>
<section id="execute_exampleexample_simple_model" class="level1">
<h1>execute_example(example_simple_model)</h1>
</section>
<section id="markdown-idopuqv2gstsql" class="level1">
<h1>%% [markdown] id=“OpuQv2GsTsqL”</h1>
</section>
<section id="part-3-a-real-world-example" class="level1">
<h1># Part 3: A Real World Example</h1>
</section>
<section id="section-92" class="level1">
<h1></h1>
</section>
<section id="now-we-consider-a-real-world-example-using-the-multi30k" class="level1">
<h1>&gt; Now we consider a real-world example using the Multi30k</h1>
</section>
<section id="german-english-translation-task.-this-task-is-much-smaller-than" class="level1">
<h1>&gt; German-English Translation task. This task is much smaller than</h1>
</section>
<section id="the-wmt-task-considered-in-the-paper-but-it-illustrates-the-whole" class="level1">
<h1>&gt; the WMT task considered in the paper, but it illustrates the whole</h1>
</section>
<section id="system.-we-also-show-how-to-use-multi-gpu-processing-to-make-it" class="level1">
<h1>&gt; system. We also show how to use multi-gpu processing to make it</h1>
</section>
<section id="really-fast." class="level1">
<h1>&gt; really fast.</h1>
</section>
<section id="markdown-id8y9dpfoltsql-tags" class="level1">
<h1>%% [markdown] id=“8y9dpfolTsqL” tags=[]</h1>
</section>
<section id="data-loading" class="level1">
<h1>## Data Loading</h1>
</section>
<section id="section-93" class="level1">
<h1></h1>
</section>
<section id="we-will-load-the-dataset-using-torchtext-and-spacy-for" class="level1">
<h1>&gt; We will load the dataset using torchtext and spacy for</h1>
</section>
<section id="tokenization." class="level1">
<h1>&gt; tokenization.</h1>
</section>
<section id="section-94" class="level1">
<h1>%%</h1>
</section>
<section id="load-spacy-tokenizer-models-download-them-if-they-havent-been" class="level1">
<h1>Load spacy tokenizer models, download them if they haven’t been</h1>
</section>
<section id="downloaded-already" class="level1">
<h1>downloaded already</h1>
<p>def load_tokenizers():</p>
<pre><code>try:
    spacy_de = spacy.load("de_core_news_sm")
except IOError:
    os.system("python -m spacy download de_core_news_sm")
    spacy_de = spacy.load("de_core_news_sm")

try:
    spacy_en = spacy.load("en_core_web_sm")
except IOError:
    os.system("python -m spacy download en_core_web_sm")
    spacy_en = spacy.load("en_core_web_sm")

return spacy_de, spacy_en</code></pre>
</section>
<section id="idt4bszxxjtsql-tags" class="level1">
<h1>%% id=“t4BszXXJTsqL” tags=[]</h1>
<p>def tokenize(text, tokenizer): return [tok.text for tok in tokenizer.tokenizer(text)]</p>
<p>def yield_tokens(data_iter, tokenizer, index): for from_to_tuple in data_iter: yield tokenizer(from_to_tuple[index])</p>
</section>
<section id="idju3kvlv5okc--tags" class="level1">
<h1>%% id=“jU3kVlV5okC-” tags=[]</h1>
<p>def build_vocabulary(spacy_de, spacy_en): def tokenize_de(text): return tokenize(text, spacy_de)</p>
<pre><code>def tokenize_en(text):
    return tokenize(text, spacy_en)

print("Building German Vocabulary ...")
train, val, test = datasets.Multi30k(language_pair=("de", "en"))
vocab_src = build_vocab_from_iterator(
    yield_tokens(train + val + test, tokenize_de, index=0),
    min_freq=2,
    specials=["&lt;s&gt;", "&lt;/s&gt;", "&lt;blank&gt;", "&lt;unk&gt;"],
)

print("Building English Vocabulary ...")
train, val, test = datasets.Multi30k(language_pair=("de", "en"))
vocab_tgt = build_vocab_from_iterator(
    yield_tokens(train + val + test, tokenize_en, index=1),
    min_freq=2,
    specials=["&lt;s&gt;", "&lt;/s&gt;", "&lt;blank&gt;", "&lt;unk&gt;"],
)

vocab_src.set_default_index(vocab_src["&lt;unk&gt;"])
vocab_tgt.set_default_index(vocab_tgt["&lt;unk&gt;"])

return vocab_src, vocab_tgt</code></pre>
<p>def load_vocab(spacy_de, spacy_en): if not exists(“vocab.pt”): vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en) torch.save((vocab_src, vocab_tgt), “vocab.pt”) else: vocab_src, vocab_tgt = torch.load(“vocab.pt”) print(“Finished.sizes:”) print(len(vocab_src)) print(len(vocab_tgt)) return vocab_src, vocab_tgt</p>
<p>if is_interactive_notebook(): # global variables used later in the script spacy_de, spacy_en = show_example(load_tokenizers) vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])</p>
</section>
<section id="markdown-id-l-tfwzftsql" class="level1">
<h1>%% [markdown] id=“-l-TFwzfTsqL”</h1>
</section>
<section id="section-95" class="level1">
<h1></h1>
</section>
<section id="batching-matters-a-ton-for-speed.-we-want-to-have-very-evenly" class="level1">
<h1>&gt; Batching matters a ton for speed. We want to have very evenly</h1>
</section>
<section id="divided-batches-with-absolutely-minimal-padding.-to-do-this-we" class="level1">
<h1>&gt; divided batches, with absolutely minimal padding. To do this we</h1>
</section>
<section id="have-to-hack-a-bit-around-the-default-torchtext-batching.-this" class="level1">
<h1>&gt; have to hack a bit around the default torchtext batching. This</h1>
</section>
<section id="code-patches-their-default-batching-to-make-sure-we-search-over" class="level1">
<h1>&gt; code patches their default batching to make sure we search over</h1>
</section>
<section id="enough-sentences-to-find-tight-batches." class="level1">
<h1>&gt; enough sentences to find tight batches.</h1>
</section>
<section id="markdown-idkdej-hcgokc--tags-jp-markdownheadingcollapsedtrue" class="level1">
<h1>%% [markdown] id=“kDEj-hCgokC-” tags=[] jp-MarkdownHeadingCollapsed=true</h1>
</section>
<section id="iterators" class="level1">
<h1>## Iterators</h1>
</section>
<section id="idwgsihfgookc_-tags" class="level1">
<h1>%% id=“wGsIHFgOokC_” tags=[]</h1>
<p>def collate_batch( batch, src_pipeline, tgt_pipeline, src_vocab, tgt_vocab, device, max_padding=128, pad_id=2, ): bs_id = torch.tensor([0], device=device) # <s> token id eos_id = torch.tensor([1], device=device) # </s> token id src_list, tgt_list = [], [] for (_src, _tgt) in batch: processed_src = torch.cat( [ bs_id, torch.tensor( src_vocab(src_pipeline(_src)), dtype=torch.int64, device=device, ), eos_id, ], 0, ) processed_tgt = torch.cat( [ bs_id, torch.tensor( tgt_vocab(tgt_pipeline(_tgt)), dtype=torch.int64, device=device, ), eos_id, ], 0, ) src_list.append( # warning - overwrites values for negative values of padding - len pad( processed_src, ( 0, max_padding - len(processed_src), ), value=pad_id, ) ) tgt_list.append( pad( processed_tgt, (0, max_padding - len(processed_tgt)), value=pad_id, ) )</p>
<pre><code>src = torch.stack(src_list)
tgt = torch.stack(tgt_list)
return (src, tgt)</code></pre>
</section>
<section id="idka2ce_wiokc_-tags" class="level1">
<h1>%% id=“ka2Ce_WIokC_” tags=[]</h1>
<p>def create_dataloaders( device, vocab_src, vocab_tgt, spacy_de, spacy_en, batch_size=12000, max_padding=128, is_distributed=True, ): # def create_dataloaders(batch_size=12000): def tokenize_de(text): return tokenize(text, spacy_de)</p>
<pre><code>def tokenize_en(text):
    return tokenize(text, spacy_en)

def collate_fn(batch):
    return collate_batch(
        batch,
        tokenize_de,
        tokenize_en,
        vocab_src,
        vocab_tgt,
        device,
        max_padding=max_padding,
        pad_id=vocab_src.get_stoi()["&lt;blank&gt;"],
    )

train_iter, valid_iter, test_iter = datasets.Multi30k(
    language_pair=("de", "en")
)

train_iter_map = to_map_style_dataset(
    train_iter
)  # DistributedSampler needs a dataset len()
train_sampler = (
    DistributedSampler(train_iter_map) if is_distributed else None
)
valid_iter_map = to_map_style_dataset(valid_iter)
valid_sampler = (
    DistributedSampler(valid_iter_map) if is_distributed else None
)

train_dataloader = DataLoader(
    train_iter_map,
    batch_size=batch_size,
    shuffle=(train_sampler is None),
    sampler=train_sampler,
    collate_fn=collate_fn,
)
valid_dataloader = DataLoader(
    valid_iter_map,
    batch_size=batch_size,
    shuffle=(valid_sampler is None),
    sampler=valid_sampler,
    collate_fn=collate_fn,
)
return train_dataloader, valid_dataloader</code></pre>
</section>
<section id="markdown-id90qm8rzctsqm" class="level1">
<h1>%% [markdown] id=“90qM8RzCTsqM”</h1>
</section>
<section id="training-the-system" class="level1">
<h1>## Training the System</h1>
</section>
<section id="section-96" class="level1">
<h1>%%</h1>
<p>def train_worker( gpu, ngpus_per_node, vocab_src, vocab_tgt, spacy_de, spacy_en, config, is_distributed=False, ): print(f”Train worker process using GPU: {gpu} for training”, flush=True) torch.cuda.set_device(gpu)</p>
<pre><code>pad_idx = vocab_tgt["&lt;blank&gt;"]
d_model = 512
model = make_model(len(vocab_src), len(vocab_tgt), N=6)
model.cuda(gpu)
module = model
is_main_process = True
if is_distributed:
    dist.init_process_group(
        "nccl", init_method="env://", rank=gpu, world_size=ngpus_per_node
    )
    model = DDP(model, device_ids=[gpu])
    module = model.module
    is_main_process = gpu == 0

criterion = LabelSmoothing(
    size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1
)
criterion.cuda(gpu)

train_dataloader, valid_dataloader = create_dataloaders(
    gpu,
    vocab_src,
    vocab_tgt,
    spacy_de,
    spacy_en,
    batch_size=config["batch_size"] // ngpus_per_node,
    max_padding=config["max_padding"],
    is_distributed=is_distributed,
)

optimizer = torch.optim.Adam(
    model.parameters(), lr=config["base_lr"], betas=(0.9, 0.98), eps=1e-9
)
lr_scheduler = LambdaLR(
    optimizer=optimizer,
    lr_lambda=lambda step: rate(
        step, d_model, factor=1, warmup=config["warmup"]
    ),
)
train_state = TrainState()

for epoch in range(config["num_epochs"]):
    if is_distributed:
        train_dataloader.sampler.set_epoch(epoch)
        valid_dataloader.sampler.set_epoch(epoch)

    model.train()
    print(f"[GPU{gpu}] Epoch {epoch} Training ====", flush=True)
    _, train_state = run_epoch(
        (Batch(b[0], b[1], pad_idx) for b in train_dataloader),
        model,
        SimpleLossCompute(module.generator, criterion),
        optimizer,
        lr_scheduler,
        mode="train+log",
        accum_iter=config["accum_iter"],
        train_state=train_state,
    )

    GPUtil.showUtilization()
    if is_main_process:
        file_path = "%s%.2d.pt" % (config["file_prefix"], epoch)
        torch.save(module.state_dict(), file_path)
    torch.cuda.empty_cache()

    print(f"[GPU{gpu}] Epoch {epoch} Validation ====", flush=True)
    model.eval()
    sloss = run_epoch(
        (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),
        model,
        SimpleLossCompute(module.generator, criterion),
        DummyOptimizer(),
        DummyScheduler(),
        mode="eval",
    )
    print(sloss)
    torch.cuda.empty_cache()

if is_main_process:
    file_path = "%sfinal.pt" % config["file_prefix"]
    torch.save(module.state_dict(), file_path)</code></pre>
</section>
<section id="tags" class="level1">
<h1>%% tags=[]</h1>
<p>def train_distributed_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config): from the_annotated_transformer import train_worker</p>
<pre><code>ngpus = torch.cuda.device_count()
os.environ["MASTER_ADDR"] = "localhost"
os.environ["MASTER_PORT"] = "12356"
print(f"Number of GPUs detected: {ngpus}")
print("Spawning training processes ...")
mp.spawn(
    train_worker,
    nprocs=ngpus,
    args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, True),
)</code></pre>
<p>def train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config): if config[“distributed”]: train_distributed_model( vocab_src, vocab_tgt, spacy_de, spacy_en, config ) else: train_worker( 0, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config, False )</p>
<p>def load_trained_model(): config = { “batch_size”: 32, “distributed”: False, “num_epochs”: 8, “accum_iter”: 10, “base_lr”: 1.0, “max_padding”: 72, “warmup”: 3000, “file_prefix”: “multi30k_model_”, } model_path = “multi30k_model_final.pt” if not exists(model_path): train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)</p>
<pre><code>model = make_model(len(vocab_src), len(vocab_tgt), N=6)
model.load_state_dict(torch.load("multi30k_model_final.pt"))
return model</code></pre>
<p>if is_interactive_notebook(): model = load_trained_model()</p>
</section>
<section id="markdown-idrzk_vjdptsqn" class="level1">
<h1>%% [markdown] id=“RZK_VjDPTsqN”</h1>
</section>
<section id="section-97" class="level1">
<h1></h1>
</section>
<section id="once-trained-we-can-decode-the-model-to-produce-a-set-of" class="level1">
<h1>&gt; Once trained we can decode the model to produce a set of</h1>
</section>
<section id="translations.-here-we-simply-translate-the-first-sentence-in-the" class="level1">
<h1>&gt; translations. Here we simply translate the first sentence in the</h1>
</section>
<section id="validation-set.-this-dataset-is-pretty-small-so-the-translations" class="level1">
<h1>&gt; validation set. This dataset is pretty small so the translations</h1>
</section>
<section id="with-greedy-search-are-reasonably-accurate." class="level1">
<h1>&gt; with greedy search are reasonably accurate.</h1>
</section>
<section id="markdown-idl50i0iextsqn" class="level1">
<h1>%% [markdown] id=“L50i0iEXTsqN”</h1>
</section>
<section id="additional-components-bpe-search-averaging" class="level1">
<h1># Additional Components: BPE, Search, Averaging</h1>
</section>
<section id="markdown-idnbx1c2_ntsqn" class="level1">
<h1>%% [markdown] id=“NBx1C2_NTsqN”</h1>
</section>
<section id="section-98" class="level1">
<h1></h1>
</section>
<section id="so-this-mostly-covers-the-transformer-model-itself.-there-are-four" class="level1">
<h1>&gt; So this mostly covers the transformer model itself. There are four</h1>
</section>
<section id="aspects-that-we-didnt-cover-explicitly.-we-also-have-all-these" class="level1">
<h1>&gt; aspects that we didn’t cover explicitly. We also have all these</h1>
</section>
<section id="additional-features-implemented-in" class="level1">
<h1>&gt; additional features implemented in</h1>
</section>
<section id="opennmt-py." class="level1">
<h1>&gt; <a href="https://github.com/opennmt/opennmt-py">OpenNMT-py</a>.</h1>
</section>
<section id="section-99" class="level1">
<h1></h1>
</section>
<section id="section-100" class="level1">
<h1></h1>
</section>
<section id="markdown-idupqv1mwntsqn" class="level1">
<h1>%% [markdown] id=“UpqV1mWnTsqN”</h1>
</section>
<section id="section-101" class="level1">
<h1></h1>
</section>
<section id="bpe-word-piece-we-can-use-a-library-to-first-preprocess-the" class="level1">
<h1>&gt; 1) BPE/ Word-piece: We can use a library to first preprocess the</h1>
</section>
<section id="data-into-subword-units.-see-rico-sennrichs" class="level1">
<h1>&gt; data into subword units. See Rico Sennrich’s</h1>
</section>
<section id="subword-nmt" class="level1">
<h1>&gt; <a href="https://github.com/rsennrich/subword-nmt">subword-nmt</a></h1>
</section>
<section id="implementation.-these-models-will-transform-the-training-data-to" class="level1">
<h1>&gt; implementation. These models will transform the training data to</h1>
</section>
<section id="look-like-this" class="level1">
<h1>&gt; look like this:</h1>
</section>
<section id="markdown-idhwj_9j0btsqn" class="level1">
<h1>%% [markdown] id=“hwJ_9J0BTsqN”</h1>
</section>
<section id="die-protokoll-datei-kann-heimlich-per-e---mail-oder-ftp" class="level1">
<h1>▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP</h1>
</section>
<section id="an-einen-bestimmte-n-empfänger-gesendet-werden-." class="level1">
<h1>▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .</h1>
</section>
<section id="markdown-id9hwejykptsqn" class="level1">
<h1>%% [markdown] id=“9HwejYkpTsqN”</h1>
</section>
<section id="section-102" class="level1">
<h1></h1>
</section>
<section id="shared-embeddings-when-using-bpe-with-shared-vocabulary-we-can" class="level1">
<h1>&gt; 2) Shared Embeddings: When using BPE with shared vocabulary we can</h1>
</section>
<section id="share-the-same-weight-vectors-between-the-source-target" class="level1">
<h1>&gt; share the same weight vectors between the source / target /</h1>
</section>
<section id="generator.-see-the-cite-for" class="level1">
<h1>&gt; generator. See the <a href="https://arxiv.org/abs/1608.05859">(cite)</a> for</h1>
</section>
<section id="details.-to-add-this-to-the-model-simply-do-this" class="level1">
<h1>&gt; details. To add this to the model simply do this:</h1>
</section>
<section id="idtb3j3cyltsqn-tags" class="level1">
<h1>%% id=“tb3j3CYLTsqN” tags=[]</h1>
<p>if False: model.src_embed[0].lut.weight = model.tgt_embeddings[0].lut.weight model.generator.lut.weight = model.tgt_embed[0].lut.weight</p>
</section>
<section id="markdown-idxdkjsswrtsqn" class="level1">
<h1>%% [markdown] id=“xDKJsSwRTsqN”</h1>
</section>
<section id="section-103" class="level1">
<h1></h1>
</section>
<section id="beam-search-this-is-a-bit-too-complicated-to-cover-here.-see-the" class="level1">
<h1>&gt; 3) Beam Search: This is a bit too complicated to cover here. See the</h1>
</section>
<section id="opennmt-py" class="level1">
<h1>&gt; <a href="https://github.com/OpenNMT/OpenNMT-py/">OpenNMT-py</a></h1>
</section>
<section id="for-a-pytorch-implementation." class="level1">
<h1>&gt; for a pytorch implementation.</h1>
</section>
<section id="section-104" class="level1">
<h1>&gt;</h1>
</section>
<section id="section-105" class="level1">
<h1></h1>
</section>
<section id="markdown-idwf3vvygztsqn" class="level1">
<h1>%% [markdown] id=“wf3vVYGZTsqN”</h1>
</section>
<section id="section-106" class="level1">
<h1></h1>
</section>
<section id="model-averaging-the-paper-averages-the-last-k-checkpoints-to" class="level1">
<h1>&gt; 4) Model Averaging: The paper averages the last k checkpoints to</h1>
</section>
<section id="create-an-ensembling-effect.-we-can-do-this-after-the-fact-if-we" class="level1">
<h1>&gt; create an ensembling effect. We can do this after the fact if we</h1>
</section>
<section id="have-a-bunch-of-models" class="level1">
<h1>&gt; have a bunch of models:</h1>
</section>
<section id="idhafea78jokdb" class="level1">
<h1>%% id=“hAFEa78JokDB”</h1>
<p>def average(model, models): “Average models into model” for ps in zip(<em>[m.params() for m in [model] + models]): ps[0].copy_(torch.sum(</em>ps[1:]) / len(ps[1:]))</p>
</section>
<section id="markdown-idkz5byj9stsqo" class="level1">
<h1>%% [markdown] id=“Kz5BYJ9sTsqO”</h1>
</section>
<section id="results" class="level1">
<h1># Results</h1>
</section>
<section id="section-107" class="level1">
<h1></h1>
</section>
<section id="on-the-wmt-2014-english-to-german-translation-task-the-big" class="level1">
<h1>On the WMT 2014 English-to-German translation task, the big</h1>
</section>
<section id="transformer-model-transformer-big-in-table-2-outperforms-the" class="level1">
<h1>transformer model (Transformer (big) in Table 2) outperforms the</h1>
</section>
<section id="best-previously-reported-models-including-ensembles-by-more-than" class="level1">
<h1>best previously reported models (including ensembles) by more than</h1>
</section>
<section id="bleu-establishing-a-new-state-of-the-art-bleu-score-of" class="level1">
<h1>2.0 BLEU, establishing a new state-of-the-art BLEU score of</h1>
</section>
<section id="the-configuration-of-this-model-is-listed-in-the-bottom-line" class="level1">
<h1>28.4. The configuration of this model is listed in the bottom line</h1>
</section>
<section id="of-table-3.-training-took-3.5-days-on-8-p100-gpus.-even-our-base" class="level1">
<h1>of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base</h1>
</section>
<section id="model-surpasses-all-previously-published-models-and-ensembles-at-a" class="level1">
<h1>model surpasses all previously published models and ensembles, at a</h1>
</section>
<section id="fraction-of-the-training-cost-of-any-of-the-competitive-models." class="level1">
<h1>fraction of the training cost of any of the competitive models.</h1>
</section>
<section id="section-108" class="level1">
<h1></h1>
</section>
<section id="on-the-wmt-2014-english-to-french-translation-task-our-big-model" class="level1">
<h1>On the WMT 2014 English-to-French translation task, our big model</h1>
</section>
<section id="achieves-a-bleu-score-of-41.0-outperforming-all-of-the-previously" class="level1">
<h1>achieves a BLEU score of 41.0, outperforming all of the previously</h1>
</section>
<section id="published-single-models-at-less-than-14-the-training-cost-of-the" class="level1">
<h1>published single models, at less than 1/4 the training cost of the</h1>
</section>
<section id="previous-state-of-the-art-model.-the-transformer-big-model-trained" class="level1">
<h1>previous state-of-the-art model. The Transformer (big) model trained</h1>
</section>
<section id="for-english-to-french-used-dropout-rate-pdrop-0.1-instead-of-0.3." class="level1">
<h1>for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.</h1>
</section>
<section id="section-109" class="level1">
<h1></h1>
</section>
<section id="markdown-3" class="level1">
<h1>%% [markdown]</h1>
</section>
<section id="section-110" class="level1">
<h1><img src="images/results.png" class="img-fluid"></h1>
</section>
<section id="markdown-idcpcnshvqtsqo" class="level1">
<h1>%% [markdown] id=“cPcnsHvQTsqO”</h1>
</section>
<section id="section-111" class="level1">
<h1></h1>
</section>
<section id="section-112" class="level1">
<h1></h1>
</section>
<section id="with-the-addtional-extensions-in-the-last-section-the-opennmt-py" class="level1">
<h1>&gt; With the addtional extensions in the last section, the OpenNMT-py</h1>
</section>
<section id="replication-gets-to-26.9-on-en-de-wmt.-here-i-have-loaded-in-those" class="level1">
<h1>&gt; replication gets to 26.9 on EN-DE WMT. Here I have loaded in those</h1>
</section>
<section id="parameters-to-our-reimplemenation." class="level1">
<h1>&gt; parameters to our reimplemenation.</h1>
</section>
<section id="section-113" class="level1">
<h1>%%</h1>
</section>
<section id="load-data-and-model-for-output-checks" class="level1">
<h1>Load data and model for output checks</h1>
</section>
<section id="section-114" class="level1">
<h1>%%</h1>
<p>def check_outputs( valid_dataloader, model, vocab_src, vocab_tgt, n_examples=15, pad_idx=2, eos_string=“”, ): results = [()] * n_examples for idx in range(n_examples): print(“%d ========” % idx) b = next(iter(valid_dataloader)) rb = Batch(b[0], b[1], pad_idx) greedy_decode(model, rb.src, rb.src_mask, 64, 0)[0]</p>
<pre><code>    src_tokens = [
        vocab_src.get_itos()[x] for x in rb.src[0] if x != pad_idx
    ]
    tgt_tokens = [
        vocab_tgt.get_itos()[x] for x in rb.tgt[0] if x != pad_idx
    ]

    print(
        "Source Text (Input)        : "
        + " ".join(src_tokens).replace("\n", "")
    )
    print(
        "Target Text (Ground Truth) : "
        + " ".join(tgt_tokens).replace("\n", "")
    )
    model_out = greedy_decode(model, rb.src, rb.src_mask, 72, 0)[0]
    model_txt = (
        " ".join(
            [vocab_tgt.get_itos()[x] for x in model_out if x != pad_idx]
        ).split(eos_string, 1)[0]
        + eos_string
    )
    print("Model Output               : " + model_txt.replace("\n", ""))
    results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)
return results</code></pre>
<p>def run_model_example(n_examples=5): global vocab_src, vocab_tgt, spacy_de, spacy_en</p>
<pre><code>print("Preparing Data ...")
_, valid_dataloader = create_dataloaders(
    torch.device("cpu"),
    vocab_src,
    vocab_tgt,
    spacy_de,
    spacy_en,
    batch_size=1,
    is_distributed=False,
)

print("Loading Trained Model ...")

model = make_model(len(vocab_src), len(vocab_tgt), N=6)
model.load_state_dict(
    torch.load("multi30k_model_final.pt", map_location=torch.device("cpu"))
)

print("Checking Model Outputs:")
example_data = check_outputs(
    valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples
)
return model, example_data</code></pre>
</section>
<section id="execute_examplerun_model_example" class="level1">
<h1>execute_example(run_model_example)</h1>
</section>
<section id="markdown-id0zkkntkltsqo" class="level1">
<h1>%% [markdown] id=“0ZkkNTKLTsqO”</h1>
</section>
<section id="attention-visualization" class="level1">
<h1>## Attention Visualization</h1>
</section>
<section id="section-115" class="level1">
<h1></h1>
</section>
<section id="even-with-a-greedy-decoder-the-translation-looks-pretty-good.-we" class="level1">
<h1>&gt; Even with a greedy decoder the translation looks pretty good. We</h1>
</section>
<section id="can-further-visualize-it-to-see-what-is-happening-at-each-layer-of" class="level1">
<h1>&gt; can further visualize it to see what is happening at each layer of</h1>
</section>
<section id="the-attention" class="level1">
<h1>&gt; the attention</h1>
</section>
<section id="section-116" class="level1">
<h1>%%</h1>
<p>def mtx2df(m, max_row, max_col, row_tokens, col_tokens): “convert a dense matrix to a data frame with row and column indices” return pd.DataFrame( [ ( r, c, float(m[r, c]), “%.3d %s” % (r, row_tokens[r] if len(row_tokens) &gt; r else “<blank>”), “%.3d %s” % (c, col_tokens[c] if len(col_tokens) &gt; c else “<blank>”), ) for r in range(m.shape[0]) for c in range(m.shape[1]) if r &lt; max_row and c &lt; max_col ], # if float(m[r,c]) != 0 and r &lt; max_row and c &lt; max_col], columns=[“row”, “column”, “value”, “row_token”, “col_token”], )</blank></blank></p>
<p>def attn_map(attn, layer, head, row_tokens, col_tokens, max_dim=30): df = mtx2df( attn[0, head].data, max_dim, max_dim, row_tokens, col_tokens, ) return ( alt.Chart(data=df) .mark_rect() .encode( x=alt.X(“col_token”, axis=alt.Axis(title=““)), y=alt.Y(”row_token”, axis=alt.Axis(title=““)), color=”value”, tooltip=[“row”, “column”, “value”, “row_token”, “col_token”], ) .properties(height=400, width=400) .interactive() )</p>
</section>
<section id="tags-1" class="level1">
<h1>%% tags=[]</h1>
<p>def get_encoder(model, layer): return model.encoder.layers[layer].self_attn.attn</p>
<p>def get_decoder_self(model, layer): return model.decoder.layers[layer].self_attn.attn</p>
<p>def get_decoder_src(model, layer): return model.decoder.layers[layer].src_attn.attn</p>
<p>def visualize_layer(model, layer, getter_fn, ntokens, row_tokens, col_tokens): # ntokens = last_example[0].ntokens attn = getter_fn(model, layer) n_heads = attn.shape[1] charts = [ attn_map( attn, 0, h, row_tokens=row_tokens, col_tokens=col_tokens, max_dim=ntokens, ) for h in range(n_heads) ] assert n_heads == 8 return alt.vconcat( charts[0] # | charts[1] | charts[2] # | charts[3] | charts[4] # | charts[5] | charts[6] # | charts[7] # layer + 1 due to 0-indexing ).properties(title=“Layer %d” % (layer + 1))</p>
</section>
<section id="markdown-4" class="level1">
<h1>%% [markdown]</h1>
</section>
<section id="encoder-self-attention" class="level1">
<h1>## Encoder Self Attention</h1>
</section>
<section id="tags-2" class="level1">
<h1>%% tags=[]</h1>
<p>def viz_encoder_self(): model, example_data = run_model_example(n_examples=1) example = example_data[ len(example_data) - 1 ] # batch object for the final example</p>
<pre><code>layer_viz = [
    visualize_layer(
        model, layer, get_encoder, len(example[1]), example[1], example[1]
    )
    for layer in range(6)
]
return alt.hconcat(
    layer_viz[0]
    # &amp; layer_viz[1]
    &amp; layer_viz[2]
    # &amp; layer_viz[3]
    &amp; layer_viz[4]
    # &amp; layer_viz[5]
)</code></pre>
<p>show_example(viz_encoder_self)</p>
</section>
<section id="markdown-5" class="level1">
<h1>%% [markdown]</h1>
</section>
<section id="decoder-self-attention" class="level1">
<h1>## Decoder Self Attention</h1>
</section>
<section id="tags-3" class="level1">
<h1>%% tags=[]</h1>
<p>def viz_decoder_self(): model, example_data = run_model_example(n_examples=1) example = example_data[len(example_data) - 1]</p>
<pre><code>layer_viz = [
    visualize_layer(
        model,
        layer,
        get_decoder_self,
        len(example[1]),
        example[1],
        example[1],
    )
    for layer in range(6)
]
return alt.hconcat(
    layer_viz[0]
    &amp; layer_viz[1]
    &amp; layer_viz[2]
    &amp; layer_viz[3]
    &amp; layer_viz[4]
    &amp; layer_viz[5]
)</code></pre>
<p>show_example(viz_decoder_self)</p>
</section>
<section id="markdown-6" class="level1">
<h1>%% [markdown]</h1>
</section>
<section id="decoder-src-attention" class="level1">
<h1>## Decoder Src Attention</h1>
</section>
<section id="tags-4" class="level1">
<h1>%% tags=[]</h1>
<p>def viz_decoder_src(): model, example_data = run_model_example(n_examples=1) example = example_data[len(example_data) - 1]</p>
<pre><code>layer_viz = [
    visualize_layer(
        model,
        layer,
        get_decoder_src,
        max(len(example[1]), len(example[2])),
        example[1],
        example[2],
    )
    for layer in range(6)
]
return alt.hconcat(
    layer_viz[0]
    &amp; layer_viz[1]
    &amp; layer_viz[2]
    &amp; layer_viz[3]
    &amp; layer_viz[4]
    &amp; layer_viz[5]
)</code></pre>
<p>show_example(viz_decoder_src)</p>
</section>
<section id="markdown-idnsseuccatsqo" class="level1">
<h1>%% [markdown] id=“nSseuCcATsqO”</h1>
</section>
<section id="conclusion" class="level1">
<h1># Conclusion</h1>
</section>
<section id="section-117" class="level1">
<h1></h1>
</section>
<section id="hopefully-this-code-is-useful-for-future-research.-please-reach" class="level1">
<h1>Hopefully this code is useful for future research. Please reach</h1>
</section>
<section id="out-if-you-have-any-issues." class="level1">
<h1>out if you have any issues.</h1>
</section>
<section id="section-118" class="level1">
<h1></h1>
</section>
<section id="section-119" class="level1">
<h1></h1>
</section>
<section id="cheers" class="level1">
<h1>Cheers,</h1>
</section>
<section id="sasha-rush-austin-huang-suraj-subramanian-jonathan-sum-khalid-almubarak" class="level1">
<h1>Sasha Rush, Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak,</h1>
</section>
<section id="stella-biderman" class="level1">
<h1>Stella Biderman</h1>
<section id="transformer-architecture" class="level2">
<h2 class="anchored" data-anchor-id="transformer-architecture">Transformer Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="02_attention_is_all_you_need_files/figure-html/d5393d47-1-815012f1-e052-4514-a18c-db31f235d612.png" class="img-fluid figure-img"></p>
<figcaption>pasted_image_815012f1-e052-4514-a18c-db31f235d612.png</figcaption>
</figure>
</div>
<section id="architecture-details" class="level3">
<h3 class="anchored" data-anchor-id="architecture-details">Architecture Details</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="02_attention_is_all_you_need_files/figure-html/3aa6dc75-1-670cb52c-fedb-4185-bb81-749fd2bffd33.png" class="img-fluid figure-img"></p>
<figcaption>pasted_image_670cb52c-fedb-4185-bb81-749fd2bffd33.png</figcaption>
</figure>
</div>
</section>
</section>
<section id="attention-1" class="level2">
<h2 class="anchored" data-anchor-id="attention-1">Attention</h2>
<p>I have given you enough context to implement a transformer from scratch. I want to learn how to implement transformers without cheating. What should I start from first? I also included the code from Annotated Transformer by Harvard NLP. I want to follow this as the benchmark tutorial, but I am not going to see any of the code. I want to do it from scratch.</p>
<section id="reply" class="level5">
<h5 class="anchored" data-anchor-id="reply">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>That’s an excellent learning goal! Implementing a transformer from scratch is one of the best ways to deeply understand how they work. Let me help you get started with a structured approach.</p>
<p>First, let me understand your background better:</p>
<ol type="1">
<li>How comfortable are you with PyTorch (or your preferred deep learning framework)?</li>
<li>Do you understand the core concepts from the “Attention is All You Need” paper - like self-attention, multi-head attention, and positional encoding?</li>
<li>Would you prefer to build this incrementally (starting with the smallest components) or tackle one complete module at a time?</li>
</ol>
<p>For a from-scratch implementation, I’d suggest starting with the <strong>attention mechanism</strong> itself, since it’s the heart of the transformer. Specifically, you could begin by implementing:</p>
<p><strong>Scaled Dot-Product Attention</strong> - the most fundamental building block.</p>
<p>This involves: - Taking query (Q), key (K), and value (V) matrices as input - Computing attention scores - Applying the scaling factor - Using softmax to get attention weights - Returning the weighted sum of values</p>
<p>Would you like me to explain what this function should do conceptually (inputs, outputs, and the mathematical operations), so you can try implementing it yourself? Or would you prefer to start with an even simpler piece, like understanding the data structures you’ll need?</p>
<p>Q * K^T / sqrt(D_K)* V</p>
</section>
<section id="reply-1" class="level5">
<h5 class="anchored" data-anchor-id="reply-1">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! You’ve got the formula down. That’s the core of scaled dot-product attention.</p>
<p>Let me make sure you understand each part:</p>
<ul>
<li><strong>Q · K^T</strong>: This gives you attention scores (how much each query should “attend to” each key)</li>
<li><strong>/ sqrt(d_k)</strong>: The scaling factor prevents the dot products from getting too large</li>
<li><strong>Then softmax</strong> (you didn’t write it, but it goes here!) converts scores to probabilities</li>
<li><strong>· V</strong>: Finally, use those probabilities as weights for the values</li>
</ul>
<p>A couple of questions to check your understanding:</p>
<ol type="1">
<li>What are the dimensions/shapes of Q, K, and V?</li>
<li>Where does the softmax operation fit in that formula you wrote?</li>
</ol>
<p>Once you’re clear on these, you can start coding the attention function. Want to clarify these points first, or are you ready to start implementing?</p>
<p>It makes sure that the outputs add up to one.</p>
</section>
<section id="reply-2" class="level5">
<h5 class="anchored" data-anchor-id="reply-2">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly! Softmax normalizes the scores so they sum to 1, turning them into a proper probability distribution over the values.</p>
<p>So the complete formula is:</p>
<p><strong>Attention(Q, K, V) = softmax(Q · K^T / sqrt(d_k)) · V</strong></p>
<p>Now, before you start coding, let’s think about one more important detail: <strong>masking</strong>.</p>
<p>In the decoder, you need to prevent positions from “looking ahead” at future tokens. This is done by setting certain attention scores to negative infinity before the softmax.</p>
<p>Do you understand why masking is needed and where it fits into the formula? Or should I clarify that first?</p>
<p>Once you’re comfortable with the concept, you can start implementing the <code>attention()</code> function!</p>
<p>Well, it makes sure that the outputs add up to one.</p>
</section>
<section id="reply-3" class="level5">
<h5 class="anchored" data-anchor-id="reply-3">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly! The softmax converts the raw similarity scores into a probability distribution - all values between 0 and 1 that sum to 1.</p>
<p>This is important because it tells us “how much attention” to pay to each value. If one key is very similar to the query, it gets a high weight (close to 1), and we focus mostly on its corresponding value.</p>
<p>Now you’re ready to implement it! Try coding the <code>attention()</code> function. It should: - Take <code>query</code>, <code>key</code>, <code>value</code> as inputs (and optionally <code>mask</code> and <code>dropout</code>) - Return the attention output (and optionally the attention weights for visualization)</p>
<p>Don’t worry about getting it perfect - just give it a try and let me know what you come up with or if you get stuck on any part!</p>
<p>def attention(Q, K, V, dropout, mask): Q@K.T Am I doing the correct thing? I am not sure what I am doing.</p>
</section>
<section id="reply-4" class="level5">
<h5 class="anchored" data-anchor-id="reply-4">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You’re on the right track! You’ve started with the Q @ K.T part, which is correct.</p>
<p>Let me help clarify what you need to do step by step:</p>
<ol type="1">
<li><strong>Q @ K.T</strong> ✓ (you have this!)</li>
<li><strong>Divide by sqrt(d_k)</strong> - you need to scale the scores. What is d_k? (Hint: it’s related to the dimension of your key vectors)</li>
<li><strong>Apply mask</strong> (if provided) - set certain positions to a very negative number</li>
<li><strong>Apply softmax</strong> - convert scores to probabilities</li>
<li><strong>Multiply by V</strong> - get the weighted sum</li>
<li><strong>Apply dropout</strong> (if provided) - for regularization</li>
</ol>
<p>Try adding step 2 next - how would you get d_k from your inputs, and how would you scale the scores?</p>
<p>is it K.shape(0)? also give me code written till now</p>
</section>
<section id="reply-5" class="level5">
<h5 class="anchored" data-anchor-id="reply-5">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Not quite! <code>d_k</code> is the dimension of each key vector, which is the <strong>last dimension</strong> of K.</p>
<p>So it would be: <code>K.shape[-1]</code> (or <code>K.size(-1)</code> in PyTorch)</p>
<p>For the code so far, I want to make sure you’re building this yourself to really learn it. Here’s what you have:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention(Q, K, V, dropout, mask):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">@</span> K.T</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Now try adding: 1. Store that result in a variable (maybe call it <code>scores</code>) 2. Get <code>d_k</code> from the key dimension 3. Divide scores by <code>sqrt(d_k)</code> (you’ll need to import <code>math</code> for <a href="https://docs.python.org/3/library/math.html#math.sqrt"><code>math.sqrt</code></a>)</p>
<p>Give it a try and show me what you get!</p>
<div id="f0109751" class="cell" data-time_run="2025-11-30T18:10:20.947962+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention(Q, K, V, dropout, mask):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    sqrt_d_k <span class="op">=</span> math.sqrt(K.size(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    attention <span class="op">=</span> torch.softmax(Q <span class="op">@</span> K.T <span class="op">/</span> sqrt_d_k)<span class="op">*</span>V</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> attention</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>How am I doing now?</p>
</section>
<section id="reply-6" class="level5">
<h5 class="anchored" data-anchor-id="reply-6">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You’re doing great! You’ve got most of the core pieces in place. A few things to adjust:</p>
<ol type="1">
<li><p><strong>Transpose operation</strong>: In PyTorch, use <code>.transpose(-2, -1)</code> instead of <code>.T</code> to handle batched matrices correctly</p></li>
<li><p><strong>Softmax dimension</strong>: You need to specify which dimension to apply softmax over - it should be the last dimension: <code>torch.softmax(..., dim=-1)</code></p></li>
<li><p><strong>Missing pieces</strong>: You haven’t used <code>mask</code> or <code>dropout</code> yet. The mask needs to be applied <em>before</em> the softmax (by setting masked positions to a very negative number like -1e9)</p></li>
<li><p><strong>Parentheses</strong>: The multiplication by V should happen <em>after</em> the softmax, so make sure your parentheses are right</p></li>
</ol>
<p>Try fixing the transpose and softmax dimension first. Does that make sense?</p>
<p>What is transpose - 2 - 1? I don’t understand what he’s saying. What do you mean applying softmax to the last dimension? I don’t understand. I think I’m weak at all this dimension kind of stuff. Please help me.</p>
</section>
<section id="reply-7" class="level5">
<h5 class="anchored" data-anchor-id="reply-7">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>No worries! Let’s break this down - dimensions can be tricky at first.</p>
<p><strong>Understanding dimensions/axes:</strong></p>
<p>When you have a tensor (multi-dimensional array), each “direction” is called a dimension or axis. For example: - A vector has 1 dimension: <code>[1, 2, 3]</code> has shape <code>(3,)</code> - A matrix has 2 dimensions: rows and columns, shape <code>(rows, cols)</code> - A batch of matrices has 3 dimensions: <code>(batch_size, rows, cols)</code></p>
<p><strong>What is <code>.transpose(-2, -1)</code>?</strong></p>
<p>This swaps the last two dimensions. The <code>-1</code> means “last dimension” and <code>-2</code> means “second-to-last”.</p>
<p>For a matrix with shape <code>(rows, cols)</code>, transposing swaps them to <code>(cols, rows)</code>.</p>
<p>For example, if K has shape <code>(batch, seq_len, d_k)</code>, then <code>K.transpose(-2, -1)</code> gives you <code>(batch, d_k, seq_len)</code>.</p>
<p><strong>What does <code>dim=-1</code> in softmax mean?</strong></p>
<p>It tells PyTorch which dimension to apply softmax across. <code>dim=-1</code> means “apply softmax across the last dimension” - so each row will sum to 1.</p>
<p>Does this help clarify things? Want to try updating your code with these changes, or do you need more explanation on any part?</p>
<p>Okay, I’ll update the code. But after I update the code and the code is proper, let’s pass to an example through it so that I understand better. Give me the code so far.</p>
</section>
<section id="reply-8" class="level5">
<h5 class="anchored" data-anchor-id="reply-8">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>I want you to try updating it yourself first - that’s how you’ll learn best!</p>
<p>Take your current code:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention(Q, K, V, dropout, mask):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    sqrt_d_k <span class="op">=</span> math.sqrt(K.size(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    attention <span class="op">=</span> torch.softmax(Q <span class="op">@</span> K.T <span class="op">/</span> sqrt_d_k)<span class="op">*</span>V</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> attention</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>And make these changes: 1. Change <code>K.T</code> to <code>K.transpose(-2, -1)</code> 2. Add <code>dim=-1</code> to the softmax</p>
<p>Give it a try! Once you’ve updated it, we’ll absolutely run an example through it to see how it works.</p>
<div id="23f84583" class="cell" data-time_run="2025-11-30T18:10:21.482479+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention(Q, K, V, dropout, mask):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    sqrt_d_k <span class="op">=</span> math.sqrt(K.size(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    dropout <span class="op">=</span> torch.nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    attention <span class="op">=</span> torch.softmax(Q <span class="op">@</span> K.transpose(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> sqrt_d_k, dim<span class="op">=-</span><span class="dv">1</span>)<span class="op">@</span>V</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dropout(attention)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Okay, am I done?</p>
</section>
<section id="reply-9" class="level5">
<h5 class="anchored" data-anchor-id="reply-9">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠</p>
<p>Almost there! Two things to fix:</p>
<ol type="1">
<li><p><strong>Mask is not used</strong>: You need to apply the mask to the scores <em>before</em> the softmax. The typical approach is:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span><span class="fl">1e9</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This sets masked positions to a very negative number, so after softmax they become ~0.</p></li>
<li><p><strong>Dropout placement</strong>: Dropout should be applied to the attention weights (after softmax, before multiplying by V), not to the final output.</p></li>
</ol>
<p>Can you restructure your code to: - Compute scores first (Q @ K.transpose…) - Apply mask to scores (if mask is provided) - Apply softmax to get attention weights - Apply dropout to attention weights (if dropout is provided) - Multiply by V - Return the result</p>
<p>Try breaking it into separate lines - it’ll be clearer and easier to debug!</p>
<p>I don’t understand why we need masks for laser clean.</p>
</section>
<section id="reply-10" class="level5">
<h5 class="anchored" data-anchor-id="reply-10">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! Let me explain why masking is needed.</p>
<p><strong>Why do we need masks?</strong></p>
<p>There are two main reasons:</p>
<ol type="1">
<li><p><strong>Padding mask</strong>: When you batch sentences together, they’re often different lengths. You pad shorter sentences with special tokens (like <code>&lt;pad&gt;</code>). You don’t want the model to pay attention to these padding tokens because they’re meaningless. So you mask them out.</p></li>
<li><p><strong>Look-ahead mask (for decoder)</strong>: In the decoder during training, when predicting word 3, the model should only see words 1 and 2, not words 4, 5, 6 (the future). This prevents “cheating” - the model can’t look at future words it’s supposed to predict. This is called a “causal mask” or “subsequent mask”.</p></li>
</ol>
<p><strong>What does masking do?</strong></p>
<p>By setting masked positions to -1e9 (very negative) before softmax, those positions become ~0 after softmax. This means “don’t pay attention to these positions at all.”</p>
<p>Does this make sense? Do you want to see a simple example of how masking works with numbers?</p>
<p>Yes, give me a simple example. I don’t understand what this command is doing. And where exactly is the mask applied?</p>
</section>
<section id="reply-11" class="level5">
<h5 class="anchored" data-anchor-id="reply-11">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! Let me show you a simple numerical example.</p>
<p><strong>Simple masking example:</strong></p>
<p>Let’s say you have attention scores (before softmax):</p>
<pre><code>scores = [2.0, 3.0, 1.0, 4.0]</code></pre>
<p>And a mask that says “ignore positions 2 and 3”:</p>
<pre><code>mask = [1, 1, 0, 0]  # 1 = keep, 0 = ignore</code></pre>
<p><strong>Step 1: Apply mask</strong> (this is where <code>scores.masked_fill(mask == 0, -1e9)</code> happens)</p>
<pre><code>scores = [2.0, 3.0, -1e9, -1e9]</code></pre>
<p><strong>Step 2: Apply softmax</strong></p>
<pre><code>After softmax: [0.27, 0.73, ~0.0, ~0.0]</code></pre>
<p>See? The masked positions become essentially 0 after softmax!</p>
<p><strong>Where is the mask applied?</strong></p>
<p>In your code, it should go right after you compute the scores and BEFORE softmax:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> Q <span class="op">@</span> K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> sqrt_d_k</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span><span class="fl">1e9</span>)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> torch.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Does this clarify it?</p>
</section>
<section id="final-attention-function" class="level3">
<h3 class="anchored" data-anchor-id="final-attention-function">Final Attention Function</h3>
<p>[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m</p>
<p>[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m</p>
<hr>
</section>
<section id="attention-2" class="level3">
<h3 class="anchored" data-anchor-id="attention-2">attention</h3>
<blockquote class="blockquote">
<pre><code> attention (Q, K, V, dropout, mask)</code></pre>
</blockquote>
<p>Cool, is this correct?</p>
<section id="reply-12" class="level5">
<h5 class="anchored" data-anchor-id="reply-12">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent! Your attention function looks correct now! 🎉</p>
<p>You’ve got all the key pieces: - ✓ Compute scores with proper transpose - ✓ Scale by sqrt(d_k) - ✓ Apply mask before softmax - ✓ Apply softmax on the right dimension - ✓ Apply dropout to attention weights - ✓ Multiply by V</p>
<p>One small improvement: it’s common to also return the <code>attention_weights</code> for visualization purposes (optional, but helpful for debugging). But for now, this works!</p>
<p>Want to test it with a simple example to see it in action? Or are you ready to move on to the next component (Multi-Head Attention)?</p>
<p>not that simple. too many 1 and 0 lol</p>
</section>
<section id="reply-13" class="level5">
<h5 class="anchored" data-anchor-id="reply-13">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Haha, fair enough! Let me make it even simpler with different numbers:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Super simple example - just 1 query and 2 keys/values</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> torch.tensor([[<span class="fl">2.0</span>, <span class="fl">2.0</span>]])  <span class="co"># 1 query, d_k=2</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> torch.tensor([[<span class="fl">1.0</span>, <span class="fl">1.0</span>],   <span class="co"># key 1</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>                  [<span class="fl">1.0</span>, <span class="fl">0.0</span>]])  <span class="co"># key 2</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> torch.tensor([[<span class="fl">10.0</span>],       <span class="co"># value 1</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>                  [<span class="fl">20.0</span>]])      <span class="co"># value 2</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> attention(Q, K, V, dropout<span class="op">=</span><span class="fl">0.0</span>, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Now try by hand: 1. Q @ K.T = ? (just dot products: [2,2]·[1,1] and [2,2]·[1,0]) 2. Divide by sqrt(2) ≈ 1.41 3. Apply softmax (which one is bigger?) 4. Weighted sum of values [10] and [20]</p>
<p>Give it a shot!</p>
<p>Q ( 1x2) K_T (2x2) Q@K_t = [4 2] div by sqrt2 = [2sqr(2) sqrt(2)] softmax approx [1 0] result = [10 0[</p>
</section>
<section id="reply-14" class="level5">
<h5 class="anchored" data-anchor-id="reply-14">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great work! You’re very close! Let me check your steps:</p>
<ol type="1">
<li><strong>Q @ K.T</strong>: ✓ Correct! <code>[4, 2]</code></li>
<li><strong>Divide by sqrt(2)</strong>: ✓ Correct! <code>[2√2, √2]</code> which is approximately <code>[2.83, 1.41]</code></li>
<li><strong>Softmax</strong>: Almost! It won’t be exactly <code>[1, 0]</code>. Since 2.83 is bigger than 1.41, the first position will have higher weight, but not exactly 1. It’ll be something like <code>[0.8, 0.2]</code> (the exact values depend on the exponentials)</li>
<li><strong>Result</strong>: Because of softmax, it’s a weighted average: <code>0.8 * 10 + 0.2 * 20 = 8 + 4 = 12</code></li>
</ol>
<p>So the result should be around <code>[[12]]</code> (a weighted average closer to 10 than 20).</p>
<p>Want to run the code now to see the actual numbers?</p>
<p>ya gimme code</p>
</section>
<section id="reply-15" class="level5">
<h5 class="anchored" data-anchor-id="reply-15">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Super simple example - just 1 query and 2 keys/values</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> torch.tensor([[<span class="fl">2.0</span>, <span class="fl">2.0</span>]])  <span class="co"># 1 query, d_k=2</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> torch.tensor([[<span class="fl">1.0</span>, <span class="fl">1.0</span>],   <span class="co"># key 1</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>                  [<span class="fl">1.0</span>, <span class="fl">0.0</span>]])  <span class="co"># key 2</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> torch.tensor([[<span class="fl">10.0</span>],       <span class="co"># value 1</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>                  [<span class="fl">20.0</span>]])      <span class="co"># value 2</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> attention(Q, K, V, dropout<span class="op">=</span><span class="fl">0.0</span>, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Result:"</span>, result)</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Shape:"</span>, result.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this and see what you get! Does it match your hand calculation?</p>
<div id="3ffdf478" class="cell" data-time_run="2025-11-30T18:10:21.684159+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Super simple example - just 1 query and 2 keys/values</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> torch.tensor([[<span class="fl">2.0</span>, <span class="fl">2.0</span>]])  <span class="co"># 1 query, d_k=2</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> torch.tensor([[<span class="fl">1.0</span>, <span class="fl">1.0</span>],   <span class="co"># key 1</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>                  [<span class="fl">1.0</span>, <span class="fl">0.0</span>]])  <span class="co"># key 2</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> torch.tensor([[<span class="fl">10.0</span>],       <span class="co"># value 1</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>                  [<span class="fl">20.0</span>]])      <span class="co"># value 2</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> attention(Q, K, V, dropout<span class="op">=</span><span class="fl">0.0</span>, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Result:"</span>, result)</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Shape:"</span>, result.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[1]</span><span class="ansi-green-fg">, line 10</span>
<span class="ansi-green-fg">      4</span> K = torch.tensor([[<span class="ansi-green-fg">1.0</span>, <span class="ansi-green-fg">1.0</span>],   <span style="font-style:italic;color:rgb(95,135,135)"># key 1</span>
<span class="ansi-green-fg">      5</span>                   [<span class="ansi-green-fg">1.0</span>, <span class="ansi-green-fg">0.0</span>]])  <span style="font-style:italic;color:rgb(95,135,135)"># key 2</span>
<span class="ansi-green-fg">      7</span> V = torch.tensor([[<span class="ansi-green-fg">10.0</span>],       <span style="font-style:italic;color:rgb(95,135,135)"># value 1</span>
<span class="ansi-green-fg">      8</span>                   [<span class="ansi-green-fg">20.0</span>]])      <span style="font-style:italic;color:rgb(95,135,135)"># value 2</span>
<span class="ansi-green-fg">---&gt; </span><span class="ansi-green-fg">10</span> result = <span class="ansi-yellow-bg">attention</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">Q</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">K</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">V</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">dropout</span><span class="ansi-yellow-bg">=</span><span class="ansi-green-fg ansi-yellow-bg">0.0</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">mask</span><span class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">     11</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">Result:</span><span class="ansi-yellow-fg">"</span>, result)
<span class="ansi-green-fg">     12</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">Shape:</span><span class="ansi-yellow-fg">"</span>, result.shape)

<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[1]</span><span class="ansi-green-fg">, line 7</span>, in <span class="ansi-cyan-fg">attention</span><span class="ansi-blue-fg">(Q, K, V, dropout, mask)</span>
<span class="ansi-green-fg">      5</span>     scores = scores.masked_fill(mask==<span class="ansi-green-fg">0</span>, -<span class="ansi-green-fg">1e9</span>)
<span class="ansi-green-fg">      6</span> attention_weights = torch.softmax(scores, dim=-<span class="ansi-green-fg">1</span>)
<span class="ansi-green-fg">----&gt; </span><span class="ansi-green-fg">7</span> attention_weights = <span class="ansi-yellow-bg">dropout</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">attention_weights</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">      8</span> output = attention_weights @ V
<span class="ansi-green-fg">      9</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> output

<span class="ansi-red-fg">TypeError</span>: 'float' object is not callable</pre>
</div>
</div>
</div>
<p>Convert this into a unit as. Assume I’m writing an NB Dev Notebook.</p>
</section>
<section id="reply-16" class="level5">
<h5 class="anchored" data-anchor-id="reply-16">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>I’ll help you convert your attention function into a unit test for an nbdev notebook. Here’s how you can structure it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| export</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention(Q, K, V, dropout<span class="op">=</span><span class="va">None</span>, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Compute Scaled Dot-Product Attention"</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    sqrt_d_k <span class="op">=</span> math.sqrt(K.size(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> Q <span class="op">@</span> K.transpose(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> sqrt_d_k</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> scores.masked_fill(mask<span class="op">==</span><span class="dv">0</span>, <span class="op">-</span><span class="fl">1e9</span>)</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> torch.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> dropout <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> dropout(attention_weights)</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> attention_weights <span class="op">@</span> V</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_attention_basic():</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test attention with simple hand-calculable example"</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> torch.tensor([[<span class="fl">2.0</span>, <span class="fl">2.0</span>]])  <span class="co"># 1 query, d_k=2</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> torch.tensor([[<span class="fl">1.0</span>, <span class="fl">1.0</span>],   <span class="co"># key 1</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>                      [<span class="fl">1.0</span>, <span class="fl">0.0</span>]])  <span class="co"># key 2</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> torch.tensor([[<span class="fl">10.0</span>],       <span class="co"># value 1</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>                      [<span class="fl">20.0</span>]])      <span class="co"># value 2</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> attention(Q, K, V, dropout<span class="op">=</span><span class="va">None</span>, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Result should be weighted average closer to 10 than 20</span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> result.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="fl">11.0</span> <span class="op">&lt;</span> result.item() <span class="op">&lt;</span> <span class="fl">13.0</span>  <span class="co"># approximately 11.96</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>A couple of notes: 1. I made <code>dropout</code> and <code>mask</code> optional parameters with default <code>None</code> 2. The <code>#| export</code> directive marks the function for export to your module 3. The <code>#| test</code> directive marks the test function</p>
<p>Would you like me to add more test cases (e.g., testing with masks, or with batched inputs)?</p>
</section>
<section id="unit-test" class="level4">
<h4 class="anchored" data-anchor-id="unit-test">Unit Test</h4>
<div id="049891c0" class="cell" data-time_run="2025-11-30T18:10:21.761775+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_attention_basic():</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test attention with simple hand-calculable example"</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> torch.tensor([[<span class="fl">2.0</span>, <span class="fl">2.0</span>]])  <span class="co"># 1 query, d_k=2</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> torch.tensor([[<span class="fl">1.0</span>, <span class="fl">1.0</span>],   <span class="co"># key 1</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>                      [<span class="fl">1.0</span>, <span class="fl">0.0</span>]])  <span class="co"># key 2</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> torch.tensor([[<span class="fl">10.0</span>],       <span class="co"># value 1</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>                      [<span class="fl">20.0</span>]])      <span class="co"># value 2</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> attention(Q, K, V, dropout<span class="op">=</span><span class="va">None</span>, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Result should be weighted average closer to 10 than 20</span></span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> result.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="fl">11.0</span> <span class="op">&lt;</span> result.item() <span class="op">&lt;</span> <span class="fl">13.0</span>  <span class="co"># approximately 11.96</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>noice But are the dimensions of q, k, v you have given realistic? What are the real ones?</p>
<section id="reply-17" class="level5">
<h5 class="anchored" data-anchor-id="reply-17">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! The dimensions I gave you were simplified for hand calculation. Let me show you realistic dimensions:</p>
<p><strong>In a real transformer:</strong></p>
<ul>
<li><strong>Batch size (B)</strong>: typically 8, 16, 32, etc.</li>
<li><strong>Sequence length (S)</strong>: how many tokens (words/subwords) - could be 10, 50, 512, etc.</li>
<li><strong>d_model</strong>: the model dimension - typically 512 (base model) or 1024 (large model)</li>
<li><strong>d_k</strong> (key/query dimension): typically d_model / num_heads. For 8 heads and d_model=512, d_k = 64</li>
</ul>
<p><strong>Realistic shapes:</strong></p>
<pre><code>Q: (batch_size, seq_len, d_k)     e.g., (32, 50, 64)
K: (batch_size, seq_len, d_k)     e.g., (32, 50, 64)
V: (batch_size, seq_len, d_v)     e.g., (32, 50, 64)</code></pre>
<p>Note: usually d_k = d_v = 64 when using 8 attention heads.</p>
<p>Want to test your attention function with more realistic dimensions to make sure it handles batches correctly?</p>
<p>ya</p>
</section>
<section id="reply-18" class="level5">
<h5 class="anchored" data-anchor-id="reply-18">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Try this:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Realistic dimensions</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>d_k <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> torch.randn(batch_size, seq_len, d_k)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> torch.randn(batch_size, seq_len, d_k)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> torch.randn(batch_size, seq_len, d_k)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> attention(Q, K, V, dropout<span class="op">=</span><span class="fl">0.0</span>, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Result shape:"</span>, result.shape)</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected shape:"</span>, (batch_size, seq_len, d_k))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this and check if the output shape matches what you expect. Does it work correctly?</p>
<div id="c6a66bde" class="cell" data-time_run="2025-11-30T18:10:21.807686+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Realistic dimensions</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>d_k <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> torch.randn(batch_size, seq_len, d_k)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> torch.randn(batch_size, seq_len, d_k)</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> torch.randn(batch_size, seq_len, d_k)</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> attention(Q, K, V, dropout<span class="op">=</span><span class="fl">0.0</span>, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Result shape:"</span>, result.shape)</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected shape:"</span>, (batch_size, seq_len, d_k))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[1]</span><span class="ansi-green-fg">, line 10</span>
<span class="ansi-green-fg">      7</span> K = torch.randn(batch_size, seq_len, d_k)
<span class="ansi-green-fg">      8</span> V = torch.randn(batch_size, seq_len, d_k)
<span class="ansi-green-fg">---&gt; </span><span class="ansi-green-fg">10</span> result = <span class="ansi-yellow-bg">attention</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">Q</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">K</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">V</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">dropout</span><span class="ansi-yellow-bg">=</span><span class="ansi-green-fg ansi-yellow-bg">0.0</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">mask</span><span class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">     11</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">Result shape:</span><span class="ansi-yellow-fg">"</span>, result.shape)
<span class="ansi-green-fg">     12</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">Expected shape:</span><span class="ansi-yellow-fg">"</span>, (batch_size, seq_len, d_k))

<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[1]</span><span class="ansi-green-fg">, line 7</span>, in <span class="ansi-cyan-fg">attention</span><span class="ansi-blue-fg">(Q, K, V, dropout, mask)</span>
<span class="ansi-green-fg">      5</span>     scores = scores.masked_fill(mask==<span class="ansi-green-fg">0</span>, -<span class="ansi-green-fg">1e9</span>)
<span class="ansi-green-fg">      6</span> attention_weights = torch.softmax(scores, dim=-<span class="ansi-green-fg">1</span>)
<span class="ansi-green-fg">----&gt; </span><span class="ansi-green-fg">7</span> attention_weights = <span class="ansi-yellow-bg">dropout</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">attention_weights</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">      8</span> output = attention_weights @ V
<span class="ansi-green-fg">      9</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> output

<span class="ansi-red-fg">TypeError</span>: 'float' object is not callable</pre>
</div>
</div>
</div>
<p>What is the philosophy behind attention? What is Q, what is K, what is V? Give a proper analogy.</p>
</section>
<section id="reply-19" class="level5">
<h5 class="anchored" data-anchor-id="reply-19">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! Let me give you an intuitive analogy.</p>
<p><strong>The Library Analogy:</strong></p>
<p>Imagine you’re in a library looking for information about “transformers in NLP”:</p>
<ul>
<li><strong>Query (Q)</strong>: Your question/search term - “What are transformers?”</li>
<li><strong>Keys (K)</strong>: The titles/keywords of all books on the shelves</li>
<li><strong>Values (V)</strong>: The actual content of those books</li>
</ul>
<p><strong>How attention works:</strong></p>
<ol type="1">
<li>You compare your query against all the book titles (Q · K^T) - which books are most relevant?</li>
<li>You get similarity scores - “Deep Learning” book scores high, “Cooking 101” scores low</li>
<li>Softmax turns these into weights - how much to focus on each book</li>
<li>You read a weighted combination of the books (weights · V) - paying more attention to relevant ones</li>
</ol>
<p><strong>In transformers:</strong></p>
<ul>
<li>Each word creates its own Query: “What information do I need?”</li>
<li>Each word provides a Key: “I have this type of information”</li>
<li>Each word provides a Value: “Here’s my actual information”</li>
</ul>
<p>The word “bank” (query) might attend strongly to “river” (high attention weight) vs “money” depending on context!</p>
<p>Does this help clarify the philosophy?</p>
<p>Okay, so I get that, but how does query, and the key, and the value relate to attention overall? Isn’t that just multiplying inputs with WQ and WK? How is that converting to query and key?</p>
</section>
<section id="reply-20" class="level5">
<h5 class="anchored" data-anchor-id="reply-20">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent question! You’re getting to the heart of how attention actually works.</p>
<p><strong>You’re absolutely right!</strong> Q, K, and V are created by multiplying the input with learned weight matrices:</p>
<pre><code>Q = Input @ W_Q
K = Input @ W_K  
V = Input @ W_V</code></pre>
<p><strong>Why do we do this?</strong></p>
<p>The input embeddings are just generic word representations. By multiplying with W_Q, W_K, W_V, we’re <strong>learning to transform</strong> the input into three different “views”:</p>
<ul>
<li><strong>W_Q learns</strong>: “How should I ask questions?” (what to look for)</li>
<li><strong>W_K learns</strong>: “How should I advertise what I have?” (what I can provide)</li>
<li><strong>W_V learns</strong>: “What information should I actually pass along?” (the content to share)</li>
</ul>
<p><strong>Example:</strong> The word “bank” has one embedding. But: - As a Query (W_Q): it might learn to ask “what’s nearby?” - As a Key (W_K): it might learn to signal “I’m a noun” - As a Value (W_V): it might learn to pass along semantic meaning</p>
<p>The magic is that W_Q, W_K, W_V are <strong>learned during training</strong> to make attention useful!</p>
<p>Does this connection make sense?</p>
<p>Excellent. Let’s move on.</p>
</section>
<section id="reply-21" class="level5">
<h5 class="anchored" data-anchor-id="reply-21">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! You’ve successfully implemented the core attention mechanism.</p>
<p>Now the next step is <strong>Multi-Head Attention</strong>. This is where we run multiple attention operations in parallel (multiple “heads”), each learning different patterns.</p>
<p>The key idea: instead of one attention, we have <code>h</code> heads (typically 8), each with its own W_Q, W_K, W_V matrices. Then we concatenate all the outputs and project them back.</p>
<p>Before we start coding, do you understand conceptually: 1. Why we’d want multiple attention heads instead of just one? 2. How the dimensions change when we split into multiple heads?</p>
<p>Or should I explain the intuition first?</p>
<p>We need multiple attention heads because we want to see: - One word can have a correlation with other words - Another attention head can give another perspective</p>
<p>How the dimension changes when we split into multiple heads? Maybe the input dimensions is divided by the number of tangent heads. That’s how it splits.</p>
</section>
<section id="reply-22" class="level5">
<h5 class="anchored" data-anchor-id="reply-22">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly right on both points! 🎉</p>
<p><strong>Why multiple heads:</strong> Each head can learn different relationships - one might focus on syntactic patterns (subject-verb), another on semantic similarity, another on positional relationships, etc.</p>
<p><strong>Dimension splitting:</strong> Yes! If d_model = 512 and we have 8 heads, each head works with d_k = 512/8 = 64 dimensions. This keeps the total computation roughly the same as single-head attention.</p>
<p><strong>The process:</strong> 1. Start with input of shape (batch, seq_len, d_model=512) 2. Project to Q, K, V using W_Q, W_K, W_V 3. <strong>Reshape/split</strong> into h heads: (batch, seq_len, h=8, d_k=64) 4. Run attention on each head in parallel 5. <strong>Concatenate</strong> heads back together: (batch, seq_len, d_model=512) 6. Final linear projection with W_O</p>
<p>Ready to start implementing? What should we build first - the reshaping logic or the overall MultiHeadAttention class structure?</p>
<p>I don’t know what’s easier to start with.</p>
</section>
<section id="reply-23" class="level5">
<h5 class="anchored" data-anchor-id="reply-23">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Let’s start with the <strong>overall class structure</strong> first - it’s easier to see the big picture, then fill in the details.</p>
<p>A MultiHeadedAttention class needs: - <code>__init__</code>: store hyperparameters (h, d_model) and create the weight matrices - <code>forward</code>: take input, split into heads, run attention, concatenate, project</p>
<p>Try writing just the skeleton:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadedAttention(nn.Module):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, h, d_model, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do we need to store here?</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What layers/parameters do we need?</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What steps go here?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Think about: what parameters does this class need to store? (Hint: you need 4 linear layers - why 4?)</p>
<p>Give it a try!</p>
<p>i havent a clu where to start</p>
</section>
<section id="reply-24" class="level5">
<h5 class="anchored" data-anchor-id="reply-24">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>No worries! Let me break it down into smaller pieces.</p>
<p><strong>What does MultiHeadedAttention need to do?</strong></p>
<p>Remember: we need to project Q, K, V, then split into heads, run attention, concatenate, and project again.</p>
<p><strong>Start with the <code>__init__</code> method. You need:</strong></p>
<ol type="1">
<li>Store <code>h</code> (number of heads) and <code>d_k</code> (dimension per head)</li>
<li>Create <strong>4 linear layers</strong>:
<ul>
<li>One for projecting to Q</li>
<li>One for projecting to K<br>
</li>
<li>One for projecting to V</li>
<li>One for the final output projection (after concatenating heads)</li>
</ul></li>
</ol>
<p>Try writing just the <code>__init__</code> method:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadedAttention(nn.Module):</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, h, d_model, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> h</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> h  <span class="co"># dimension per head</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create 4 linear layers here</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each should be nn.Linear(d_model, d_model)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Can you add the 4 linear layers? Maybe store them in a list or as separate attributes?</p>
<p>So, bro, instead of doing all this guesswork, should I read the paper first and then come back to this notebook?</p>
</section>
<section id="reply-25" class="level5">
<h5 class="anchored" data-anchor-id="reply-25">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠</p>
<p>That’s a totally fair question! There’s no single right answer - it depends on how you learn best.</p>
<p><strong>Option A: Read the paper first</strong> - Pros: You’ll understand the “why” behind each component, the math notation, and the full architecture before coding - Cons: The paper can be dense; some parts might not click until you actually implement them</p>
<p><strong>Option B: Keep coding, refer to paper as needed</strong> - Pros: Hands-on learning, immediate feedback, you discover the “why” as you hit roadblocks - Cons: Can feel like guesswork (which is what you’re feeling now!)</p>
<p><strong>Option C: Hybrid approach</strong> (my suggestion) - Read just the <strong>Section 3.2 (Attention)</strong> of the paper - it’s only ~1.5 pages - It explains Multi-Head Attention with diagrams and equations - Then come back and implement with that understanding</p>
<p>The paper is here: https://arxiv.org/pdf/1706.03762 - Section 3.2 starts around page 3.</p>
<p>What sounds best to you? Would you like to take a break and read that section, or would you prefer I explain the Multi-Head Attention concept more thoroughly here first?</p>
<p>ill read the architecture part first i guess. all the misc experiments i can see later</p>
</section>
<section id="reply-26" class="level5">
<h5 class="anchored" data-anchor-id="reply-26">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great plan! Reading the architecture section first will give you the foundation you need.</p>
<p><strong>What to focus on:</strong></p>
<ul>
<li><strong>Section 3</strong>: Model Architecture (pages 3-5)
<ul>
<li>3.1: Overall encoder-decoder structure</li>
<li>3.2: Attention mechanisms (especially Multi-Head Attention)</li>
<li>3.3: Position-wise Feed-Forward Networks</li>
<li>3.4: Embeddings and Positional Encoding</li>
</ul></li>
</ul>
<p>You can skip sections 4 (training details) and 5-6 (experiments/results) for now.</p>
<p>Take your time reading, and when you come back, let me know: 1. What clicked for you? 2. What’s still confusing? 3. Whether you’re ready to continue implementing Multi-Head Attention!</p>
<p>See you when you’re back! 📚</p>
</section>
</section>
</section>
</section>
<section id="reading-paper" class="level2">
<h2 class="anchored" data-anchor-id="reading-paper">Reading Paper</h2>
<div id="9128c347" class="cell" data-time_run="2025-11-30T18:10:21.884266+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pypdf2</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PyPDF2 <span class="im">import</span> PdfReader</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>reader <span class="op">=</span> PdfReader(<span class="st">'../Attention.pdf'</span>)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join([page.extract_text() <span class="cf">for</span> page <span class="kw">in</span> reader.pages])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Requirement already satisfied: pypdf2 in /app/data/.local/lib/python3.12/site-packages (3.0.1)</code></pre>
</div>
</div>
<p>Attention Paper Text $<code>text</code></p>
<p>is the attention paper in context now?</p>
<section id="reply-27" class="level5">
<h5 class="anchored" data-anchor-id="reply-27">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Yes! I can see you’ve loaded the Attention paper PDF and extracted the text into the <code>text</code> variable. The paper content is now available in your context.</p>
<p>Would you like me to help you: 1. Find specific sections (like Section 3.2 on Multi-Head Attention)? 2. Explain parts that are confusing as you read? 3. Answer questions about what you’ve read so far?</p>
<p>Or are you still reading through it on your own first?</p>
<p>What does transduction mean?</p>
</section>
<section id="reply-28" class="level5">
<h5 class="anchored" data-anchor-id="reply-28">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good question! <strong>Transduction</strong> in the context of this paper means <strong>transforming one sequence into another sequence</strong>.</p>
<p><strong>Examples of transduction tasks:</strong> - <strong>Machine translation</strong>: English sentence → French sentence - <strong>Text summarization</strong>: Long article → Short summary<br>
- <strong>Speech recognition</strong>: Audio sequence → Text sequence</p>
<p>It’s basically any task where you take an input sequence and produce an output sequence, which might be in a different “form” or language.</p>
<p>The paper calls the Transformer a “transduction model” because it takes a source sequence (e.g., English) and produces a target sequence (e.g., German).</p>
<p>Does that make sense? What section are you reading now?</p>
<p>The encoder transforms the sequence X to sequence Z. What are they referring to?</p>
</section>
<section id="reply-29" class="level5">
<h5 class="anchored" data-anchor-id="reply-29">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! Let me clarify what X and Z represent:</p>
<p><strong>X (input sequence)</strong>: The original input to the encoder - Example: The English sentence “Hello world” - Represented as token embeddings: (x₁, x₂, …, xₙ) - These are discrete tokens converted to vectors</p>
<p><strong>Z (continuous representations)</strong>: The encoder’s output - A sequence of continuous vector representations: (z₁, z₂, …, zₙ) - Same length as input, but now “enriched” with context from the whole sentence - Each zᵢ contains information about xᵢ AND its relationship to all other tokens</p>
<p><strong>The transformation</strong>: X → Z - X is the “raw” embedded input - Z is the contextualized representation after all the encoder layers process it - Z captures meaning, relationships, and context</p>
<p>Then the decoder uses Z (along with previous outputs) to generate the target sequence Y.</p>
<p>Does that clarify it? What part of the architecture are you reading about now?</p>
<p>I’m reading Section 3.1.</p>
</section>
<section id="reply-30" class="level5">
<h5 class="anchored" data-anchor-id="reply-30">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Section 3.1 covers the overall encoder-decoder structure.</p>
<p><strong>Key points in 3.1:</strong></p>
<ul>
<li><strong>Encoder</strong>: Takes input sequence, produces contextualized representations (the Z we just discussed)</li>
<li><strong>Decoder</strong>: Takes Z + previously generated outputs, produces next token (auto-regressive)</li>
<li>Both encoder and decoder are stacks of N=6 identical layers</li>
<li>Each layer has sub-layers with residual connections and layer normalization</li>
</ul>
<p>As you read through, are the concepts of: 1. <strong>Stacked layers</strong> (N=6 identical layers on top of each other) clear? 2. <strong>Auto-regressive generation</strong> (decoder generates one token at a time, using previous outputs) making sense?</p>
<p>Let me know what questions come up or when you’re ready to move to Section 3.2 (the attention mechanisms)!</p>
<p>Why do we divide by square root of dk?</p>
</section>
<section id="reply-31" class="level5">
<h5 class="anchored" data-anchor-id="reply-31">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent question! This is one of the key insights in the attention mechanism.</p>
<p><strong>The problem without scaling:</strong></p>
<p>When d_k (the dimension) is large, the dot products Q·K^T can get very large in magnitude.</p>
<p><strong>Why is this bad?</strong></p>
<p>Large dot products → softmax gets pushed into regions with extremely small gradients → model can’t learn well (vanishing gradients)</p>
<p><strong>The math intuition:</strong></p>
<p>If Q and K have components that are random variables with mean 0 and variance 1, then their dot product has: - Mean: 0 - Variance: d_k (grows with dimension!)</p>
<p>By dividing by √d_k, we scale the variance back down to ~1, keeping the values in a reasonable range for softmax.</p>
<p><strong>In simple terms:</strong> It prevents the attention scores from getting too extreme, which keeps the softmax working properly and gradients flowing during training.</p>
<p>Does this make sense? Have you gotten to Section 3.2.1 yet where they explain this?</p>
<p>Give me an actual numerical example.</p>
</section>
<section id="reply-32" class="level5">
<h5 class="anchored" data-anchor-id="reply-32">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let me show you with actual numbers.</p>
<p><strong>Without scaling (d_k = 64):</strong></p>
<p>Say Q and K are random vectors with values around -1 to 1:</p>
<pre><code>Q = [0.5, -0.3, 0.8, ..., 0.2]  (64 dimensions)
K = [0.4, 0.6, -0.5, ..., 0.7]  (64 dimensions)

Q·K = 0.5*0.4 + (-0.3)*0.6 + ... (64 terms)
    ≈ 25.3  (large number!)</code></pre>
<p><strong>After softmax on large scores:</strong></p>
<pre><code>scores = [25.3, 18.7, 30.1, 12.4]
softmax = [0.82, 0.001, 0.999, 0.0001]  ← Very extreme! Almost one-hot</code></pre>
<p><strong>With scaling (divide by √64 = 8):</strong></p>
<pre><code>scaled_scores = [3.16, 2.34, 3.76, 1.55]
softmax = [0.35, 0.15, 0.45, 0.05]  ← Much smoother distribution</code></pre>
<p>See the difference? Without scaling, softmax becomes too “confident” (almost all weight on one item). With scaling, you get a more balanced attention distribution.</p>
<p>Does this numerical example clarify why we need the √d_k scaling?</p>
<p>So, these scores are what for each sequence length?</p>
</section>
<section id="reply-33" class="level5">
<h5 class="anchored" data-anchor-id="reply-33">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! Let me clarify what these scores represent.</p>
<p><strong>The scores are for each pair of positions in the sequence.</strong></p>
<p>If you have a sequence of length 4 (e.g., 4 words), you get a <strong>4×4 matrix</strong> of scores:</p>
<pre><code>        word1  word2  word3  word4
word1   [25.3  18.7  30.1  12.4]  ← how much word1 attends to each word
word2   [20.1  22.5  15.3  28.7]  ← how much word2 attends to each word
word3   [19.2  31.4  21.8  16.5]  ← how much word3 attends to each word
word4   [27.6  14.2  23.1  19.8]  ← how much word4 attends to each word</code></pre>
<p>Each <strong>row</strong> is one query asking “how much should I attend to each position?”</p>
<p>Each <strong>column</strong> is one key saying “here’s how relevant I am to each query”</p>
<p>So for a sequence of length n, you get an <strong>n×n attention score matrix</strong>.</p>
<p>Does that clarify what the scores represent?</p>
<p>The paper we mentioned, “Additive Attention”, what the heck is that? Give some examples with numbers as well.</p>
</section>
<section id="reply-34" class="level5">
<h5 class="anchored" data-anchor-id="reply-34">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good question! <strong>Additive attention</strong> is an older attention mechanism that came before the dot-product attention used in Transformers.</p>
<p><strong>Additive Attention (Bahdanau attention):</strong></p>
<p>Instead of Q·K^T, it uses a small neural network:</p>
<pre><code>score = v^T · tanh(W₁·Q + W₂·K)</code></pre>
<p>Where v, W₁, W₂ are learned parameters.</p>
<p><strong>Numerical example:</strong></p>
<p>Say d_k = 3, and we have:</p>
<pre><code>Q = [1, 2, 1]
K = [2, 0, 1]

Step 1: Linear transformations
W₁·Q = [0.5, 1.5]  (project to hidden dim=2)
W₂·K = [1.0, 0.5]

Step 2: Add and apply tanh
tanh([0.5+1.0, 1.5+0.5]) = tanh([1.5, 2.0]) = [0.91, 0.96]

Step 3: Project to scalar
v^T · [0.91, 0.96] = [0.5, 0.6] · [0.91, 0.96] = 1.03</code></pre>
<p><strong>Key difference:</strong> Additive uses a feedforward network, dot-product just multiplies. Dot-product is faster (just matrix multiplication) which is why Transformers use it.</p>
<p>Does this comparison make sense?</p>
<p>The paper says it works better for larger values of decay. So do we use additive attention for larger values of decay?</p>
</section>
<section id="reply-35" class="level5">
<h5 class="anchored" data-anchor-id="reply-35">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good catch reading carefully! Let me clarify what the paper actually says:</p>
<p>The paper says <strong>additive attention outperforms dot-product attention WITHOUT scaling</strong> for larger values of d_k.</p>
<p>But then they add the scaling factor (dividing by √d_k), which <strong>fixes the problem</strong>. So with scaling, dot-product attention works well even for large d_k.</p>
<p><strong>In practice:</strong> - Transformers use <strong>scaled dot-product attention</strong> (what you implemented) - Additive attention is rarely used anymore because scaled dot-product is: - Faster (just matrix multiplication, highly optimized) - More space-efficient - Works just as well with the scaling fix</p>
<p>So no, we don’t switch to additive attention for large d_k. We just use the √d_k scaling to keep dot-product working well!</p>
<p>Does that clear up the confusion?</p>
<p>So, in practise, we are never using additive attention, am I correct?</p>
</section>
<section id="reply-36" class="level5">
<h5 class="anchored" data-anchor-id="reply-36">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly correct!</p>
<p>In modern practice, <strong>we don’t use additive attention</strong>. The Transformer paper mentions it mainly for historical context and to explain why they chose scaled dot-product attention.</p>
<p><strong>What we actually use:</strong> - Scaled dot-product attention (what you’ve already implemented!) - It’s faster, simpler, and works great with the √d_k scaling</p>
<p>The paper just wanted to acknowledge that additive attention exists and explain why dot-product + scaling is the better choice.</p>
<p>Are you ready to move on to Section 3.2.2 about Multi-Head Attention, or do you have more questions about the attention mechanism itself?</p>
<p>Okay, I’m reading this. I’m kind of lost. We take q, k, and v as inputs and project them using wiq, wiv, and wik. Am I correct?</p>
</section>
<section id="reply-37" class="level5">
<h5 class="anchored" data-anchor-id="reply-37">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Yes, you’re on the right track! Let me clarify the flow:</p>
<p><strong>In Multi-Head Attention:</strong></p>
<ol type="1">
<li>You start with the <strong>same input</strong> (let’s call it X)</li>
<li>You project it to get Q, K, V:
<ul>
<li>Q = X @ W^Q</li>
<li>K = X @ W^K<br>
</li>
<li>V = X @ W^V</li>
</ul></li>
<li><strong>Then for multi-head</strong>, each head has its own projection matrices:
<ul>
<li>Head i uses: W^Q_i, W^K_i, W^V_i</li>
<li>So you get Q_i, K_i, V_i for each head</li>
</ul></li>
<li>Run attention on each head separately</li>
<li>Concatenate all head outputs</li>
<li>Final projection with W^O</li>
</ol>
<p><strong>Key point:</strong> You’re projecting the input X multiple times (once per head), not projecting Q, K, V again.</p>
<p>Does this clarify the flow? Or are you still confused about which projections happen when?</p>
<p>Oh, so the outputs are D_K. Which is basically the input length divided by the number of heads. Am I correct?</p>
</section>
<section id="reply-38" class="level5">
<h5 class="anchored" data-anchor-id="reply-38">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Not quite! Let me clarify the dimensions:</p>
<p><strong>d_k is NOT the input length.</strong> It’s the <strong>dimension per head</strong>.</p>
<p>Here’s the breakdown:</p>
<ul>
<li><strong>d_model</strong>: The full model dimension (e.g., 512)</li>
<li><strong>h</strong>: Number of heads (e.g., 8)</li>
<li><strong>d_k</strong>: Dimension per head = d_model / h (e.g., 512/8 = 64)</li>
</ul>
<p><strong>The shapes:</strong></p>
<pre><code>Input X: (batch, seq_len, d_model)
         e.g., (32, 50, 512)

After projecting and splitting into heads:
Q, K, V: (batch, h, seq_len, d_k)
         e.g., (32, 8, 50, 64)</code></pre>
<p>So <strong>seq_len</strong> (input length) stays the same. We’re splitting the <strong>feature dimension</strong> (d_model) across heads, not the sequence length!</p>
<p>Does this distinction make sense?</p>
<p>But we are basically multiplying the whole d_model input with the W to make projections of size d_k. Am I correct?</p>
</section>
<section id="reply-39" class="level5">
<h5 class="anchored" data-anchor-id="reply-39">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Almost! Let me clarify the projection step:</p>
<p><strong>What actually happens:</strong></p>
<p>You multiply the input (d_model) with W to get an output that’s <strong>still d_model size</strong>, then you <strong>reshape/split</strong> it into h heads of size d_k each.</p>
<pre><code>Input: (batch, seq_len, d_model=512)
       ↓
W^Q projection: (d_model=512, d_model=512)
       ↓
Q: (batch, seq_len, d_model=512)
       ↓
Reshape/split into heads: (batch, h=8, seq_len, d_k=64)</code></pre>
<p>So the projection matrix W^Q is <strong>512×512</strong>, not 512×64. You get the full d_model output, then split it into heads.</p>
<p><strong>Alternative approach</strong> (mathematically equivalent): Some implementations do use h separate smaller matrices (d_model → d_k), but conceptually it’s the same.</p>
<p>Does this clarify the projection dimensions?</p>
<p>Okay, I’m pretty much saying the same thing because the paper talks about WI. Which means the weight for each head.</p>
</section>
<section id="reply-40" class="level5">
<h5 class="anchored" data-anchor-id="reply-40">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Ah yes, you’re absolutely right! I apologize for the confusion.</p>
<p>You’re correct - the paper uses <strong>W^Q_i, W^K_i, W^V_i</strong> for each head i, which are the <strong>individual projection matrices for that specific head</strong>.</p>
<p>So yes, each head has its own: - W^Q_i: projects from d_model to d_k - W^K_i: projects from d_model to d_k<br>
- W^V_i: projects from d_model to d_k</p>
<p>Then you run attention separately on each head’s Q_i, K_i, V_i.</p>
<p>You had it right! The paper does describe it with separate weight matrices per head (the subscript i indicates which head).</p>
<p>So to confirm: you multiply the d_model input with W_i to get projections of size d_k for each head. ✓</p>
<p>Ready to keep reading, or want to clarify anything else about multi-head attention?</p>
<p>Well, I understand your point too that in application, you just have a consolidated W. Am I correct?</p>
</section>
<section id="reply-41" class="level5">
<h5 class="anchored" data-anchor-id="reply-41">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly! You’ve got it!</p>
<p><strong>In practice (implementation):</strong> - You often have one big consolidated W^Q matrix (d_model → d_model) - Then you reshape/split the output into h heads of d_k each - This is computationally more efficient (one big matrix multiply vs.&nbsp;h small ones)</p>
<p><strong>Conceptually (in the paper):</strong> - They describe it as h separate W^Q_i matrices (d_model → d_k) - This makes the math clearer to understand</p>
<p>Both views are equivalent - just different ways of thinking about/implementing the same operation!</p>
<p>Have you finished reading Section 3.2.2 on Multi-Head Attention? Ready to start implementing it, or do you want to read more of the paper first?</p>
</section>
</section>
<section id="multiheadedattention" class="level2">
<h2 class="anchored" data-anchor-id="multiheadedattention">MultiHeadedAttention</h2>
<p>yes lets implement now</p>
<section id="reply-42" class="level5">
<h5 class="anchored" data-anchor-id="reply-42">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let’s implement Multi-Head Attention.</p>
<p>We’ll start with the <code>__init__</code> method. You need to store: 1. Number of heads (h) and dimension per head (d_k) 2. The projection matrices (we’ll use the consolidated approach - 4 linear layers) 3. A dropout layer</p>
<p>Try writing this:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadedAttention(nn.Module):</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, h, d_model, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_model <span class="op">%</span> h <span class="op">==</span> <span class="dv">0</span>  <span class="co"># d_model must be divisible by h</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> h</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> h</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create 4 linear layers (Q, K, V projections + final output)</span></span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: you can use a list or create them individually</span></span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Can you add the 4 linear layers? Each should be <code>nn.Linear(d_model, d_model)</code>.</p>
<div id="b265321b" class="cell" data-time_run="2025-11-30T18:10:22.127511+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadedAttention(torch.nn.Module):</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, h, d_model, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_model <span class="op">%</span> h <span class="op">==</span> <span class="dv">0</span>  <span class="co"># d_model must be divisible by h</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> h</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> h</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> torch.nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn<span class="op">=</span><span class="va">None</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create 4 linear layers (Q, K, V projections + final output)</span></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: you can use a list or create them individually</span></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_layers <span class="op">=</span> torch.nn.ModuleList([torch.nn.Linear(d_model, d_model) <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>all good?</p>
</section>
<section id="reply-43" class="level5">
<h5 class="anchored" data-anchor-id="reply-43">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent! Your <code>__init__</code> is perfect! 🎉</p>
<p>You’ve got: - ✓ d_k calculation - ✓ All 4 linear layers in a ModuleList - ✓ Dropout layer - ✓ self.attn for storing attention weights (good thinking!)</p>
<p>Now let’s implement the <code>forward</code> method. The steps are:</p>
<ol type="1">
<li>Get batch size</li>
<li>Project input to Q, K, V using first 3 linear layers</li>
<li>Reshape from (batch, seq_len, d_model) to (batch, h, seq_len, d_k)</li>
<li>Apply attention function</li>
<li>Reshape back and concatenate heads</li>
<li>Apply final linear layer</li>
</ol>
<p>Try starting with steps 1-2. Can you write the code to: - Get the batch size from the query input - Apply the first 3 linear layers to query, key, value?</p>
<div id="30a661ed" class="cell" data-time_run="2025-11-30T18:10:22.176080+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Project query, key, value using the first 3 linear layers</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">0</span>](query)</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">1</span>](key)</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">2</span>](value)</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Reshape to split into h heads</span></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> Q.size(<span class="dv">0</span>)</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>    seq_len <span class="op">=</span> Q.size(<span class="dv">1</span>)</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> Q.view(batch_size, <span class="va">self</span>.h, seq_len, <span class="va">self</span>.d_k)</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> K.view(batch_size, <span class="va">self</span>.h, seq_len, <span class="va">self</span>.d_k)</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> V.view(batch_size, <span class="va">self</span>.h, seq_len, <span class="va">self</span>.d_k)</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Apply attention</span></span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: Concatenate heads back together</span></span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 5: Apply final linear projection</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>So far, so good?</p>
</section>
<section id="reply-44" class="level5">
<h5 class="anchored" data-anchor-id="reply-44">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great progress! Your projection and reshaping look good. One small issue with the reshape:</p>
<p>The order should be <code>(batch_size, seq_len, h, d_k)</code> first, then use <code>.transpose(1, 2)</code> to get <code>(batch_size, h, seq_len, d_k)</code>.</p>
<p>Try changing:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> Q.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This makes the reshaping cleaner. Does that make sense?</p>
<p>Once you fix that, try Step 3 - call your <code>attention()</code> function with Q, K, V. What do you get back, and what shape will it be?</p>
<p>Why is that cleaner? I don’t understand.</p>
</section>
<section id="reply-45" class="level5">
<h5 class="anchored" data-anchor-id="reply-45">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good question! Let me explain why.</p>
<p><strong>Your approach:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>Q.view(batch_size, <span class="va">self</span>.h, seq_len, <span class="va">self</span>.d_k)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This directly reshapes the flat d_model dimension into (h, seq_len, d_k), but the data in memory isn’t organized that way yet.</p>
<p><strong>The cleaner approach:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>Q.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Step 1: <code>view(batch_size, seq_len, self.h, self.d_k)</code> - splits d_model into (h, d_k) while keeping seq_len intact Step 2: <code>.transpose(1, 2)</code> - swaps seq_len and h dimensions to get (batch_size, h, seq_len, d_k)</p>
<p><strong>Why is it cleaner?</strong> The view operation splits the last dimension naturally (d_model → h × d_k), which matches how the data is laid out in memory. Then transpose just reorders the dimensions.</p>
<p>Your way might work, but the view→transpose pattern is the standard approach and less likely to cause weird reshaping bugs.</p>
<p>Want to try updating your code with this pattern?</p>
<p>Oh, so I understand now. We are first trying to split DM into H pieces. Then we swap the dimension edge and sequence length. So that we can pass it into H attention heads.</p>
</section>
<section id="reply-46" class="level5">
<h5 class="anchored" data-anchor-id="reply-46">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly! You’ve got it! 🎉</p>
<p><strong>The logic:</strong> 1. Split d_model into h pieces of size d_k: <code>(batch, seq_len, d_model) → (batch, seq_len, h, d_k)</code> 2. Swap so heads come before seq_len: <code>(batch, seq_len, h, d_k) → (batch, h, seq_len, d_k)</code> 3. Now each head can process its own (seq_len, d_k) slice independently in parallel!</p>
<p>This organization makes it easy to run attention on all h heads at once.</p>
<p>Now try updating your reshape code with this pattern, then move to Step 3 - calling the <code>attention()</code> function. What will you pass to it?</p>
<div id="937be209" class="cell" data-time_run="2025-11-30T18:10:22.235871+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Project query, key, value using the first 3 linear layers</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">0</span>](query)</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">1</span>](key)</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">2</span>](value)</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Reshape to split into h heads</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> Q.size(<span class="dv">0</span>)</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>    seq_len <span class="op">=</span> Q.size(<span class="dv">1</span>)</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> Q.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> K.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> V.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Apply attention</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: Concatenate heads back together</span></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 5: Apply final linear projection</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>better now?</p>
</section>
<section id="reply-47" class="level5">
<h5 class="anchored" data-anchor-id="reply-47">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your reshaping is now correct. ✓</p>
<p>Now for Step 3 - applying attention. You need to call your <code>attention()</code> function.</p>
<p>What arguments will you pass to it? Remember the signature:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>attention(Q, K, V, dropout, mask)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What should each argument be? Give it a try!</p>
<p>good?</p>
</section>
<section id="reply-48" class="level5">
<h5 class="anchored" data-anchor-id="reply-48">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Almost! A few small issues:</p>
<ol type="1">
<li><strong>self.mask doesn’t exist</strong> - you should pass the <code>mask</code> parameter from the function arguments</li>
<li><strong>You’re passing mask twice</strong> - check your attention function signature</li>
<li><strong>Store the result</strong> - you need to capture what attention returns</li>
</ol>
<p>Your attention function signature is:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention(Q, K, V, dropout, mask):</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>So it should be:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> attention(Q, K, V, <span class="va">self</span>.dropout, mask)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Also, one more thing: if mask is provided, you need to unsqueeze it to match the head dimension. Try:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> mask.unsqueeze(<span class="dv">1</span>)  <span class="co"># Add head dimension</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Can you fix these issues?</p>
<p>What is mask unsqueeze 1? I don’t understand.</p>
</section>
<section id="reply-49" class="level5">
<h5 class="anchored" data-anchor-id="reply-49">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good question! Let me explain <code>unsqueeze</code>.</p>
<p><strong>What is unsqueeze?</strong></p>
<p>It adds a new dimension to a tensor at the specified position.</p>
<p><strong>Example:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>]])  <span class="co"># shape: (1, 4)</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>mask.unsqueeze(<span class="dv">0</span>)  <span class="co"># shape: (1, 1, 4) - adds dim at position 0</span></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>mask.unsqueeze(<span class="dv">1</span>)  <span class="co"># shape: (1, 1, 4) - adds dim at position 1</span></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>mask.unsqueeze(<span class="dv">2</span>)  <span class="co"># shape: (1, 4, 1) - adds dim at position 2</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why do we need it here?</strong></p>
<p>Your mask comes in with shape: <code>(batch_size, seq_len, seq_len)</code></p>
<p>But after splitting into heads, Q, K, V have shape: <code>(batch_size, h, seq_len, d_k)</code></p>
<p>So the mask needs to be: <code>(batch_size, 1, seq_len, seq_len)</code> - the <code>1</code> broadcasts across all h heads.</p>
<p><code>mask.unsqueeze(1)</code> adds that dimension at position 1 (after batch_size).</p>
<p>Does this make sense? Want to see a numerical example?</p>
<p>I still don’t understand how the one broadcast across all H heads.</p>
</section>
<section id="reply-50" class="level5">
<h5 class="anchored" data-anchor-id="reply-50">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! Let me explain <strong>broadcasting</strong>.</p>
<p><strong>What is broadcasting?</strong></p>
<p>When you do operations between tensors of different shapes, PyTorch automatically “stretches” dimensions of size 1 to match the other tensor.</p>
<p><strong>Example:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.tensor([[[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]]])  <span class="co"># shape: (1, 1, 3)</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> torch.tensor([[[<span class="dv">10</span>], [<span class="dv">20</span>]]])  <span class="co"># shape: (1, 2, 1)</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">+</span> B  <span class="co"># Result shape: (1, 2, 3)</span></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="co"># The 1s get "broadcast" (repeated) to match</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>In our case:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>mask shape:      (batch, <span class="dv">1</span>,     seq_len, seq_len)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>attention shape: (batch, h<span class="op">=</span><span class="dv">8</span>,   seq_len, seq_len)</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>                          ↑</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>                    This <span class="dv">1</span> broadcasts to <span class="dv">8</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The mask with dimension 1 automatically gets <strong>copied/repeated</strong> 8 times (once for each head) when you apply it.</p>
<p>So the same mask applies to all heads, but PyTorch handles the copying automatically!</p>
<p>Does this clarify how broadcasting works?</p>
<div id="6ed41cdd" class="cell" data-time_run="2025-11-30T18:10:22.284388+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Project query, key, value using the first 3 linear layers</span></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">0</span>](query)</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">1</span>](key)</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">2</span>](value)</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Reshape to split into h heads</span></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> Q.size(<span class="dv">0</span>)</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>    seq_len <span class="op">=</span> Q.size(<span class="dv">1</span>)</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> Q.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> K.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> V.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Apply attention</span></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> mask.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> attention(Q,K,V, <span class="va">self</span>.dropout, mask)</span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: Concatenate heads back together</span></span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 5: Apply final linear projection</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this better now?</p>
</section>
<section id="reply-51" class="level5">
<h5 class="anchored" data-anchor-id="reply-51">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent! Step 3 is now correct! ✓</p>
<p>You’ve got: - ✓ Mask unsqueeze for broadcasting across heads - ✓ Calling attention with the right arguments - ✓ Storing result in <code>x</code></p>
<p>Now for <strong>Step 4: Concatenate heads back together</strong></p>
<p>After attention, <code>x</code> has shape: <code>(batch_size, h, seq_len, d_k)</code></p>
<p>You need to get back to: <code>(batch_size, seq_len, d_model)</code></p>
<p>This is the reverse of what you did earlier: 1. Transpose to swap h and seq_len back 2. Reshape/view to merge h and d_k into d_model</p>
<p>Try writing the code for step 4. What operations do you need?</p>
<p>Wait, I’m seriously confused. How is reshaping the whole thing led to multiple attention heads? this is the attn fn def attention(Q, K, V, dropout, mask): sqrt_d_k = math.sqrt(K.size(-1)) dropout_layer = torch.nn.Dropout(p=dropout) scores = Q @ K.transpose(-2,-1) / sqrt_d_k if mask is not None: scores = scores.masked_fill(mask==0, -1e9) attention_weights = torch.softmax(scores, dim=-1) attention_weights = dropout_layer(attention_weights) output = attention_weights @ V return output</p>
</section>
<section id="reply-52" class="level5">
<h5 class="anchored" data-anchor-id="reply-52">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent question! This is a key insight about how multi-head attention works.</p>
<p><strong>The magic is in the tensor dimensions!</strong></p>
<p>When you pass Q, K, V with shape <code>(batch_size, h, seq_len, d_k)</code> to the attention function, PyTorch’s matrix operations work <strong>independently on each head automatically</strong>.</p>
<p>Let’s trace through it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>Q shape: (batch<span class="op">=</span><span class="dv">2</span>, h<span class="op">=</span><span class="dv">8</span>, seq_len<span class="op">=</span><span class="dv">10</span>, d_k<span class="op">=</span><span class="dv">64</span>)</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>K shape: (batch<span class="op">=</span><span class="dv">2</span>, h<span class="op">=</span><span class="dv">8</span>, seq_len<span class="op">=</span><span class="dv">10</span>, d_k<span class="op">=</span><span class="dv">64</span>)</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>Q <span class="op">@</span> K.transpose(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The <code>@</code> operator does matrix multiplication on the <strong>last two dimensions</strong> only. The first dimensions (batch, h) are treated as “batch dimensions” - so it runs 2×8=16 separate attention operations in parallel!</p>
<p><strong>In other words:</strong> - You have 8 heads - Each head processes its slice: Q[:, 0, :, :], Q[:, 1, :, :], … Q[:, 7, :, :] - The matrix operations happen independently for each head - All in one efficient operation!</p>
<p>Does this click now? The reshaping sets up the dimensions so that one function call runs all heads in parallel!</p>
<p>Okay, this makes sense and clicks. No wonder attention (incl multiheaded attention) is so much faster than previous methods like R and N and LSTM.</p>
</section>
<section id="reply-53" class="level5">
<h5 class="anchored" data-anchor-id="reply-53">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly! You’ve hit on one of the key innovations! 🎉</p>
<p><strong>Why Transformers are faster:</strong></p>
<ul>
<li><strong>RNNs/LSTMs</strong>: Process sequentially (word 1 → word 2 → word 3…). Can’t parallelize across the sequence.</li>
<li><strong>Transformers</strong>: All positions processed at once! The entire attention matrix is computed in parallel.</li>
</ul>
<p>Plus, with multi-head attention, all 8 heads also run in parallel thanks to the clever tensor reshaping you just understood.</p>
<p>This parallelization is what makes Transformers scale so well with modern GPUs!</p>
<p>Now, ready to finish Step 4? You need to concatenate the heads back together. Remember: - Current shape: <code>(batch_size, h, seq_len, d_k)</code> - Target shape: <code>(batch_size, seq_len, d_model)</code></p>
<p>What operations do you need? (Hint: reverse of what you did earlier)</p>
<div id="393a274e" class="cell" data-time_run="2025-11-30T18:10:22.343087+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Project query, key, value using the first 3 linear layers</span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">0</span>](query)</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">1</span>](key)</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">2</span>](value)</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Reshape to split into h heads</span></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> Q.size(<span class="dv">0</span>)</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>    seq_len <span class="op">=</span> Q.size(<span class="dv">1</span>)</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> Q.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> K.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> V.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Apply attention</span></span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> mask.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> attention(Q,K,V, <span class="va">self</span>.dropout, mask)</span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: Concatenate heads back together</span></span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.transpose(<span class="dv">1</span>,<span class="dv">2</span>).view(batch_size,seq_len, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb88-19"><a href="#cb88-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 5: Apply final linear projection</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this correct?</p>
</section>
<section id="reply-54" class="level5">
<h5 class="anchored" data-anchor-id="reply-54">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Step 4 is correct! ✓</p>
<p>You’ve done: - <code>.transpose(1,2)</code> to swap h and seq_len back: <code>(batch, h, seq_len, d_k) → (batch, seq_len, h, d_k)</code> - <code>.view(batch_size, seq_len, -1)</code> to merge the last two dims: <code>(batch, seq_len, h, d_k) → (batch, seq_len, d_model)</code></p>
<p>The <code>-1</code> automatically calculates h × d_k = d_model. Nice!</p>
<p>Now for <strong>Step 5: Apply final linear projection</strong></p>
<p>You need to pass <code>x</code> through the 4th linear layer (the output projection W^O). This mixes information across all the concatenated heads.</p>
<p>Can you add that final step?</p>
<div id="118dc776" class="cell" data-time_run="2025-11-30T18:10:22.391017+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Project query, key, value using the first 3 linear layers</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">0</span>](query)</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">1</span>](key)</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">2</span>](value)</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Reshape to split into h heads</span></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> Q.size(<span class="dv">0</span>)</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>    seq_len <span class="op">=</span> Q.size(<span class="dv">1</span>)</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> Q.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> K.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> V.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Apply attention</span></span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> mask.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> attention(Q,K,V, <span class="va">self</span>.dropout, mask)</span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: Concatenate heads back together</span></span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.transpose(<span class="dv">1</span>,<span class="dv">2</span>).view(batch_size,seq_len, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 5: Apply final linear projection</span></span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">3</span>](x)</span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Am I successful now?</p>
</section>
<section id="reply-55" class="level5">
<h5 class="anchored" data-anchor-id="reply-55">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent work! Your Multi-Head Attention implementation is complete! 🎉</p>
<p>You’ve successfully implemented: - ✓ Projecting to Q, K, V - ✓ Splitting into multiple heads - ✓ Running attention on all heads in parallel - ✓ Concatenating heads back - ✓ Final output projection</p>
<p>Now let’s test it to make sure it works! Try this:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with realistic dimensions</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> mha.forward(x, x, x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this and see if it works! What do you get?</p>
<p>Wait a minute, shouldn’t we put the class and the function together?</p>
</section>
<section id="reply-56" class="level5">
<h5 class="anchored" data-anchor-id="reply-56">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good catch! Yes, you need to add the <code>forward</code> method <strong>inside</strong> the <code>MultiHeadedAttention</code> class!</p>
<p>Right now you have the class definition with <code>__init__</code>, but the <code>forward</code> method is separate. You need to indent the <code>forward</code> method so it’s part of the class.</p>
<p>It should look like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadedAttention(torch.nn.Module):</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, h, d_model, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... your init code ...</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... your forward code ...</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Can you combine them properly?</p>
<div id="5f267327" class="cell" data-time_run="2025-11-30T18:10:22.439276+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadedAttention(torch.nn.Module):</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, h, d_model, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_model <span class="op">%</span> h <span class="op">==</span> <span class="dv">0</span>  <span class="co"># d_model must be divisible by h</span></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> h</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> h</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> torch.nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn<span class="op">=</span><span class="va">None</span></span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create 4 linear layers (Q, K, V projections + final output)</span></span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: you can use a list or create them individually</span></span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_layers <span class="op">=</span> torch.nn.ModuleList([torch.nn.Linear(d_model, d_model) <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)])</span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: Project query, key, value using the first 3 linear layers</span></span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">0</span>](query)</span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">1</span>](key)</span>
<span id="cb92-19"><a href="#cb92-19" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">2</span>](value)</span>
<span id="cb92-20"><a href="#cb92-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: Reshape to split into h heads</span></span>
<span id="cb92-21"><a href="#cb92-21" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> Q.size(<span class="dv">0</span>)</span>
<span id="cb92-22"><a href="#cb92-22" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> Q.size(<span class="dv">1</span>)</span>
<span id="cb92-23"><a href="#cb92-23" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> Q.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb92-24"><a href="#cb92-24" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> K.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb92-25"><a href="#cb92-25" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> V.view(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb92-26"><a href="#cb92-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: Apply attention</span></span>
<span id="cb92-27"><a href="#cb92-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb92-28"><a href="#cb92-28" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> mask.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb92-29"><a href="#cb92-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> attention(Q,K,V, <span class="va">self</span>.dropout, mask)</span>
<span id="cb92-30"><a href="#cb92-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: Concatenate heads back together</span></span>
<span id="cb92-31"><a href="#cb92-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>,<span class="dv">2</span>).view(batch_size,seq_len, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb92-32"><a href="#cb92-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: Apply final linear projection</span></span>
<span id="cb92-33"><a href="#cb92-33" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">3</span>](x)</span>
<span id="cb92-34"><a href="#cb92-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>all good?</p>
</section>
<section id="reply-57" class="level5">
<h5 class="anchored" data-anchor-id="reply-57">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>MultiHeadedAttention</code> class is now complete and properly structured! ✓</p>
<p>Now let’s test it to make sure everything works:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with realistic dimensions</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> mha.forward(x, x, x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this and let me know if it works! Does the output shape match what you expect?</p>
<div id="b537959b" class="cell" data-time_run="2025-11-30T18:10:22.499137+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with realistic dimensions</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> mha.forward(x, x, x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">RuntimeError</span>                              Traceback (most recent call last)
<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[1]</span><span class="ansi-green-fg">, line 5</span>
<span class="ansi-green-fg">      2</span> mha = MultiHeadedAttention(h=<span class="ansi-green-fg">8</span>, d_model=<span class="ansi-green-fg">512</span>, dropout=<span class="ansi-green-fg">0.1</span>)
<span class="ansi-green-fg">      3</span> x = torch.randn(<span class="ansi-green-fg">2</span>, <span class="ansi-green-fg">10</span>, <span class="ansi-green-fg">512</span>)  <span style="font-style:italic;color:rgb(95,135,135)"># batch=2, seq_len=10, d_model=512</span>
<span class="ansi-green-fg">----&gt; </span><span class="ansi-green-fg">5</span> output = <span class="ansi-yellow-bg">mha</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">forward</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">mask</span><span class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">      6</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">Output shape:</span><span class="ansi-yellow-fg">"</span>, output.shape)
<span class="ansi-green-fg">      7</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">Expected:</span><span class="ansi-yellow-fg">"</span>, (<span class="ansi-green-fg">2</span>, <span class="ansi-green-fg">10</span>, <span class="ansi-green-fg">512</span>))

<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[1]</span><span class="ansi-green-fg">, line 31</span>, in <span class="ansi-cyan-fg">MultiHeadedAttention.forward</span><span class="ansi-blue-fg">(self, query, key, value, mask)</span>
<span class="ansi-green-fg">     29</span> x = attention(Q,K,V, <span style="color:rgb(0,135,0)">self</span>.dropout, mask)
<span class="ansi-green-fg">     30</span> <span style="font-style:italic;color:rgb(95,135,135)"># Step 4: Concatenate heads back together</span>
<span class="ansi-green-fg">---&gt; </span><span class="ansi-green-fg">31</span> x = <span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">transpose</span><span class="ansi-yellow-bg">(</span><span class="ansi-green-fg ansi-yellow-bg">1</span><span class="ansi-yellow-bg">,</span><span class="ansi-green-fg ansi-yellow-bg">2</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">view</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">batch_size</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg">seq_len</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">-</span><span class="ansi-green-fg ansi-yellow-bg">1</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">     32</span> <span style="font-style:italic;color:rgb(95,135,135)"># Step 5: Apply final linear projection</span>
<span class="ansi-green-fg">     33</span> output = <span style="color:rgb(0,135,0)">self</span>.linear_layers[<span class="ansi-green-fg">3</span>](x)

<span class="ansi-red-fg">RuntimeError</span>: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.</pre>
</div>
</div>
</div>
<p>no, whyyyy?</p>
</section>
<section id="reply-58" class="level5">
<h5 class="anchored" data-anchor-id="reply-58">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Don’t worry, this is a common issue! The error is telling you exactly what to do.</p>
<p><strong>The problem:</strong> After <code>.transpose()</code>, the tensor is no longer “contiguous” in memory, so <code>.view()</code> doesn’t work.</p>
<p><strong>The fix:</strong> Use <code>.contiguous()</code> before <code>.view()</code>, or just use <code>.reshape()</code> instead:</p>
<p>Change this line:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x.transpose(<span class="dv">1</span>,<span class="dv">2</span>).view(batch_size, seq_len, <span class="op">-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>To either:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x.transpose(<span class="dv">1</span>,<span class="dv">2</span>).contiguous().view(batch_size, seq_len, <span class="op">-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Or simply:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x.transpose(<span class="dv">1</span>,<span class="dv">2</span>).reshape(batch_size, seq_len, <span class="op">-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><code>.reshape()</code> handles the contiguity automatically! Try updating your code and run the test again.</p>
<p>I don’t understand, what do you mean by “contiguous?” What exactly is the issue?</p>
</section>
<section id="reply-59" class="level5">
<h5 class="anchored" data-anchor-id="reply-59">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! Let me explain <strong>contiguous memory</strong>.</p>
<p><strong>What is contiguous?</strong></p>
<p>Contiguous means the tensor’s elements are stored in a single, continuous block of memory in the order you’d expect.</p>
<p><strong>Example:</strong></p>
<p>Original tensor: <code>[[1, 2], [3, 4]]</code> stored in memory as: <code>[1, 2, 3, 4]</code> ✓ contiguous</p>
<p>After transpose: <code>[[1, 3], [2, 4]]</code> but memory is still: <code>[1, 2, 3, 4]</code></p>
<p>PyTorch just changes how it <em>interprets</em> the memory (using strides), but doesn’t actually move the data. So now it’s not contiguous anymore!</p>
<p><strong>Why does <code>.view()</code> care?</strong></p>
<p><code>.view()</code> requires contiguous memory because it’s a “zero-copy” operation - it just reinterprets the existing memory layout. If the memory isn’t laid out right, it can’t do that.</p>
<p><strong>Solutions:</strong> - <code>.contiguous()</code> - actually rearranges memory to be contiguous (makes a copy) - <code>.reshape()</code> - automatically calls <code>.contiguous()</code> if needed</p>
<p>Does this make sense? Try using <code>.reshape()</code> instead of <code>.view()</code> and it should work!</p>
</section>
<section id="final-multiheaded-attention-class" class="level3">
<h3 class="anchored" data-anchor-id="final-multiheaded-attention-class">Final MultiHeaded Attention Class</h3>
<hr>
</section>
<section id="multiheadedattention-1" class="level3">
<h3 class="anchored" data-anchor-id="multiheadedattention-1">MultiHeadedAttention</h3>
<blockquote class="blockquote">
<pre><code> MultiHeadedAttention (h, d_model, dropout=0.1)</code></pre>
</blockquote>
<p>*Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:<code>to</code>, etc.</p>
<p>.. note:: As per the example above, an <code>__init__()</code> call to the parent class must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*</p>
<p>Is this better?</p>
<section id="reply-60" class="level5">
<h5 class="anchored" data-anchor-id="reply-60">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Now let’s test it again:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with realistic dimensions</span></span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> mha.forward(x, x, x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this and see if it works now!</p>
<div id="d5bb3f9e" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with realistic dimensions</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> mha.forward(x, x, x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Awesome. Convert this into unit test. Assume I’m using NB dev.</p>
</section>
<section id="reply-61" class="level5">
<h5 class="anchored" data-anchor-id="reply-61">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Here’s your Multi-Head Attention as a unit test for nbdev:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| export</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadedAttention(torch.nn.Module):</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Multi-Head Attention mechanism"</span></span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, h, d_model, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_model <span class="op">%</span> h <span class="op">==</span> <span class="dv">0</span>  <span class="co"># d_model must be divisible by h</span></span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> h</span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> h</span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> torch.nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> <span class="va">None</span></span>
<span id="cb102-12"><a href="#cb102-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb102-13"><a href="#cb102-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create 4 linear layers (Q, K, V projections + final output)</span></span>
<span id="cb102-14"><a href="#cb102-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_layers <span class="op">=</span> torch.nn.ModuleList([torch.nn.Linear(d_model, d_model) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)])</span>
<span id="cb102-15"><a href="#cb102-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb102-16"><a href="#cb102-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb102-17"><a href="#cb102-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: Project query, key, value using the first 3 linear layers</span></span>
<span id="cb102-18"><a href="#cb102-18" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">0</span>](query)</span>
<span id="cb102-19"><a href="#cb102-19" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">1</span>](key)</span>
<span id="cb102-20"><a href="#cb102-20" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">2</span>](value)</span>
<span id="cb102-21"><a href="#cb102-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: Reshape to split into h heads</span></span>
<span id="cb102-22"><a href="#cb102-22" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> Q.size(<span class="dv">0</span>)</span>
<span id="cb102-23"><a href="#cb102-23" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> Q.size(<span class="dv">1</span>)</span>
<span id="cb102-24"><a href="#cb102-24" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> Q.reshape(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb102-25"><a href="#cb102-25" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> K.reshape(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb102-26"><a href="#cb102-26" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> V.reshape(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb102-27"><a href="#cb102-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: Apply attention</span></span>
<span id="cb102-28"><a href="#cb102-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb102-29"><a href="#cb102-29" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> mask.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb102-30"><a href="#cb102-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> attention(Q, K, V, <span class="va">self</span>.dropout, mask)</span>
<span id="cb102-31"><a href="#cb102-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: Concatenate heads back together</span></span>
<span id="cb102-32"><a href="#cb102-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>,<span class="dv">2</span>).reshape(batch_size, seq_len, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb102-33"><a href="#cb102-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: Apply final linear projection</span></span>
<span id="cb102-34"><a href="#cb102-34" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.linear_layers[<span class="dv">3</span>](x)</span>
<span id="cb102-35"><a href="#cb102-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_multihead_attention():</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Multi-Head Attention with realistic dimensions"</span></span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>    mha <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> mha.forward(x, x, x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Great work! You’ve successfully implemented Multi-Head Attention from scratch! 🎉</p>
<p>What would you like to tackle next? Position-wise Feed-Forward Networks, or something else from the architecture?</p>
</section>
<section id="unit-test-1" class="level4">
<h4 class="anchored" data-anchor-id="unit-test-1">Unit Test</h4>
<div id="4057bdc9" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_multihead_attention():</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Multi-Head Attention with realistic dimensions"</span></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>    mha <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> mha.forward(x, x, x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="a9e57b65" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>test_multihead_attention()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Great, I’ve exported using NB Dev, now let’s move on.</p>
<section id="reply-62" class="level5">
<h5 class="anchored" data-anchor-id="reply-62">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Awesome! 🎉 Your Multi-Head Attention is working perfectly!</p>
<p>You’ve now implemented two major components: 1. ✓ Scaled Dot-Product Attention 2. ✓ Multi-Head Attention</p>
<p><strong>What’s next?</strong></p>
<p>Looking at the Transformer architecture, the main components left are:</p>
<ol type="1">
<li><strong>Position-wise Feed-Forward Network</strong> - a simple 2-layer MLP applied to each position</li>
<li><strong>Positional Encoding</strong> - adds position information to embeddings</li>
<li><strong>Layer Normalization &amp; Residual Connections</strong> - for stable training</li>
<li><strong>Encoder Layer</strong> - combines attention + feed-forward</li>
<li><strong>Decoder Layer</strong> - similar but with masked attention + cross-attention</li>
<li><strong>Full Encoder/Decoder stacks</strong></li>
</ol>
<p>Which would you like to tackle next? I’d suggest the <strong>Feed-Forward Network</strong> since it’s simpler and will give you a quick win!</p>
<p>I just have one question. So, all these linear layers we are using During multi-headed attention, why aren’t there any non-linearities like ReLU at the end of these linear layers?</p>
</section>
<section id="reply-63" class="level5">
<h5 class="anchored" data-anchor-id="reply-63">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent question! This is a really important design choice.</p>
<p><strong>Why no non-linearity in the projections?</strong></p>
<p>The Q, K, V projections are meant to be <strong>linear transformations</strong> of the input into different subspaces. Adding non-linearities would change the nature of the dot-product attention mechanism.</p>
<p>Think about it: attention computes similarity via dot products (Q·K^T). If you applied ReLU before this, you’d be computing similarity between “rectified” representations, which doesn’t have the same mathematical properties.</p>
<p><strong>Where DO we use non-linearities?</strong></p>
<p>The non-linearity (ReLU) comes in the <strong>Feed-Forward Network</strong> that follows the attention layer! That’s where the model gets its non-linear transformation capacity.</p>
<p><strong>The pattern in each layer:</strong> 1. Multi-Head Attention (linear projections only) 2. Add &amp; Norm (residual connection) 3. <strong>Feed-Forward Network (has ReLU!)</strong> ← non-linearity here 4. Add &amp; Norm</p>
<p>So the model does have non-linearities, just not inside the attention projections themselves.</p>
<p>Does this distinction make sense?</p>
</section>
</section>
</section>
</section>
<section id="positionwisefeedforward" class="level2">
<h2 class="anchored" data-anchor-id="positionwisefeedforward">PositionwiseFeedForward</h2>
<p>Yes, this makes sense. Let’s move on to the next topic. Make sure we’re sticking to the code which is given in the beginning.</p>
<section id="reply-64" class="level5">
<h5 class="anchored" data-anchor-id="reply-64">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let’s move on to the <strong>Position-wise Feed-Forward Network</strong>.</p>
<p>Looking at the paper (Section 3.3), this is a simple component:</p>
<p><strong>FFN(x) = max(0, xW₁ + b₁)W₂ + b₂</strong></p>
<p>It’s just: 1. A linear layer that expands dimensions (d_model → d_ff) 2. ReLU activation 3. Another linear layer that projects back (d_ff → d_model)</p>
<p>Typically d_ff = 2048 when d_model = 512 (4x expansion).</p>
<p>Try implementing the class:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionwiseFeedForward(nn.Module):</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, d_ff, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you need here?</span></span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What's the sequence of operations?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What layers do you need in <code>__init__</code>? Give it a try!</p>
<div id="a67d4ed9" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionwiseFeedForward(torch.nn.Module):</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, d_ff, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you need here?</span></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_layer <span class="op">=</span> torch.nn.Linear(d_model, d_ff)</span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> torch.nn.ReLU()</span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> torch.nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> torch.nn.Linear(d_ff, d_model)</span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What's the sequence of operations?</span></span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this correct?</p>
</section>
<section id="reply-65" class="level5">
<h5 class="anchored" data-anchor-id="reply-65">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent! Your <code>__init__</code> is perfect! ✓</p>
<p>You’ve got all the pieces: - ✓ First linear layer (d_model → d_ff) - ✓ ReLU activation - ✓ Dropout - ✓ Second linear layer (d_ff → d_model)</p>
<p>Now for the <code>forward</code> method - what’s the sequence of operations?</p>
<p>Apply them in order: linear → relu → dropout → linear</p>
<p>Give it a try!</p>
</section>
<section id="final-positionwisefeedforward-class" class="level3">
<h3 class="anchored" data-anchor-id="final-positionwisefeedforward-class">Final PositionwiseFeedForward Class</h3>
<hr>
</section>
<section id="positionwisefeedforward-1" class="level3">
<h3 class="anchored" data-anchor-id="positionwisefeedforward-1">PositionwiseFeedForward</h3>
<blockquote class="blockquote">
<pre><code> PositionwiseFeedForward (d_model, d_ff, dropout=0.1)</code></pre>
</blockquote>
<p>*Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:<code>to</code>, etc.</p>
<p>.. note:: As per the example above, an <code>__init__()</code> call to the parent class must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*</p>
<p>Should be fine, right?</p>
<section id="reply-66" class="level5">
<h5 class="anchored" data-anchor-id="reply-66">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your Feed-Forward Network is complete and correct! ✓</p>
<p>The sequence is exactly right: - ✓ Expand with first linear layer - ✓ Apply ReLU non-linearity - ✓ Apply dropout for regularization - ✓ Project back with second linear layer</p>
<p>Let’s test it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>ffn <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> ffn(x)</span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this to verify it works, then we can move on to the next component!</p>
<div id="949a636a" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>ffn <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> ffn(x)</span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Assume we are using NB dev. Convert this into a unit test.</p>
</section>
<section id="reply-67" class="level5">
<h5 class="anchored" data-anchor-id="reply-67">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Here’s your Position-wise Feed-Forward Network as a unit test for nbdev:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| export</span></span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionwiseFeedForward(torch.nn.Module):</span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Position-wise Feed-Forward Network"</span></span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, d_ff, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_layer <span class="op">=</span> torch.nn.Linear(d_model, d_ff)</span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> torch.nn.ReLU()</span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> torch.nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb112-9"><a href="#cb112-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> torch.nn.Linear(d_ff, d_model)</span>
<span id="cb112-10"><a href="#cb112-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb112-11"><a href="#cb112-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb112-12"><a href="#cb112-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_layer(x)</span>
<span id="cb112-13"><a href="#cb112-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(x)</span>
<span id="cb112-14"><a href="#cb112-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb112-15"><a href="#cb112-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb112-16"><a href="#cb112-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_positionwise_feedforward():</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Position-wise Feed-Forward Network"</span></span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>    ffn <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> ffn(x)</span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb113-9"><a href="#cb113-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Ready to move on to the next component?</p>
</section>
<section id="unit-test-2" class="level4">
<h4 class="anchored" data-anchor-id="unit-test-2">Unit Test</h4>
<div id="d162fba8" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_positionwise_feedforward():</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Position-wise Feed-Forward Network"</span></span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>    ffn <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> ffn(x)</span>
<span id="cb114-7"><a href="#cb114-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb114-8"><a href="#cb114-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="1d495e12" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>test_positionwise_feedforward()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Why is it called position-wise feedforward?</p>
<section id="reply-68" class="level5">
<h5 class="anchored" data-anchor-id="reply-68">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question!</p>
<p><strong>“Position-wise”</strong> means the same feed-forward network is applied to <strong>each position independently</strong>.</p>
<p>Think of it this way: - You have a sequence: <code>[word1, word2, word3, ...]</code> - Each word has a vector of size d_model=512 - The FFN processes each word’s vector <strong>separately</strong> - word1’s vector doesn’t interact with word2’s vector</p>
<p>It’s like applying the same 2-layer neural network to each position in the sequence independently. The same weights are shared across all positions, but each position is processed on its own.</p>
<p><strong>Contrast with attention:</strong> Attention mixes information <strong>across</strong> positions (word1 looks at word2, word3, etc.). The FFN does <strong>not</strong> mix across positions - it just transforms each position’s representation individually.</p>
<p>Does this clarify why it’s called “position-wise”?</p>
<p>Okay, that makes it clear. Let’s move on now.</p>
</section>
<section id="reply-69" class="level5">
<h5 class="anchored" data-anchor-id="reply-69">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! You’ve now completed: 1. ✓ Scaled Dot-Product Attention 2. ✓ Multi-Head Attention 3. ✓ Position-wise Feed-Forward Network</p>
<p><strong>What’s next?</strong></p>
<p>The remaining key components are:</p>
<ol type="1">
<li><strong>Embeddings</strong> - convert tokens to vectors</li>
<li><strong>Positional Encoding</strong> - add position information (since attention has no inherent notion of order)</li>
<li><strong>Layer Normalization</strong> - for stable training</li>
<li><strong>Residual Connections</strong> - skip connections around sub-layers</li>
<li><strong>Encoder Layer</strong> - combines attention + FFN with residuals and norms</li>
<li><strong>Decoder Layer</strong> - similar but with masked self-attention + cross-attention</li>
</ol>
<p>I’d suggest <strong>Positional Encoding</strong> next - it’s a unique and interesting part of the Transformer. It explains how the model knows the order of words.</p>
<p>Want to tackle that? Or would you prefer a different component?</p>
</section>
</section>
</section>
</section>
<section id="positional-encoding-1" class="level2">
<h2 class="anchored" data-anchor-id="positional-encoding-1">Positional Encoding</h2>
<p>Okay, I’m down for positional encoding. Let’s do it.</p>
<section id="reply-70" class="level5">
<h5 class="anchored" data-anchor-id="reply-70">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let’s tackle <strong>Positional Encoding</strong>.</p>
<p><strong>Why do we need it?</strong></p>
<p>Attention has no inherent sense of order - it treats the sequence like a set. “I love pizza” and “pizza love I” would look the same! Positional encoding adds information about where each word is in the sequence.</p>
<p><strong>How does it work?</strong></p>
<p>The paper uses sine and cosine functions of different frequencies:</p>
<ul>
<li>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</li>
<li>PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</li>
</ul>
<p>Where: - <code>pos</code> = position in sequence (0, 1, 2, …) - <code>i</code> = dimension index (0, 1, 2, …, d_model/2)</p>
<p><strong>The class structure:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(nn.Module):</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, dropout, max_len<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create the positional encoding matrix</span></span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store it as a buffer (not a parameter)</span></span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb116-8"><a href="#cb116-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add positional encoding to input x</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Before coding, do you understand why they use sin/cos functions? Want me to explain the intuition?</p>
<p>I think they use sine and cos because sine and cos have different peaks. For example, sin is 1 while cos is 0.</p>
</section>
<section id="reply-71" class="level5">
<h5 class="anchored" data-anchor-id="reply-71">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>That’s part of it! But there’s more to why sin/cos are special here.</p>
<p><strong>Key properties they wanted:</strong></p>
<ol type="1">
<li><strong>Unique encoding for each position</strong> - different positions get different patterns</li>
<li><strong>Relative position information</strong> - the model can learn “word A is 3 positions before word B”</li>
<li><strong>Extrapolation</strong> - can handle sequences longer than seen in training</li>
</ol>
<p><strong>Why sin/cos specifically?</strong></p>
<p>For any fixed offset k, PE(pos+k) can be represented as a <strong>linear function</strong> of PE(pos). This is because of the trigonometric identity:</p>
<p>sin(a + b) = sin(a)cos(b) + cos(a)sin(b)</p>
<p>This means the model can easily learn to attend by relative positions!</p>
<p><strong>Different frequencies:</strong> Each dimension uses a different frequency (from 2π to 10000·2π), so different dimensions capture patterns at different scales - some for nearby words, some for distant words.</p>
<p>Does this deeper intuition make sense? Ready to start implementing it?</p>
<p>No, I’m still not getting it. What I’m seeing from the formula is that for even dim we’re using sine, and for odd dim we’re using cosine. Secondly, what I am getting is the frequency changes as the dimension index changes. It goes from lower frequency to higher frequency. Is what I’m saying making sense? Am I getting the correct intuition?</p>
</section>
<section id="reply-72" class="level5">
<h5 class="anchored" data-anchor-id="reply-72">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Yes! You’re getting it exactly right! 🎉</p>
<p><strong>What you understand correctly:</strong></p>
<ol type="1">
<li>✓ <strong>Even dimensions use sine, odd dimensions use cosine</strong> - this gives you pairs of (sin, cos) at the same frequency</li>
<li>✓ <strong>Frequency changes with dimension</strong> - but actually it goes from <strong>higher to lower</strong> frequency as i increases (because we divide by larger numbers: 10000^(2i/d_model) gets bigger)</li>
</ol>
<p><strong>Why does frequency matter?</strong></p>
<ul>
<li><strong>High frequency</strong> (early dimensions): Changes quickly, good for capturing nearby positions (position 1 vs 2 vs 3)</li>
<li><strong>Low frequency</strong> (later dimensions): Changes slowly, good for capturing distant positions (position 10 vs 50 vs 100)</li>
</ul>
<p>So different dimensions encode position information at different scales!</p>
<p><strong>The sin/cos pairing:</strong> Using both at the same frequency helps the model learn relative positions through those trig identities I mentioned.</p>
<p>Does this click now? Ready to implement it?</p>
<p>How are early dimensions high frequency and later low frequency? Give a numerical example.</p>
</section>
<section id="reply-73" class="level5">
<h5 class="anchored" data-anchor-id="reply-73">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! Let me show you with numbers.</p>
<p><strong>The formula for frequency:</strong></p>
<pre><code>div_term = 10000^(2i/d_model)</code></pre>
<p>Let’s say d_model = 512:</p>
<p><strong>Early dimensions (i=0):</strong></p>
<pre><code>10000^(2*0/512) = 10000^0 = 1
frequency = 1/1 = 1  (HIGH frequency - changes fast!)</code></pre>
<p><strong>Middle dimensions (i=64):</strong></p>
<pre><code>10000^(2*64/512) = 10000^0.25 ≈ 10
frequency = 1/10 = 0.1  (medium frequency)</code></pre>
<p><strong>Later dimensions (i=255):</strong></p>
<pre><code>10000^(2*255/512) = 10000^0.996 ≈ 6310
frequency = 1/6310 ≈ 0.00016  (LOW frequency - changes slowly!)</code></pre>
<p><strong>What this means:</strong> - Dimension 0: sin(pos/1) changes every position - Dimension 128: sin(pos/10) changes every ~10 positions<br>
- Dimension 510: sin(pos/6310) barely changes across the sequence</p>
<p>Does this numerical example clarify how frequency decreases as i increases?</p>
<p>Okay, before jumping into the implementation, let’s do some visualisation. Give me some visualisation code without thinking.</p>
</section>
<section id="reply-74" class="level5">
<h5 class="anchored" data-anchor-id="reply-74">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create positional encoding for visualization</span></span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a>d_model <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a>max_len <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb121-6"><a href="#cb121-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-7"><a href="#cb121-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute positional encodings</span></span>
<span id="cb121-8"><a href="#cb121-8" aria-hidden="true" tabindex="-1"></a>pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb121-9"><a href="#cb121-9" aria-hidden="true" tabindex="-1"></a>position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb121-10"><a href="#cb121-10" aria-hidden="true" tabindex="-1"></a>div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>) <span class="op">*</span> <span class="op">-</span>(math.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb121-11"><a href="#cb121-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-12"><a href="#cb121-12" aria-hidden="true" tabindex="-1"></a>pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb121-13"><a href="#cb121-13" aria-hidden="true" tabindex="-1"></a>pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb121-14"><a href="#cb121-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-15"><a href="#cb121-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb121-16"><a href="#cb121-16" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb121-17"><a href="#cb121-17" aria-hidden="true" tabindex="-1"></a>plt.imshow(pe.T, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb121-18"><a href="#cb121-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Position'</span>)</span>
<span id="cb121-19"><a href="#cb121-19" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Dimension'</span>)</span>
<span id="cb121-20"><a href="#cb121-20" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb121-21"><a href="#cb121-21" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Positional Encoding Heatmap'</span>)</span>
<span id="cb121-22"><a href="#cb121-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb121-23"><a href="#cb121-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-24"><a href="#cb121-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot specific dimensions</span></span>
<span id="cb121-25"><a href="#cb121-25" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb121-26"><a href="#cb121-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dim <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">30</span>]:</span>
<span id="cb121-27"><a href="#cb121-27" aria-hidden="true" tabindex="-1"></a>    plt.plot(pe[:, dim], label<span class="op">=</span><span class="ss">f'dim </span><span class="sc">{</span>dim<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb121-28"><a href="#cb121-28" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Position'</span>)</span>
<span id="cb121-29"><a href="#cb121-29" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Encoding Value'</span>)</span>
<span id="cb121-30"><a href="#cb121-30" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb121-31"><a href="#cb121-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Positional Encoding for Different Dimensions'</span>)</span>
<span id="cb121-32"><a href="#cb121-32" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this to see how the encodings look!</p>
<p>Ok, let’s implement now and much clearer.</p>
</section>
<section id="reply-75" class="level5">
<h5 class="anchored" data-anchor-id="reply-75">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let’s implement Positional Encoding.</p>
<p><strong>Step 1: Create the positional encoding matrix in <code>__init__</code></strong></p>
<p>You need to: 1. Create a matrix <code>pe</code> of shape (max_len, d_model) filled with zeros 2. Create a <code>position</code> vector: [0, 1, 2, …, max_len-1] 3. Calculate <code>div_term</code> using the formula 4. Fill in the even dimensions with sine 5. Fill in the odd dimensions with cosine</p>
<p>Try starting with the first 3 steps:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(nn.Module):</span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, dropout, max_len<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb122-5"><a href="#cb122-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb122-6"><a href="#cb122-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create pe matrix</span></span>
<span id="cb122-7"><a href="#cb122-7" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb122-8"><a href="#cb122-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb122-9"><a href="#cb122-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create position vector - try this yourself!</span></span>
<span id="cb122-10"><a href="#cb122-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate div_term - try this yourself!</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Can you create the <code>position</code> vector and calculate <code>div_term</code>? Remember the formulas we discussed!</p>
<div id="cc5fb146" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(torch.nn.Module):</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, dropout, max_len<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb123-5"><a href="#cb123-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb123-6"><a href="#cb123-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create pe matrix</span></span>
<span id="cb123-7"><a href="#cb123-7" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb123-8"><a href="#cb123-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-9"><a href="#cb123-9" aria-hidden="true" tabindex="-1"></a>        half_range <span class="op">=</span> torch.<span class="bu">range</span>(d_model<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb123-10"><a href="#cb123-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb123-11"><a href="#cb123-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create position vector - try this yourself!</span></span>
<span id="cb123-12"><a href="#cb123-12" aria-hidden="true" tabindex="-1"></a>        cosine <span class="op">=</span> torch.cos(pos<span class="op">/</span> (<span class="dv">1000</span><span class="op">**</span>(<span class="dv">2</span><span class="op">*</span>half_range<span class="op">/</span>d_model)) )</span>
<span id="cb123-13"><a href="#cb123-13" aria-hidden="true" tabindex="-1"></a>        sin <span class="op">=</span> torch.sin(pos<span class="op">/</span> (<span class="dv">1000</span><span class="op">**</span>(<span class="dv">2</span><span class="op">*</span>half_range<span class="op">/</span>d_model)) )</span>
<span id="cb123-14"><a href="#cb123-14" aria-hidden="true" tabindex="-1"></a>        pe[::<span class="dv">2</span>] <span class="op">=</span> cosine</span>
<span id="cb123-15"><a href="#cb123-15" aria-hidden="true" tabindex="-1"></a>        pe[<span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> sin</span>
<span id="cb123-16"><a href="#cb123-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate div_term - try this yourself!</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Am I doing well so far?</p>
</section>
<section id="reply-76" class="level5">
<h5 class="anchored" data-anchor-id="reply-76">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You’re on the right track with the concept, but there are a few issues to fix:</p>
<ol type="1">
<li><strong><code>pos</code> is not defined</strong> - you need to create a position vector first: <code>torch.arange(0, max_len)</code></li>
<li><strong><code>half_range</code></strong> - use <code>torch.arange(0, d_model, 2)</code> to get even indices</li>
<li><strong>The indexing <code>pe[::2]</code></strong> - this won’t work correctly. You need to fill columns (dimensions), not rows. Use <code>pe[:, 0::2]</code> for even dims and <code>pe[:, 1::2]</code> for odd dims</li>
<li><strong>Broadcasting</strong> - <code>pos</code> needs to be reshaped to broadcast correctly with <code>div_term</code></li>
</ol>
<p>Try fixing step 1 first - create the position vector:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len).unsqueeze(<span class="dv">1</span>)  <span class="co"># shape: (max_len, 1)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Does this make sense? Want to try the next step?</p>
<p>What’s the difference between A-range and range?</p>
</section>
<section id="reply-77" class="level5">
<h5 class="anchored" data-anchor-id="reply-77">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good question!</p>
<p><strong><code>torch.range()</code> (deprecated):</strong> - Older function, not recommended - Includes the end value - <code>torch.range(0, 5)</code> gives <code>[0, 1, 2, 3, 4, 5]</code> (6 elements!)</p>
<p><strong><code>torch.arange()</code> (use this!):</strong> - Modern version, works like Python’s <code>range()</code> - Excludes the end value - <code>torch.arange(0, 5)</code> gives <code>[0, 1, 2, 3, 4]</code> (5 elements)</p>
<p>So always use <code>torch.arange()</code> - it’s more predictable and matches Python conventions!</p>
<p>Ready to continue with the implementation?</p>
<p>Why are we unsqueezing?</p>
</section>
<section id="reply-78" class="level5">
<h5 class="anchored" data-anchor-id="reply-78">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question!</p>
<p><strong>Why unsqueeze?</strong></p>
<p>We need the shapes to broadcast correctly for the calculation.</p>
<p><strong>The shapes:</strong> - <code>position</code>: We want shape <code>(max_len, 1)</code> - each row is a position - <code>div_term</code>: Will have shape <code>(d_model/2,)</code> - different frequencies</p>
<p><strong>When we multiply them:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>position <span class="op">*</span> div_term</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Broadcasting gives us shape <code>(max_len, d_model/2)</code> - every position combined with every frequency!</p>
<p><strong>Without unsqueeze:</strong> - <code>position</code> would be shape <code>(max_len,)</code> - Multiplication wouldn’t broadcast the way we want</p>
<p><strong>Visual example:</strong></p>
<pre><code>position (100, 1):        div_term (256,):
[[0],                     [1.0, 0.5, 0.25, ...]
 [1],
 [2],                     Result (100, 256):
 ...]                     Each pos × each frequency</code></pre>
<p>Does this clarify why we need that extra dimension?</p>
<div id="af36cb48" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(torch.nn.Module):</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, dropout, max_len<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb127-6"><a href="#cb127-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create pe matrix</span></span>
<span id="cb127-7"><a href="#cb127-7" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb127-8"><a href="#cb127-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-9"><a href="#cb127-9" aria-hidden="true" tabindex="-1"></a>        half_range <span class="op">=</span> torch.<span class="bu">range</span>(d_model<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb127-10"><a href="#cb127-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb127-11"><a href="#cb127-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create position vector - try this yourself!</span></span>
<span id="cb127-12"><a href="#cb127-12" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb127-13"><a href="#cb127-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate div_term - try this yourself!</span></span>
<span id="cb127-14"><a href="#cb127-14" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> <span class="dv">10000</span><span class="op">**</span>(half_range<span class="op">/</span>d_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Are we doing better?</p>
</section>
<section id="reply-79" class="level5">
<h5 class="anchored" data-anchor-id="reply-79">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Getting closer! A few more fixes needed:</p>
<ol type="1">
<li><p><strong><code>half_range</code></strong>: Should be <code>torch.arange(0, d_model, 2)</code> not <code>torch.range(d_model//2)</code>. You want the even indices: [0, 2, 4, 6, …]</p></li>
<li><p><strong><code>div_term</code> formula</strong>: You have it backwards! It should be dividing, not multiplying. Remember: we want 10000^(2i/d_model), which means:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>) <span class="op">*</span> <span class="op">-</span>(math.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This computes the exponential in a numerically stable way.</p></li>
</ol>
<p>Try fixing these two things. Does the <code>div_term</code> formula make sense, or do you want me to explain why we use <code>exp</code> and <code>log</code>?</p>
<div id="5621088a" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(torch.nn.Module):</span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, dropout, max_len<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create pe matrix</span></span>
<span id="cb129-7"><a href="#cb129-7" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb129-8"><a href="#cb129-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-9"><a href="#cb129-9" aria-hidden="true" tabindex="-1"></a>        even_indices <span class="op">=</span> torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>)</span>
<span id="cb129-10"><a href="#cb129-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb129-11"><a href="#cb129-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create position vector - try this yourself!</span></span>
<span id="cb129-12"><a href="#cb129-12" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb129-13"><a href="#cb129-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate div_term - try this yourself!</span></span>
<span id="cb129-14"><a href="#cb129-14" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(<span class="op">-</span>even_indices<span class="op">*</span>(torch.log(<span class="dv">10000</span>))<span class="op">/</span>d_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>All good now?</p>
</section>
<section id="reply-80" class="level5">
<h5 class="anchored" data-anchor-id="reply-80">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your position vector and div_term are now correct! ✓</p>
<p>Now you need to fill in the <code>pe</code> matrix with sine and cosine values:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This fills: - Even columns (0, 2, 4, …) with sine - Odd columns (1, 3, 5, …) with cosine</p>
<p>Then you need to: 1. Add a batch dimension: <code>pe = pe.unsqueeze(0)</code> 2. Register it as a buffer (not a trainable parameter): <code>self.register_buffer('pe', pe)</code></p>
<p>Can you add these steps to complete the <code>__init__</code> method?</p>
<div id="f0c8a29c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(torch.nn.Module):</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, dropout, max_len<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create pe matrix</span></span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb131-8"><a href="#cb131-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-9"><a href="#cb131-9" aria-hidden="true" tabindex="-1"></a>        even_indices <span class="op">=</span> torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>)</span>
<span id="cb131-10"><a href="#cb131-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb131-11"><a href="#cb131-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create position vector - try this yourself!</span></span>
<span id="cb131-12"><a href="#cb131-12" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb131-13"><a href="#cb131-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate div_term - try this yourself!</span></span>
<span id="cb131-14"><a href="#cb131-14" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(<span class="op">-</span>even_indices<span class="op">*</span>(torch.log(<span class="dv">10000</span>))<span class="op">/</span>d_model)</span>
<span id="cb131-15"><a href="#cb131-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-16"><a href="#cb131-16" aria-hidden="true" tabindex="-1"></a>        pe[:, ::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb131-17"><a href="#cb131-17" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb131-18"><a href="#cb131-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pe</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Awesome, that’s great.</p>
</section>
<section id="reply-81" class="level5">
<h5 class="anchored" data-anchor-id="reply-81">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Almost there! But you can’t <code>return</code> from <code>__init__</code> - it’s a constructor method.</p>
<p>Instead, you need to: 1. Add a batch dimension to <code>pe</code> 2. Register it as a buffer so it’s saved with the model but not trained</p>
<p>Add these two lines at the end of <code>__init__</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>pe <span class="op">=</span> pe.unsqueeze(<span class="dv">0</span>)  <span class="co"># Add batch dimension: (1, max_len, d_model)</span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe)  <span class="co"># Save as buffer, not parameter</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then you’ll need to implement the <code>forward</code> method. Can you add these lines first?</p>
<p>Let me guess; we are unsqueezing to accommodate batch size, aren’t we?</p>
</section>
<section id="reply-82" class="level5">
<h5 class="anchored" data-anchor-id="reply-82">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly right! 🎉</p>
<p>The <code>pe</code> matrix is currently shape <code>(max_len, d_model)</code>, but your input will have shape <code>(batch_size, seq_len, d_model)</code>.</p>
<p>By unsqueezing to <code>(1, max_len, d_model)</code>, the batch dimension of size 1 will <strong>broadcast</strong> across whatever batch_size you have in your input.</p>
<p>You’re really getting the hang of these dimension manipulations!</p>
<p>Now add those two lines to your code, then we can implement the <code>forward</code> method. Ready?</p>
<div id="ae9979c4" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(torch.nn.Module):</span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, dropout, max_len<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create pe matrix</span></span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true" tabindex="-1"></a>        even_indices <span class="op">=</span> torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>)</span>
<span id="cb133-10"><a href="#cb133-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb133-11"><a href="#cb133-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create position vector - try this yourself!</span></span>
<span id="cb133-12"><a href="#cb133-12" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb133-13"><a href="#cb133-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate div_term - try this yourself!</span></span>
<span id="cb133-14"><a href="#cb133-14" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(<span class="op">-</span>even_indices<span class="op">*</span>(torch.log(<span class="dv">10000</span>))<span class="op">/</span>d_model)</span>
<span id="cb133-15"><a href="#cb133-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-16"><a href="#cb133-16" aria-hidden="true" tabindex="-1"></a>        pe[:, ::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb133-17"><a href="#cb133-17" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb133-18"><a href="#cb133-18" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> pe.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb133-19"><a href="#cb133-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Also, we are saving PE as a buffer, as that does not change at all.</p>
</section>
<section id="reply-83" class="level5">
<h5 class="anchored" data-anchor-id="reply-83">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly! 💯</p>
<p><strong>Why use <code>register_buffer</code> instead of a parameter?</strong></p>
<ul>
<li><strong>Parameters</strong> (created with <code>nn.Parameter</code>): Are trained/updated during backpropagation</li>
<li><strong>Buffers</strong> (created with <code>register_buffer</code>): Are part of the model state but NOT trained</li>
</ul>
<p>Since positional encodings are <strong>fixed</strong> (computed once using sin/cos, never updated), they should be a buffer, not a parameter!</p>
<p>This also ensures <code>pe</code> gets moved to GPU when you call <code>model.to('cuda')</code> and gets saved/loaded with the model.</p>
<p>Now let’s implement the <code>forward</code> method. What should it do? 1. Take input <code>x</code> of shape <code>(batch, seq_len, d_model)</code> 2. Add the positional encoding for the first <code>seq_len</code> positions 3. Apply dropout 4. Return the result</p>
<p>Can you try writing the <code>forward</code> method?</p>
</section>
<section id="final-positionalencoding" class="level3">
<h3 class="anchored" data-anchor-id="final-positionalencoding">Final PositionalEncoding</h3>
<hr>
</section>
<section id="positionalencoding" class="level3">
<h3 class="anchored" data-anchor-id="positionalencoding">PositionalEncoding</h3>
<blockquote class="blockquote">
<pre><code> PositionalEncoding (d_model, dropout, max_len=5000)</code></pre>
</blockquote>
<p>*Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:<code>to</code>, etc.</p>
<p>.. note:: As per the example above, an <code>__init__()</code> call to the parent class must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*</p>
<p>Is this good?</p>
<section id="reply-84" class="level5">
<h5 class="anchored" data-anchor-id="reply-84">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your Positional Encoding implementation is complete and correct! ✓</p>
<p>You’ve got: - ✓ Extract seq_len from input - ✓ Slice the positional encoding to match: <code>self.pe[:, :seq_len, :]</code> - ✓ Set <code>requires_grad_(False)</code> to ensure it’s not trained - ✓ Add it to the input - ✓ Apply dropout</p>
<p>Let’s test it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>pe <span class="op">=</span> PositionalEncoding(d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>, max_len<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> pe(x)</span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this to verify it works!</p>
<div id="0cf52787" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb137"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>pe <span class="op">=</span> PositionalEncoding(d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>, max_len<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb137-3"><a href="#cb137-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-4"><a href="#cb137-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> pe(x)</span>
<span id="cb137-5"><a href="#cb137-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb137-6"><a href="#cb137-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Assume we are using NB Dev. Convert this into a unit test.</p>
</section>
<section id="reply-85" class="level5">
<h5 class="anchored" data-anchor-id="reply-85">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Here’s your Positional Encoding as a unit test for nbdev:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| export</span></span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(torch.nn.Module):</span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Positional Encoding using sine and cosine functions"</span></span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, dropout, max_len<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb138-6"><a href="#cb138-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> torch.nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb138-7"><a href="#cb138-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb138-8"><a href="#cb138-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create pe matrix</span></span>
<span id="cb138-9"><a href="#cb138-9" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb138-10"><a href="#cb138-10" aria-hidden="true" tabindex="-1"></a>        even_indices <span class="op">=</span> torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>)</span>
<span id="cb138-11"><a href="#cb138-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb138-12"><a href="#cb138-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create position vector</span></span>
<span id="cb138-13"><a href="#cb138-13" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb138-14"><a href="#cb138-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate div_term</span></span>
<span id="cb138-15"><a href="#cb138-15" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(<span class="op">-</span>even_indices<span class="op">*</span>(torch.log(torch.tensor(<span class="fl">10000.0</span>))<span class="op">/</span>d_model))</span>
<span id="cb138-16"><a href="#cb138-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb138-17"><a href="#cb138-17" aria-hidden="true" tabindex="-1"></a>        pe[:, ::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb138-18"><a href="#cb138-18" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb138-19"><a href="#cb138-19" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> pe.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb138-20"><a href="#cb138-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe)</span>
<span id="cb138-21"><a href="#cb138-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb138-22"><a href="#cb138-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb138-23"><a href="#cb138-23" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> x.size(<span class="dv">1</span>)</span>
<span id="cb138-24"><a href="#cb138-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pe[:, :seq_len, :].requires_grad_(<span class="va">False</span>)</span>
<span id="cb138-25"><a href="#cb138-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_positional_encoding():</span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Positional Encoding"</span></span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> PositionalEncoding(d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>, max_len<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb139-6"><a href="#cb139-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb139-7"><a href="#cb139-7" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> pe(x)</span>
<span id="cb139-8"><a href="#cb139-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb139-9"><a href="#cb139-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Great progress! Ready to move on to the next component?</p>
</section>
<section id="unit-test-3" class="level4">
<h4 class="anchored" data-anchor-id="unit-test-3">Unit Test</h4>
<div id="fc166074" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_positional_encoding():</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Positional Encoding"</span></span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> PositionalEncoding(d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>, max_len<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># batch=2, seq_len=10, d_model=512</span></span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb140-6"><a href="#cb140-6" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> pe(x)</span>
<span id="cb140-7"><a href="#cb140-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb140-8"><a href="#cb140-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c3f25aa0" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb141"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>test_positional_encoding()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>That’s amazing, let’s go on to the next block.</p>
<section id="reply-86" class="level5">
<h5 class="anchored" data-anchor-id="reply-86">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent work! You’ve now completed the core building blocks: 1. ✓ Scaled Dot-Product Attention 2. ✓ Multi-Head Attention 3. ✓ Position-wise Feed-Forward Network 4. ✓ Positional Encoding</p>
<p><strong>What’s next?</strong></p>
<p>Now we need to build the <strong>Encoder Layer</strong> - this combines the components you’ve built into a complete encoder block.</p>
<p>An Encoder Layer contains: 1. Multi-Head Self-Attention 2. Add &amp; Norm (residual connection + layer normalization) 3. Feed-Forward Network 4. Add &amp; Norm again</p>
<p>Before we code, do you understand what <strong>residual connections</strong> and <strong>layer normalization</strong> are? Should I explain those concepts first, or do you want to dive into the implementation?</p>
</section>
</section>
</section>
</section>
<section id="encoder-layer" class="level2">
<h2 class="anchored" data-anchor-id="encoder-layer">Encoder Layer</h2>
<p>Yeah, you can test me on these concepts. I somewhat know them.</p>
<section id="reply-87" class="level5">
<h5 class="anchored" data-anchor-id="reply-87">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let’s test your understanding:</p>
<p><strong>Question 1: Residual Connections</strong></p>
<p>What is a residual connection, and why do we use them in deep networks? Can you write the simple equation for a residual connection?</p>
<p><strong>Question 2: Layer Normalization</strong></p>
<p>What does layer normalization do? How is it different from batch normalization?</p>
<p>Give it a shot - explain in your own words!</p>
<p>q1 Basically a residual connection is you after you do an operation on an input, you also bring the input forward bypassing the operation. so, output = x + sublayer(x)</p>
</section>
<section id="reply-88" class="level5">
<h5 class="anchored" data-anchor-id="reply-88">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent! That’s exactly right! ✓</p>
<p><strong>Residual connection:</strong> <code>output = x + sublayer(x)</code></p>
<p>You’ve got the key insight - the input “skips” around the operation and gets added back. This helps with: - Gradient flow during backpropagation (prevents vanishing gradients) - Allows the network to learn identity mappings easily - Makes training very deep networks possible</p>
<p><strong>Now for Question 2:</strong></p>
<p>What does layer normalization do, and how does it work? (Hint: think about computing mean and standard deviation)</p>
<p>So basically, across all the dimensions, we take the mean and standard deviation, and we normalise the outputs.</p>
</section>
<section id="reply-89" class="level5">
<h5 class="anchored" data-anchor-id="reply-89">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly right! ✓</p>
<p><strong>Layer Normalization:</strong> - Computes mean and std <strong>across all dimensions (features)</strong> for each example - Normalizes: <code>(x - mean) / std</code> - Then applies learned scale (γ) and shift (β) parameters</p>
<p><strong>Key difference from Batch Normalization:</strong> - <strong>Batch Norm</strong>: normalizes across the batch dimension (all examples) - <strong>Layer Norm</strong>: normalizes across the feature dimension (for each example independently)</p>
<p>Layer Norm works better for sequences because sequence lengths can vary, making batch statistics unreliable.</p>
<p>Now you understand both concepts! Ready to implement the <strong>Encoder Layer</strong> that uses these? It will combine: - Multi-Head Attention - Residual + Layer Norm - Feed-Forward - Residual + Layer Norm again</p>
<p>Want to start coding?</p>
<p>lets code!!!</p>
</section>
<section id="reply-90" class="level5">
<h5 class="anchored" data-anchor-id="reply-90">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Awesome! Let’s build the <strong>Encoder Layer</strong>!</p>
<p>First, let’s think about what we need:</p>
<ol type="1">
<li><strong>In <code>__init__</code></strong>: Store the components
<ul>
<li>Multi-Head Attention module</li>
<li>Feed-Forward module</li>
<li>Layer Norm modules (how many do we need?)</li>
<li>Store the size (d_model)</li>
</ul></li>
<li><strong>In <code>forward</code></strong>: Apply the operations with residual connections</li>
</ol>
<p>Before coding, a question: How many Layer Norm modules do we need? (Hint: think about how many residual connections we have)</p>
<p>Once you answer, try writing the <code>__init__</code> method skeleton:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb142"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderLayer(nn.Module):</span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, self_attn, feed_forward, dropout):</span>
<span id="cb142-3"><a href="#cb142-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb142-4"><a href="#cb142-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do we need to store?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Give it a try!</p>
<p>For layer normalisation, you will need one layer norm after the position-wise feed forward and another layer norm after the multi-head attention.</p>
</section>
<section id="reply-91" class="level5">
<h5 class="anchored" data-anchor-id="reply-91">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly right! You need <strong>2 Layer Norm modules</strong> - one after each sub-layer (attention and feed-forward).</p>
<p>Now try implementing the <code>__init__</code> method. You need to store: - <code>self.size</code> (d_model) - <code>self.self_attn</code> (the multi-head attention module passed in) - <code>self.feed_forward</code> (the FFN module passed in) - 2 Layer Norm modules (use <code>nn.LayerNorm(size)</code>) - Dropout</p>
<p>Give it a try! What does your <code>__init__</code> look like?</p>
<div id="76d7a192" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb143"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderLayer(torch.nn.Module):</span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, self_attn, feed_forward, dropout):</span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do we need to store?</span></span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.size <span class="op">=</span> size</span>
<span id="cb143-6"><a href="#cb143-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> self_attn</span>
<span id="cb143-7"><a href="#cb143-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> feed_forward</span>
<span id="cb143-8"><a href="#cb143-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norms <span class="op">=</span> torch.nn.ModuleList([torch.nn.LayerNorm(size) <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)])</span>
<span id="cb143-9"><a href="#cb143-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> dropout</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this a cognizant with the original code?</p>
</section>
<section id="reply-92" class="level5">
<h5 class="anchored" data-anchor-id="reply-92">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Almost! Looking at the original code pattern, there’s one difference:</p>
<p>Instead of storing <code>dropout</code> as a value, you should create a <code>Dropout</code> layer:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>But actually, looking at the original code more carefully, they use a <strong><code>SublayerConnection</code></strong> module that handles both the residual connection AND layer norm together. This is a cleaner pattern.</p>
<p>Do you want to: 1. <strong>Option A</strong>: Implement <code>SublayerConnection</code> class first (handles residual + norm), then use it in EncoderLayer 2. <strong>Option B</strong>: Continue with your current approach (manual residual connections in forward)</p>
<p>The original code uses Option A. Which would you prefer?</p>
</section>
<section id="sublayer-connection" class="level3">
<h3 class="anchored" data-anchor-id="sublayer-connection">Sublayer Connection</h3>
<p>Where is this mentioned in the original code? Tell me.</p>
<section id="reply-93" class="level5">
<h5 class="anchored" data-anchor-id="reply-93">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Looking at the original code you provided at the beginning (the Annotated Transformer), you can find the <code>SublayerConnection</code> class. Let me point you to it:</p>
<p>It’s defined in the section about <strong>Encoder and Decoder Stacks</strong>. Here’s where it appears:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb145"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SublayerConnection(nn.Module):</span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A residual connection followed by a layer norm.</span></span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Note for code simplicity the norm is first as opposed to last.</span></span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb145-6"><a href="#cb145-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-7"><a href="#cb145-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, dropout):</span>
<span id="cb145-8"><a href="#cb145-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SublayerConnection, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb145-9"><a href="#cb145-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(size)</span>
<span id="cb145-10"><a href="#cb145-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb145-11"><a href="#cb145-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-12"><a href="#cb145-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, sublayer):</span>
<span id="cb145-13"><a href="#cb145-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Apply residual connection to any sublayer with the same size."</span></span>
<span id="cb145-14"><a href="#cb145-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> <span class="va">self</span>.dropout(sublayer(<span class="va">self</span>.norm(x)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>And then <code>EncoderLayer</code> uses it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb146"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.sublayer <span class="op">=</span> clones(SublayerConnection(size, dropout), <span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>So they create 2 <code>SublayerConnection</code> modules. Do you want to implement <code>SublayerConnection</code> first, or stick with your current approach?</p>
<p>What should we do, man?</p>
</section>
<section id="reply-94" class="level5">
<h5 class="anchored" data-anchor-id="reply-94">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good question! Here’s my recommendation:</p>
<p><strong>Follow the original code structure</strong> - implement <code>SublayerConnection</code> first. Here’s why:</p>
<ol type="1">
<li>It’s cleaner and more modular</li>
<li>You’ll reuse it in the Decoder Layer too</li>
<li>It matches the paper’s approach (norm → sublayer → dropout → residual)</li>
</ol>
<p>The <code>SublayerConnection</code> is simple - it just wraps any sublayer with: - Layer normalization (applied first) - The sublayer operation - Dropout - Residual connection</p>
<p>Want to implement <code>SublayerConnection</code> first? It’s a small class - should only take a few minutes!</p>
<p>ok</p>
</section>
<section id="reply-95" class="level5">
<h5 class="anchored" data-anchor-id="reply-95">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let’s implement <code>SublayerConnection</code>.</p>
<p>Looking at the original code, it needs:</p>
<p><strong>In <code>__init__</code>:</strong> - Store a LayerNorm module - Store a Dropout module</p>
<p><strong>In <code>forward(self, x, sublayer)</code>:</strong> - Apply layer norm to x - Pass normalized x through the sublayer function - Apply dropout - Add the residual connection (original x)</p>
<p>The pattern is: <code>x + dropout(sublayer(norm(x)))</code></p>
<p>Try implementing it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb147"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SublayerConnection(nn.Module):</span>
<span id="cb147-2"><a href="#cb147-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"A residual connection followed by a layer norm"</span></span>
<span id="cb147-3"><a href="#cb147-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, dropout):</span>
<span id="cb147-4"><a href="#cb147-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb147-5"><a href="#cb147-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you need here?</span></span>
<span id="cb147-6"><a href="#cb147-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb147-7"><a href="#cb147-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, sublayer):</span>
<span id="cb147-8"><a href="#cb147-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What's the operation?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Give it a shot!</p>
</section>
</section>
<section id="final-sublayerconnection-function" class="level3">
<h3 class="anchored" data-anchor-id="final-sublayerconnection-function">Final SublayerConnection Function</h3>
<hr>
</section>
<section id="sublayerconnection" class="level3">
<h3 class="anchored" data-anchor-id="sublayerconnection">SublayerConnection</h3>
<blockquote class="blockquote">
<pre><code> SublayerConnection (size, dropout)</code></pre>
</blockquote>
<p><em>A residual connection followed by a layer norm</em></p>
<p>is this good? always refer to original code</p>
<section id="reply-96" class="level5">
<h5 class="anchored" data-anchor-id="reply-96">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>SublayerConnection</code> is correct and matches the original code! ✓</p>
<p>You’ve got: - ✓ Layer norm applied first - ✓ Sublayer operation - ✓ Dropout - ✓ Residual connection (x + y)</p>
<p>Now let’s go back to the <code>EncoderLayer</code> and use <code>SublayerConnection</code>.</p>
<p>Update your <code>EncoderLayer.__init__</code> to use:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb149"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.sublayer <span class="op">=</span> nn.ModuleList([SublayerConnection(size, dropout) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Instead of the separate layer_norms. Can you update your <code>EncoderLayer</code> code?</p>
<div id="b11b35b0" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb150"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.sublayer <span class="op">=</span> nn.ModuleList([SublayerConnection(size, dropout) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>like this?</p>
</section>
<section id="reply-97" class="level5">
<h5 class="anchored" data-anchor-id="reply-97">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>EncoderLayer.__init__</code> is now correct! ✓</p>
<p>Now let’s implement the <code>forward</code> method. Looking at the original code, you need to:</p>
<ol type="1">
<li>Apply first sublayer connection with self-attention</li>
<li>Apply second sublayer connection with feed-forward</li>
</ol>
<p>The tricky part: you need to pass a <strong>lambda function</strong> to sublayer because self-attention takes multiple arguments (query, key, value, mask).</p>
<p>Try writing the <code>forward</code> method. Here’s the signature:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb151"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x, mask):</span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Apply self-attention with first sublayer connection</span></span>
<span id="cb151-3"><a href="#cb151-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Apply feed-forward with second sublayer connection</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Hint: Look at how the original code uses <code>lambda</code> to wrap the self_attn call.</p>
<p>Give it a try!</p>
<p>I don’t want to look at the original code, give me a clue.</p>
</section>
<section id="reply-98" class="level5">
<h5 class="anchored" data-anchor-id="reply-98">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good! Let me give you clues without showing the code.</p>
<p><strong>Clue 1:</strong> The <code>sublayer</code> in <code>SublayerConnection.forward(x, sublayer)</code> expects a <strong>function</strong> that takes one argument (x) and returns output.</p>
<p><strong>Clue 2:</strong> But <code>self.self_attn</code> needs 4 arguments: query, key, value, mask. For self-attention, query=key=value=x.</p>
<p><strong>Clue 3:</strong> You need to wrap <code>self.self_attn</code> in a function that takes only x as input. Use a <code>lambda</code> to do this.</p>
<p><strong>Clue 4:</strong> For the feed-forward, it’s simpler - it already takes just x as input.</p>
<p>Try writing:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb152"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x, mask):</span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.sublayer[<span class="dv">0</span>](x, <span class="kw">lambda</span> x: ???)  <span class="co"># What goes here?</span></span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.sublayer[<span class="dv">1</span>](x, ???)  <span class="co"># What goes here?</span></span>
<span id="cb152-4"><a href="#cb152-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What would you put in the <code>???</code> spots?</p>
</section>
</section>
<section id="final-encoderlayer-function" class="level3">
<h3 class="anchored" data-anchor-id="final-encoderlayer-function">Final EncoderLayer Function</h3>
<hr>
</section>
<section id="encoderlayer" class="level3">
<h3 class="anchored" data-anchor-id="encoderlayer">EncoderLayer</h3>
<blockquote class="blockquote">
<pre><code> EncoderLayer (size, self_attn, feed_forward, dropout)</code></pre>
</blockquote>
<p>*Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:<code>to</code>, etc.</p>
<p>.. note:: As per the example above, an <code>__init__()</code> call to the parent class must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*</p>
<p>Is this correct?</p>
<section id="reply-99" class="level5">
<h5 class="anchored" data-anchor-id="reply-99">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent work! Your <code>EncoderLayer</code> is now correct! ✓</p>
<p>You’ve got: - ✓ First sublayer: self-attention with <code>lambda x: self.self_attn(x, x, x, mask)</code> (query=key=value=x) - ✓ Second sublayer: feed-forward with <code>lambda x: self.feed_forward(x)</code> - ✓ Both wrapped in <code>SublayerConnection</code> for residual + norm</p>
<p>Let’s test it to make sure everything works:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb155"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the components</span></span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a>attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb155-3"><a href="#cb155-3" aria-hidden="true" tabindex="-1"></a>ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb155-4"><a href="#cb155-4" aria-hidden="true" tabindex="-1"></a>encoder_layer <span class="op">=</span> EncoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>attn, feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb155-5"><a href="#cb155-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-6"><a href="#cb155-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb155-7"><a href="#cb155-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)</span>
<span id="cb155-8"><a href="#cb155-8" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb155-9"><a href="#cb155-9" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> encoder_layer(x, mask)</span>
<span id="cb155-10"><a href="#cb155-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb155-11"><a href="#cb155-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this and see if it works!</p>
<div id="474420d1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb156"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the components</span></span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a>attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb156-3"><a href="#cb156-3" aria-hidden="true" tabindex="-1"></a>ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb156-4"><a href="#cb156-4" aria-hidden="true" tabindex="-1"></a>encoder_layer <span class="op">=</span> EncoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>attn, feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb156-5"><a href="#cb156-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb156-6"><a href="#cb156-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb156-7"><a href="#cb156-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)</span>
<span id="cb156-8"><a href="#cb156-8" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb156-9"><a href="#cb156-9" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> encoder_layer(x, mask)</span>
<span id="cb156-10"><a href="#cb156-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb156-11"><a href="#cb156-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Very cool, now assume that you are an NB dev and create a unit test out of this.</p>
</section>
<section id="reply-100" class="level5">
<h5 class="anchored" data-anchor-id="reply-100">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Here’s your Encoder Layer as a unit test for nbdev:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb157"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| export</span></span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SublayerConnection(torch.nn.Module):</span>
<span id="cb157-3"><a href="#cb157-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"A residual connection followed by a layer norm"</span></span>
<span id="cb157-4"><a href="#cb157-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, dropout):</span>
<span id="cb157-5"><a href="#cb157-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb157-6"><a href="#cb157-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm <span class="op">=</span> torch.nn.LayerNorm(size)</span>
<span id="cb157-7"><a href="#cb157-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> torch.nn.Dropout(dropout)</span>
<span id="cb157-8"><a href="#cb157-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb157-9"><a href="#cb157-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, sublayer):</span>
<span id="cb157-10"><a href="#cb157-10" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.layer_norm(x)</span>
<span id="cb157-11"><a href="#cb157-11" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> sublayer(y)</span>
<span id="cb157-12"><a href="#cb157-12" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.dropout(y)</span>
<span id="cb157-13"><a href="#cb157-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> y</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb158"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| export</span></span>
<span id="cb158-2"><a href="#cb158-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderLayer(torch.nn.Module):</span>
<span id="cb158-3"><a href="#cb158-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Encoder layer with self-attention and feed-forward"</span></span>
<span id="cb158-4"><a href="#cb158-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, self_attn, feed_forward, dropout):</span>
<span id="cb158-5"><a href="#cb158-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb158-6"><a href="#cb158-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.size <span class="op">=</span> size</span>
<span id="cb158-7"><a href="#cb158-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> self_attn</span>
<span id="cb158-8"><a href="#cb158-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> feed_forward</span>
<span id="cb158-9"><a href="#cb158-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sublayer <span class="op">=</span> torch.nn.ModuleList([SublayerConnection(size, dropout) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)])</span>
<span id="cb158-10"><a href="#cb158-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb158-11"><a href="#cb158-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask):</span>
<span id="cb158-12"><a href="#cb158-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayer[<span class="dv">0</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.self_attn(x, x, x, mask))</span>
<span id="cb158-13"><a href="#cb158-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayer[<span class="dv">1</span>](x, <span class="va">self</span>.feed_forward)</span>
<span id="cb158-14"><a href="#cb158-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb159"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb159-2"><a href="#cb159-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_encoder_layer():</span>
<span id="cb159-3"><a href="#cb159-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Encoder Layer"</span></span>
<span id="cb159-4"><a href="#cb159-4" aria-hidden="true" tabindex="-1"></a>    attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb159-5"><a href="#cb159-5" aria-hidden="true" tabindex="-1"></a>    ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb159-6"><a href="#cb159-6" aria-hidden="true" tabindex="-1"></a>    encoder_layer <span class="op">=</span> EncoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>attn, feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb159-7"><a href="#cb159-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb159-8"><a href="#cb159-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)</span>
<span id="cb159-9"><a href="#cb159-9" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> encoder_layer(x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb159-10"><a href="#cb159-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb159-11"><a href="#cb159-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Great work! Ready for the next component?</p>
</section>
<section id="unit-test-4" class="level4">
<h4 class="anchored" data-anchor-id="unit-test-4">Unit Test</h4>
<div id="0e1b0571" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb160"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_encoder_layer():</span>
<span id="cb160-2"><a href="#cb160-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Encoder Layer"</span></span>
<span id="cb160-3"><a href="#cb160-3" aria-hidden="true" tabindex="-1"></a>    attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb160-4"><a href="#cb160-4" aria-hidden="true" tabindex="-1"></a>    ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb160-5"><a href="#cb160-5" aria-hidden="true" tabindex="-1"></a>    encoder_layer <span class="op">=</span> EncoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>attn, feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb160-6"><a href="#cb160-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb160-7"><a href="#cb160-7" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)</span>
<span id="cb160-8"><a href="#cb160-8" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> encoder_layer(x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb160-9"><a href="#cb160-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb160-10"><a href="#cb160-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="32068f48" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb161"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a>test_encoder_layer()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>That’s great, let’s move on. Also be honest to the code I provided in the beginning.</p>
<section id="reply-101" class="level5">
<h5 class="anchored" data-anchor-id="reply-101">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Awesome! You’re making excellent progress! 🎉</p>
<p>You’ve now built: 1. ✓ Attention mechanism 2. ✓ Multi-Head Attention 3. ✓ Feed-Forward Network 4. ✓ Positional Encoding 5. ✓ Sublayer Connection (residual + norm) 6. ✓ Encoder Layer</p>
<p><strong>What’s next?</strong></p>
<p>Looking at the original code, the next component is the <strong>Encoder</strong> - which stacks N=6 identical <code>EncoderLayer</code>s on top of each other.</p>
<p>The Encoder class needs to: - Take a single <code>EncoderLayer</code> and clone it N times - Apply them sequentially - Apply a final layer normalization</p>
<p>Before we code, do you understand why we need to <strong>clone</strong> the layer instead of just reusing the same one N times?</p>
<p>We are using clone because we want a deep copy, and we don’t want to use the same address.</p>
</section>
<section id="reply-102" class="level5">
<h5 class="anchored" data-anchor-id="reply-102">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly right! 🎉</p>
<p>If you just reused the same layer N times, they’d all share the <strong>same parameters</strong> (same memory address). So when you update weights during training, all “layers” would be identical - defeating the purpose of having multiple layers!</p>
<p>By cloning (deep copy), each layer gets its <strong>own independent parameters</strong> that can learn different things.</p>
<p>Now let’s implement the <strong>Encoder</strong>. Looking at the original code, you’ll need:</p>
<p><strong>In <code>__init__</code>:</strong> - Clone the layer N times (there’s a helper function <code>clones</code> in the original code) - Create a final LayerNorm</p>
<p><strong>In <code>forward</code>:</strong> - Pass input through each layer sequentially - Apply final norm - Return output</p>
<p>First, let’s implement the <code>clones</code> helper function:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb162"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clones(module, N):</span>
<span id="cb162-2"><a href="#cb162-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Produce N identical layers"</span></span>
<span id="cb162-3"><a href="#cb162-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># How would you create N deep copies?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Can you implement this helper function?</p>
</section>
</section>
</section>
<section id="final-clones-function" class="level3">
<h3 class="anchored" data-anchor-id="final-clones-function">Final clones function</h3>
<hr>
</section>
<section id="clones" class="level3">
<h3 class="anchored" data-anchor-id="clones">clones</h3>
<blockquote class="blockquote">
<pre><code> clones (module, N)</code></pre>
</blockquote>
<p><em>Produce N identical layers</em></p>
</section>
</section>
<section id="encoder-class" class="level2">
<h2 class="anchored" data-anchor-id="encoder-class">Encoder Class</h2>
<p>Is this correct?</p>
<section id="reply-103" class="level5">
<h5 class="anchored" data-anchor-id="reply-103">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>clones</code> function is exactly right! ✓</p>
<p>You’re using: - <code>copy.deepcopy(module)</code> to create independent copies - <code>torch.nn.ModuleList</code> to properly register them with PyTorch - List comprehension to create N copies</p>
<p>Now let’s implement the <strong>Encoder</strong> class. Try writing it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb164"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb164-2"><a href="#cb164-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Stack of N encoder layers"</span></span>
<span id="cb164-3"><a href="#cb164-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, layer, N):</span>
<span id="cb164-4"><a href="#cb164-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb164-5"><a href="#cb164-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you need here?</span></span>
<span id="cb164-6"><a href="#cb164-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb164-7"><a href="#cb164-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask):</span>
<span id="cb164-8"><a href="#cb164-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What's the sequence of operations?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What do you need in <code>__init__</code>? (Hint: use <code>clones</code> and create a LayerNorm)</p>
<div id="a398cd55" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb165"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(torch.nn.Module):</span>
<span id="cb165-2"><a href="#cb165-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Stack of N encoder layers"</span></span>
<span id="cb165-3"><a href="#cb165-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, layer, N):</span>
<span id="cb165-4"><a href="#cb165-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb165-5"><a href="#cb165-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you need here?</span></span>
<span id="cb165-6"><a href="#cb165-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoders <span class="op">=</span> clones(layer, N)</span>
<span id="cb165-7"><a href="#cb165-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb165-8"><a href="#cb165-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask):</span>
<span id="cb165-9"><a href="#cb165-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What's the sequence of operations?</span></span>
<span id="cb165-10"><a href="#cb165-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> encoder <span class="kw">in</span> encoders:</span>
<span id="cb165-11"><a href="#cb165-11" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> encoder(x, mask)</span>
<span id="cb165-12"><a href="#cb165-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this correct?</p>
</section>
<section id="reply-104" class="level5">
<h5 class="anchored" data-anchor-id="reply-104">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Almost! You’re missing two things:</p>
<ol type="1">
<li><p><strong>Missing final LayerNorm</strong>: The original code applies a final layer normalization after all the encoder layers. Add:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb166"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(layer.size)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Bug in forward</strong>: You wrote <code>encoders</code> but it should be <code>self.encoders</code></p></li>
</ol>
<p>Can you fix these two issues?</p>
<p>Why we need final layer norm. It’s not there in the paper. Isn’t the layer norm included in the sublayer anyway?</p>
</section>
<section id="reply-105" class="level5">
<h5 class="anchored" data-anchor-id="reply-105">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent question! You’re absolutely right to question this.</p>
<p><strong>You’re correct</strong> - in the paper’s diagram (Figure 1), the layer norm is shown as part of each sublayer connection, not as a separate final norm.</p>
<p><strong>However</strong>, looking at the original Annotated Transformer code, they add a final layer norm after all encoder layers. This is a common implementation detail that helps with: - Stabilizing the output before it goes to the decoder - Ensuring the encoder output is well-normalized</p>
<p><strong>The paper vs implementation:</strong> The paper doesn’t explicitly show this final norm in the diagram, but many implementations (including the original authors’ code) include it for better training stability.</p>
<p>So it’s an implementation choice rather than strictly from the paper. Do you want to: 1. Include it (matches the original code you provided) 2. Skip it (strictly follows the paper diagram)</p>
<p>What would you prefer?</p>
<p>I’ll include it.</p>
</section>
<section id="reply-106" class="level5">
<h5 class="anchored" data-anchor-id="reply-106">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let’s include the final layer norm to match the original code.</p>
<p>So your <code>Encoder</code> should have:</p>
<p><strong>In <code>__init__</code>:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb167"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb167-1"><a href="#cb167-1" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.layers <span class="op">=</span> clones(layer, N)</span>
<span id="cb167-2"><a href="#cb167-2" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(layer.size)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>In <code>forward</code>:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb168"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb168-2"><a href="#cb168-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> layer(x, mask)</span>
<span id="cb168-3"><a href="#cb168-3" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> <span class="va">self</span>.norm(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Can you update your code with these changes?</p>
</section>
<section id="final-encoder-function." class="level3">
<h3 class="anchored" data-anchor-id="final-encoder-function.">Final encoder function.</h3>
<hr>
</section>
<section id="encoder-1" class="level3">
<h3 class="anchored" data-anchor-id="encoder-1">Encoder</h3>
<blockquote class="blockquote">
<pre><code> Encoder (layer, N)</code></pre>
</blockquote>
<p><em>Stack of N encoder layers</em></p>
<p>Is this fine?</p>
<section id="reply-107" class="level5">
<h5 class="anchored" data-anchor-id="reply-107">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>Encoder</code> class is now correct! ✓</p>
<p>You’ve got: - ✓ Cloning N encoder layers - ✓ Final layer normalization - ✓ Sequential application of all layers - ✓ Returning normalized output</p>
<p>Let’s test it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb170"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create components</span></span>
<span id="cb170-2"><a href="#cb170-2" aria-hidden="true" tabindex="-1"></a>attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb170-3"><a href="#cb170-3" aria-hidden="true" tabindex="-1"></a>ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb170-4"><a href="#cb170-4" aria-hidden="true" tabindex="-1"></a>encoder_layer <span class="op">=</span> EncoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>attn, feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb170-5"><a href="#cb170-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-6"><a href="#cb170-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create encoder with N=6 layers</span></span>
<span id="cb170-7"><a href="#cb170-7" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> Encoder(encoder_layer, N<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb170-8"><a href="#cb170-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-9"><a href="#cb170-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb170-10"><a href="#cb170-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)</span>
<span id="cb170-11"><a href="#cb170-11" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> encoder(x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb170-12"><a href="#cb170-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb170-13"><a href="#cb170-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this to verify it works!</p>
<div id="a4e69bdd" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb171"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create components</span></span>
<span id="cb171-2"><a href="#cb171-2" aria-hidden="true" tabindex="-1"></a>attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb171-3"><a href="#cb171-3" aria-hidden="true" tabindex="-1"></a>ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb171-4"><a href="#cb171-4" aria-hidden="true" tabindex="-1"></a>encoder_layer <span class="op">=</span> EncoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>attn, feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb171-5"><a href="#cb171-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-6"><a href="#cb171-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create encoder with N=6 layers</span></span>
<span id="cb171-7"><a href="#cb171-7" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> Encoder(encoder_layer, N<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb171-8"><a href="#cb171-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-9"><a href="#cb171-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb171-10"><a href="#cb171-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)</span>
<span id="cb171-11"><a href="#cb171-11" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> encoder(x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb171-12"><a href="#cb171-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb171-13"><a href="#cb171-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Assume we are using NBDEV and create a unit test out of this.</p>
</section>
<section id="reply-108" class="level5">
<h5 class="anchored" data-anchor-id="reply-108">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Here’s your Encoder as a unit test for nbdev:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb172"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| export</span></span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clones(module, N):</span>
<span id="cb172-3"><a href="#cb172-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Produce N identical layers"</span></span>
<span id="cb172-4"><a href="#cb172-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.nn.ModuleList([copy.deepcopy(module) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(N)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb173"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| export</span></span>
<span id="cb173-2"><a href="#cb173-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(torch.nn.Module):</span>
<span id="cb173-3"><a href="#cb173-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Stack of N encoder layers"</span></span>
<span id="cb173-4"><a href="#cb173-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, layer, N):</span>
<span id="cb173-5"><a href="#cb173-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb173-6"><a href="#cb173-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoders <span class="op">=</span> clones(layer, N)</span>
<span id="cb173-7"><a href="#cb173-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm <span class="op">=</span> torch.nn.LayerNorm(layer.size)</span>
<span id="cb173-8"><a href="#cb173-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb173-9"><a href="#cb173-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask):</span>
<span id="cb173-10"><a href="#cb173-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> encoder <span class="kw">in</span> <span class="va">self</span>.encoders:</span>
<span id="cb173-11"><a href="#cb173-11" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> encoder(x, mask)</span>
<span id="cb173-12"><a href="#cb173-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layer_norm(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb174"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_encoder():</span>
<span id="cb174-3"><a href="#cb174-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Encoder stack"</span></span>
<span id="cb174-4"><a href="#cb174-4" aria-hidden="true" tabindex="-1"></a>    attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb174-5"><a href="#cb174-5" aria-hidden="true" tabindex="-1"></a>    ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb174-6"><a href="#cb174-6" aria-hidden="true" tabindex="-1"></a>    encoder_layer <span class="op">=</span> EncoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>attn, feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb174-7"><a href="#cb174-7" aria-hidden="true" tabindex="-1"></a>    encoder <span class="op">=</span> Encoder(encoder_layer, N<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb174-8"><a href="#cb174-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb174-9"><a href="#cb174-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)</span>
<span id="cb174-10"><a href="#cb174-10" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> encoder(x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb174-11"><a href="#cb174-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb174-12"><a href="#cb174-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Excellent progress! The encoder is complete. Ready to move on to the Decoder components?</p>
</section>
<section id="unit-test-5" class="level4">
<h4 class="anchored" data-anchor-id="unit-test-5">Unit test</h4>
<div id="067d6afb" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb175"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb175-1"><a href="#cb175-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_encoder():</span>
<span id="cb175-2"><a href="#cb175-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Encoder stack"</span></span>
<span id="cb175-3"><a href="#cb175-3" aria-hidden="true" tabindex="-1"></a>    attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb175-4"><a href="#cb175-4" aria-hidden="true" tabindex="-1"></a>    ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb175-5"><a href="#cb175-5" aria-hidden="true" tabindex="-1"></a>    encoder_layer <span class="op">=</span> EncoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>attn, feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb175-6"><a href="#cb175-6" aria-hidden="true" tabindex="-1"></a>    encoder <span class="op">=</span> Encoder(encoder_layer, N<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb175-7"><a href="#cb175-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb175-8"><a href="#cb175-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)</span>
<span id="cb175-9"><a href="#cb175-9" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> encoder(x, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb175-10"><a href="#cb175-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb175-11"><a href="#cb175-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="626d505a" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb176"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a>test_encoder()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
</section>
<section id="decoder-layer" class="level2">
<h2 class="anchored" data-anchor-id="decoder-layer">Decoder Layer</h2>
<p>Now, what is next?</p>
<section id="reply-109" class="level5">
<h5 class="anchored" data-anchor-id="reply-109">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great work! You’ve completed the entire <strong>Encoder</strong> side of the Transformer! 🎉</p>
<p><strong>What’s next: The Decoder</strong></p>
<p>The Decoder is similar to the Encoder but has some key differences:</p>
<ol type="1">
<li><strong>Masked Self-Attention</strong> - prevents looking at future tokens</li>
<li><strong>Cross-Attention</strong> - attends to the encoder output</li>
<li><strong>Three sub-layers</strong> instead of two (self-attn, cross-attn, feed-forward)</li>
</ol>
<p>Let’s start with the <strong>Decoder Layer</strong>. It will have: - Masked multi-head self-attention - Multi-head cross-attention (attends to encoder output) - Position-wise feed-forward - Three <code>SublayerConnection</code> modules (one for each sub-layer)</p>
<p>Before coding, do you understand the difference between: - <strong>Self-attention</strong> (what the encoder uses) - <strong>Masked self-attention</strong> (decoder looks at previous tokens only) - <strong>Cross-attention</strong> (decoder attends to encoder output)</p>
<p>Want me to clarify these concepts first?</p>
<p>I’d rather you quiz me on the concepts.</p>
</section>
<section id="reply-110" class="level5">
<h5 class="anchored" data-anchor-id="reply-110">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let’s test your understanding:</p>
<p><strong>Question 1: Masked Self-Attention</strong></p>
<p>In the decoder, why do we need to mask the self-attention? What would happen if we didn’t mask it during training?</p>
<p><strong>Question 2: Cross-Attention</strong></p>
<p>In cross-attention, where do the Query (Q), Key (K), and Value (V) come from? (Hint: think about what the decoder has access to)</p>
<p><strong>Question 3: Decoder Architecture</strong></p>
<p>How many sublayers does a decoder layer have, and what does each one do?</p>
<p>Give these a shot!</p>
<p>I think masking is needed mostly while training. The thing is, the eventual output is already known. But we want to make sure that we do not leak the future word output words beforehand.</p>
</section>
<section id="reply-111" class="level5">
<h5 class="anchored" data-anchor-id="reply-111">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly right! ✓</p>
<p><strong>Masked Self-Attention:</strong> During training, we have the entire target sequence available. Without masking, the decoder could “cheat” by looking at future words it’s supposed to predict. The mask ensures position i can only attend to positions ≤ i.</p>
<p><strong>Key insight:</strong> During inference (actual use), we generate one token at a time, so there ARE no future tokens to look at. But during training, we need the mask to simulate this autoregressive behavior.</p>
<p>Great answer! Now try <strong>Question 2:</strong></p>
<p>In cross-attention (the second attention mechanism in the decoder), where do Q, K, and V come from? Think about what inputs the decoder has access to.</p>
<p>Let’s explore this in a bit of detail. The input is, “You are good.” And the output is. Vous etes bons</p>
</section>
<section id="reply-112" class="level5">
<h5 class="anchored" data-anchor-id="reply-112">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! Let’s trace through this example step by step.</p>
<p><strong>Input (English):</strong> “You are good” <strong>Output (French):</strong> “Vous êtes bons”</p>
<p><strong>During training, let’s say we’re predicting the 2nd word “êtes”:</strong></p>
<p><strong>Encoder side:</strong> - Processes “You are good” → produces contextualized representations (memory)</p>
<p><strong>Decoder side at position 2:</strong></p>
<ol type="1">
<li><strong>Masked Self-Attention:</strong>
<ul>
<li>Query from “êtes” position</li>
<li>Can attend to: “Vous” and “êtes” (not “bons” - that’s future!)</li>
<li>Q, K, V all come from decoder’s own input</li>
</ul></li>
<li><strong>Cross-Attention:</strong> (This is your Question 2!)
<ul>
<li>Where does Q come from?</li>
<li>Where do K and V come from?</li>
</ul></li>
</ol>
<p>Think about what the decoder needs to “look at” from the encoder. What would make sense?</p>
<p>In cross-attention, Q and K come from the encoder outputs. And V comes from the output of the Masked multi-head attention. So basically, the query is from the input, and the key that is the dictionary is also from the input, but the eventual value will come from the output, or in this case, the translation.</p>
</section>
<section id="reply-113" class="level5">
<h5 class="anchored" data-anchor-id="reply-113">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Not quite! Let me clarify:</p>
<p><strong>In Cross-Attention:</strong></p>
<ul>
<li><strong>Q (Query)</strong>: Comes from the <strong>decoder</strong> (the output of masked self-attention)</li>
<li><strong>K (Key)</strong>: Comes from the <strong>encoder</strong> output (memory)</li>
<li><strong>V (Value)</strong>: Comes from the <strong>encoder</strong> output (memory)</li>
</ul>
<p>So the decoder is asking “what information do I need from the input?” and the encoder provides both the keys (what’s available) and values (the actual information).</p>
<p><strong>Think of it this way:</strong> - Decoder says: “I’m trying to generate ‘êtes’, what from the English sentence is relevant?” (Query) - Encoder says: “Here’s what I have: ‘You’, ‘are’, ‘good’” (Keys) - Decoder attends strongly to “are” - Encoder provides: “Here’s the information about ‘are’” (Value)</p>
<p>Does this clarify where Q, K, V come from in cross-attention?</p>
<p>Okay, that’s clear. That query is from the decoder, and the key and value are from the encoder.</p>
</section>
<section id="reply-114" class="level5">
<h5 class="anchored" data-anchor-id="reply-114">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Now you’ve got it! ✓</p>
<p><strong>Cross-Attention summary:</strong> - Q: from decoder (asking “what do I need?”) - K, V: from encoder (providing “here’s what’s available”)</p>
<p>Now let’s move to <strong>Question 3:</strong></p>
<p>How many sublayers does a decoder layer have, and what does each one do?</p>
<p>Give it a try!</p>
<p>There are three sublayers. The first sub-layer is self-attention on the mast output. The second sub-layer is cross-attention. Like I said, the query is from the decoder, and the key and value from the encoder. And the third sublayer once again is the position-wise feedforward network.</p>
</section>
<section id="reply-115" class="level5">
<h5 class="anchored" data-anchor-id="reply-115">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent! You’ve got it perfectly! ✓</p>
<p><strong>Three sublayers in Decoder Layer:</strong> 1. <strong>Masked Self-Attention</strong> - decoder attends to its own previous outputs 2. <strong>Cross-Attention</strong> - decoder attends to encoder output (Q from decoder, K&amp;V from encoder) 3. <strong>Position-wise Feed-Forward</strong> - same as encoder</p>
<p>Each sublayer has a residual connection + layer norm wrapped around it (using <code>SublayerConnection</code>).</p>
<p>Now you’re ready to implement the <strong>Decoder Layer</strong>!</p>
<p>Looking at the structure, what do you think you’ll need in <code>__init__</code>?</p>
<p>Hint: You’ll need to store 3 attention modules and create 3 <code>SublayerConnection</code> modules.</p>
<p>Want to give it a try?</p>
<p>Yes, I would like to give it a try.</p>
</section>
<section id="reply-116" class="level5">
<h5 class="anchored" data-anchor-id="reply-116">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let’s implement the <strong>Decoder Layer</strong>.</p>
<p>Start with the <code>__init__</code> method. You need to store:</p>
<ol type="1">
<li><code>self.size</code> (d_model)</li>
<li>Three attention modules:
<ul>
<li><code>self.self_attn</code> (masked self-attention)</li>
<li><code>self.src_attn</code> (cross-attention to encoder)</li>
<li><code>self.feed_forward</code> (FFN)</li>
</ul></li>
<li>Three <code>SublayerConnection</code> modules (one for each sublayer)</li>
</ol>
<p>Try writing the <code>__init__</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb177"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb177-1"><a href="#cb177-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DecoderLayer(nn.Module):</span>
<span id="cb177-2"><a href="#cb177-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, self_attn, src_attn, feed_forward, dropout):</span>
<span id="cb177-3"><a href="#cb177-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb177-4"><a href="#cb177-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you need here?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Give it a shot!</p>
<div id="8d7108b0" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb178"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DecoderLayer(torch.nn.Module):</span>
<span id="cb178-2"><a href="#cb178-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, self_attn, src_attn, feed_forward, dropout):</span>
<span id="cb178-3"><a href="#cb178-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb178-4"><a href="#cb178-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you need here?</span></span>
<span id="cb178-5"><a href="#cb178-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.size <span class="op">=</span> size</span>
<span id="cb178-6"><a href="#cb178-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> self_attn</span>
<span id="cb178-7"><a href="#cb178-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src_attn <span class="op">=</span> src_attn</span>
<span id="cb178-8"><a href="#cb178-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> feed_forward</span>
<span id="cb178-9"><a href="#cb178-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sublayers <span class="op">=</span> clones(SublayerConnection(size, dropout), <span class="dv">3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>All good. Be faithful to the original code.</p>
</section>
<section id="reply-117" class="level5">
<h5 class="anchored" data-anchor-id="reply-117">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>__init__</code> is exactly right and matches the original code! ✓</p>
<p>You’ve got: - ✓ All three attention modules stored - ✓ Feed-forward stored - ✓ Three <code>SublayerConnection</code> modules cloned</p>
<p>Now for the <code>forward</code> method. You need to implement:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb179"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask, tgt_mask):</span>
<span id="cb179-2"><a href="#cb179-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># What are the three sublayer operations?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Where: - <code>x</code> = decoder input - <code>memory</code> = encoder output - <code>src_mask</code> = mask for encoder (used in cross-attention) - <code>tgt_mask</code> = mask for decoder (used in masked self-attention)</p>
<p>Try writing the three sublayer calls using lambdas (like you did in EncoderLayer). What would each one look like?</p>
<p>Is this good?</p>
</section>
<section id="reply-118" class="level5">
<h5 class="anchored" data-anchor-id="reply-118">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Close! But there are a few issues to fix:</p>
<ol type="1">
<li><p><strong>First sublayer</strong>: You’re missing the <code>x</code> argument to the lambda, and the mask should be <code>tgt_mask</code> (not <code>None</code>):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb180"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">0</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.self_attn(x, x, x, tgt_mask))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Second sublayer</strong>: What are <code>K</code> and <code>V</code>? They should come from <code>memory</code> (the encoder output). The query comes from decoder (<code>x</code>), but keys and values come from encoder:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb181"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">1</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.src_attn(x, memory, memory, src_mask))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Third sublayer</strong>: This one is correct!</p></li>
</ol>
<p>Can you fix the first two sublayers?</p>
<p>I’m not sure I understand what’s the difference between tgt_mask and src_mask. Can you explain to me? I would guess that TGT mask is to avoid leakage of outputs, but I am not sure about SRC mask.</p>
</section>
<section id="reply-119" class="level5">
<h5 class="anchored" data-anchor-id="reply-119">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! Let me clarify the two masks:</p>
<p><strong>tgt_mask (target mask):</strong> You’re exactly right! This is the <strong>causal/look-ahead mask</strong> for the decoder. It prevents position i from seeing positions &gt; i. This is the triangular mask that ensures no future leakage.</p>
<p><strong>src_mask (source mask):</strong> This is a <strong>padding mask</strong> for the encoder output. Remember when you batch sentences together, shorter sentences get padded with <code>&lt;pad&gt;</code> tokens? The src_mask tells the decoder “don’t pay attention to these padding positions in the encoder output - they’re meaningless.”</p>
<p><strong>Where they’re used:</strong> - <code>tgt_mask</code>: In masked self-attention (decoder looking at its own tokens) - <code>src_mask</code>: In cross-attention (decoder looking at encoder output)</p>
<p><strong>Example:</strong></p>
<pre><code>Source: "Hello world &lt;pad&gt; &lt;pad&gt;"  ← src_mask hides the &lt;pad&gt;
Target: "Bonjour le monde"         ← tgt_mask prevents future leakage</code></pre>
<p>Does this clarify the difference?</p>
<div id="6d5230b1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb183"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask, tgt_mask):</span>
<span id="cb183-2"><a href="#cb183-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># What are the three sublayer operations?</span></span>
<span id="cb183-3"><a href="#cb183-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">0</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.self_attn(x, x, x, tgt_mask))</span>
<span id="cb183-4"><a href="#cb183-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">1</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.src_attn(x, K, V, src_mask))</span>
<span id="cb183-5"><a href="#cb183-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">2</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.feed_forward(x))</span>
<span id="cb183-6"><a href="#cb183-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this correct?</p>
</section>
<section id="reply-120" class="level5">
<h5 class="anchored" data-anchor-id="reply-120">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Almost! You still have <code>K</code> and <code>V</code> as variables in the second sublayer, but they’re not defined.</p>
<p>Remember: in cross-attention, both the keys and values come from <code>memory</code> (the encoder output). So it should be:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb184"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb184-1"><a href="#cb184-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">1</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.src_attn(x, memory, memory, src_mask))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The pattern is: <code>self.src_attn(query, key, value, mask)</code> where: - query = <code>x</code> (from decoder) - key = <code>memory</code> (from encoder) - value = <code>memory</code> (from encoder)</p>
<p>Can you fix that line?</p>
<div id="21a0cdd7" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb185"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask, tgt_mask):</span>
<span id="cb185-2"><a href="#cb185-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># What are the three sublayer operations?</span></span>
<span id="cb185-3"><a href="#cb185-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">0</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.self_attn(x, x, x, tgt_mask))</span>
<span id="cb185-4"><a href="#cb185-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">1</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.src_attn(x, memory, memory, src_mask))</span>
<span id="cb185-5"><a href="#cb185-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">2</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.feed_forward(x))</span>
<span id="cb185-6"><a href="#cb185-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this correct? Stay loyal to the original code.</p>
</section>
<section id="reply-121" class="level5">
<h5 class="anchored" data-anchor-id="reply-121">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>DecoderLayer.forward</code> is now correct and matches the original code! ✓</p>
<p>You’ve got: - ✓ Masked self-attention with <code>tgt_mask</code> - ✓ Cross-attention with query from decoder, K&amp;V from encoder memory, using <code>src_mask</code> - ✓ Feed-forward network</p>
<p>Now let’s combine the <code>__init__</code> and <code>forward</code> into the complete class and test it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb186"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb186-1"><a href="#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DecoderLayer(torch.nn.Module):</span>
<span id="cb186-2"><a href="#cb186-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, self_attn, src_attn, feed_forward, dropout):</span>
<span id="cb186-3"><a href="#cb186-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb186-4"><a href="#cb186-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.size <span class="op">=</span> size</span>
<span id="cb186-5"><a href="#cb186-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> self_attn</span>
<span id="cb186-6"><a href="#cb186-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src_attn <span class="op">=</span> src_attn</span>
<span id="cb186-7"><a href="#cb186-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> feed_forward</span>
<span id="cb186-8"><a href="#cb186-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sublayers <span class="op">=</span> clones(SublayerConnection(size, dropout), <span class="dv">3</span>)</span>
<span id="cb186-9"><a href="#cb186-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb186-10"><a href="#cb186-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask, tgt_mask):</span>
<span id="cb186-11"><a href="#cb186-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">0</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.self_attn(x, x, x, tgt_mask))</span>
<span id="cb186-12"><a href="#cb186-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">1</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.src_attn(x, memory, memory, src_mask))</span>
<span id="cb186-13"><a href="#cb186-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">2</span>](x, <span class="va">self</span>.feed_forward)</span>
<span id="cb186-14"><a href="#cb186-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Want to test it now?</p>
</section>
<section id="final-decoder-layer-function." class="level3">
<h3 class="anchored" data-anchor-id="final-decoder-layer-function.">Final decoder layer Function.</h3>
<hr>
</section>
<section id="decoderlayer" class="level3">
<h3 class="anchored" data-anchor-id="decoderlayer">DecoderLayer</h3>
<blockquote class="blockquote">
<pre><code> DecoderLayer (size, self_attn, src_attn, feed_forward, dropout)</code></pre>
</blockquote>
<p>*Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:<code>to</code>, etc.</p>
<p>.. note:: As per the example above, an <code>__init__()</code> call to the parent class must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*</p>
<p>Okay, let’s test it.</p>
<section id="reply-122" class="level5">
<h5 class="anchored" data-anchor-id="reply-122">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! Let’s test your <code>DecoderLayer</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb189"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create components</span></span>
<span id="cb189-2"><a href="#cb189-2" aria-hidden="true" tabindex="-1"></a>self_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb189-3"><a href="#cb189-3" aria-hidden="true" tabindex="-1"></a>src_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb189-4"><a href="#cb189-4" aria-hidden="true" tabindex="-1"></a>ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb189-5"><a href="#cb189-5" aria-hidden="true" tabindex="-1"></a>decoder_layer <span class="op">=</span> DecoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>self_attn, src_attn<span class="op">=</span>src_attn, </span>
<span id="cb189-6"><a href="#cb189-6" aria-hidden="true" tabindex="-1"></a>                             feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb189-7"><a href="#cb189-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-8"><a href="#cb189-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb189-9"><a href="#cb189-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># decoder input</span></span>
<span id="cb189-10"><a href="#cb189-10" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">15</span>, <span class="dv">512</span>)  <span class="co"># encoder output (different seq_len is fine)</span></span>
<span id="cb189-11"><a href="#cb189-11" aria-hidden="true" tabindex="-1"></a>src_mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb189-12"><a href="#cb189-12" aria-hidden="true" tabindex="-1"></a>tgt_mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb189-13"><a href="#cb189-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-14"><a href="#cb189-14" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> decoder_layer(x, memory, src_mask, tgt_mask)</span>
<span id="cb189-15"><a href="#cb189-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb189-16"><a href="#cb189-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this and see if it works!</p>
<div id="0a5e9f1c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb190"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb190-1"><a href="#cb190-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create components</span></span>
<span id="cb190-2"><a href="#cb190-2" aria-hidden="true" tabindex="-1"></a>self_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb190-3"><a href="#cb190-3" aria-hidden="true" tabindex="-1"></a>src_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb190-4"><a href="#cb190-4" aria-hidden="true" tabindex="-1"></a>ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb190-5"><a href="#cb190-5" aria-hidden="true" tabindex="-1"></a>decoder_layer <span class="op">=</span> DecoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>self_attn, src_attn<span class="op">=</span>src_attn, </span>
<span id="cb190-6"><a href="#cb190-6" aria-hidden="true" tabindex="-1"></a>                             feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb190-7"><a href="#cb190-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-8"><a href="#cb190-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb190-9"><a href="#cb190-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># decoder input</span></span>
<span id="cb190-10"><a href="#cb190-10" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">15</span>, <span class="dv">512</span>)  <span class="co"># encoder output (different seq_len is fine)</span></span>
<span id="cb190-11"><a href="#cb190-11" aria-hidden="true" tabindex="-1"></a>src_mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb190-12"><a href="#cb190-12" aria-hidden="true" tabindex="-1"></a>tgt_mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb190-13"><a href="#cb190-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-14"><a href="#cb190-14" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> decoder_layer(x, memory, src_mask, tgt_mask)</span>
<span id="cb190-15"><a href="#cb190-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb190-16"><a href="#cb190-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>What happened, bro?</p>
</section>
<section id="reply-123" class="level5">
<h5 class="anchored" data-anchor-id="reply-123">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Ah, I see the issue! The problem is in your <code>MultiHeadedAttention.forward</code> method.</p>
<p><strong>The bug:</strong> In cross-attention, the <code>key</code> and <code>value</code> come from <code>memory</code> which has a <strong>different sequence length</strong> (15) than the query which comes from <code>x</code> (10).</p>
<p>But in your code, you’re using <code>seq_len</code> from Q for reshaping K and V:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb191"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> Q.size(<span class="dv">1</span>)  <span class="co"># This is 10 (from decoder)</span></span>
<span id="cb191-2"><a href="#cb191-2" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> K.reshape(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k)  <span class="co"># But K has seq_len=15!</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>The fix:</strong> You need to get the sequence length from each tensor individually:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb192"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb192-1"><a href="#cb192-1" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> Q.reshape(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb192-2"><a href="#cb192-2" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> K.reshape(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb192-3"><a href="#cb192-3" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> V.reshape(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Using <code>-1</code> lets PyTorch infer the sequence length automatically!</p>
<p>Can you update your <code>MultiHeadedAttention</code> class to fix this?</p>
<p>Are you sure? Is this honest to the original code?</p>
</section>
<section id="reply-124" class="level5">
<h5 class="anchored" data-anchor-id="reply-124">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You’re right to question me! Let me check the original code more carefully.</p>
<p>Looking at the original <code>MultiHeadedAttention.forward</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb193"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a>nbatches <span class="op">=</span> query.size(<span class="dv">0</span>)</span>
<span id="cb193-2"><a href="#cb193-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-3"><a href="#cb193-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span></span>
<span id="cb193-4"><a href="#cb193-4" aria-hidden="true" tabindex="-1"></a>query, key, value <span class="op">=</span> [</span>
<span id="cb193-5"><a href="#cb193-5" aria-hidden="true" tabindex="-1"></a>    lin(x).view(nbatches, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb193-6"><a href="#cb193-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> lin, x <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.linears, (query, key, value))</span>
<span id="cb193-7"><a href="#cb193-7" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>You’re right - the original code uses <code>-1</code> to automatically infer the sequence length! So yes, my fix is correct and matches the original.</p>
<p>Your current code has:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb194"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb194-1"><a href="#cb194-1" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> Q.size(<span class="dv">1</span>)</span>
<span id="cb194-2"><a href="#cb194-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> Q.reshape(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb194-3"><a href="#cb194-3" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> K.reshape(batch_size, seq_len, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)  <span class="co"># Bug here!</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Should be:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb195"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> Q.reshape(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb195-2"><a href="#cb195-2" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> K.reshape(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb195-3"><a href="#cb195-3" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> V.reshape(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Can you update your <code>MultiHeadedAttention</code> class?</p>
<p>I could fix that, but what about the part where we’re concatenating and we’re using seq_length again?</p>
</section>
<section id="reply-125" class="level5">
<h5 class="anchored" data-anchor-id="reply-125">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent catch! You’re absolutely right - there’s the same issue in the concatenation step!</p>
<p>Looking at your code, you probably have something like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb196"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb196-1"><a href="#cb196-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x.transpose(<span class="dv">1</span>,<span class="dv">2</span>).reshape(batch_size, seq_len, <span class="op">-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>But <code>seq_len</code> was computed from Q, so it won’t work when K and V have different lengths.</p>
<p><strong>The fix:</strong> Use <code>-1</code> there too:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb197"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x.transpose(<span class="dv">1</span>,<span class="dv">2</span>).reshape(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h <span class="op">*</span> <span class="va">self</span>.d_k)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Or even simpler (matching the original code):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb198"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb198-1"><a href="#cb198-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x.transpose(<span class="dv">1</span>,<span class="dv">2</span>).contiguous().view(nbatches, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h <span class="op">*</span> <span class="va">self</span>.d_k)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Can you update both the reshaping at the beginning AND the concatenation at the end of your <code>MultiHeadedAttention.forward</code> method?</p>
<p>fixed it now</p>
</section>
<section id="reply-126" class="level5">
<h5 class="anchored" data-anchor-id="reply-126">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! Now let’s test your <code>DecoderLayer</code> again with the fixed <code>MultiHeadedAttention</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb199"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create components</span></span>
<span id="cb199-2"><a href="#cb199-2" aria-hidden="true" tabindex="-1"></a>self_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb199-3"><a href="#cb199-3" aria-hidden="true" tabindex="-1"></a>src_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb199-4"><a href="#cb199-4" aria-hidden="true" tabindex="-1"></a>ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb199-5"><a href="#cb199-5" aria-hidden="true" tabindex="-1"></a>decoder_layer <span class="op">=</span> DecoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>self_attn, src_attn<span class="op">=</span>src_attn, </span>
<span id="cb199-6"><a href="#cb199-6" aria-hidden="true" tabindex="-1"></a>                             feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb199-7"><a href="#cb199-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-8"><a href="#cb199-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb199-9"><a href="#cb199-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># decoder input</span></span>
<span id="cb199-10"><a href="#cb199-10" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">15</span>, <span class="dv">512</span>)  <span class="co"># encoder output (different seq_len)</span></span>
<span id="cb199-11"><a href="#cb199-11" aria-hidden="true" tabindex="-1"></a>src_mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb199-12"><a href="#cb199-12" aria-hidden="true" tabindex="-1"></a>tgt_mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb199-13"><a href="#cb199-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-14"><a href="#cb199-14" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> decoder_layer(x, memory, src_mask, tgt_mask)</span>
<span id="cb199-15"><a href="#cb199-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb199-16"><a href="#cb199-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this and let me know if it works now!</p>
<div id="e090d2e8" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb200"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb200-1"><a href="#cb200-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create components</span></span>
<span id="cb200-2"><a href="#cb200-2" aria-hidden="true" tabindex="-1"></a>self_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb200-3"><a href="#cb200-3" aria-hidden="true" tabindex="-1"></a>src_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb200-4"><a href="#cb200-4" aria-hidden="true" tabindex="-1"></a>ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb200-5"><a href="#cb200-5" aria-hidden="true" tabindex="-1"></a>decoder_layer <span class="op">=</span> DecoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>self_attn, src_attn<span class="op">=</span>src_attn, </span>
<span id="cb200-6"><a href="#cb200-6" aria-hidden="true" tabindex="-1"></a>                             feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb200-7"><a href="#cb200-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb200-8"><a href="#cb200-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb200-9"><a href="#cb200-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># decoder input</span></span>
<span id="cb200-10"><a href="#cb200-10" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">15</span>, <span class="dv">512</span>)  <span class="co"># encoder output (different seq_len)</span></span>
<span id="cb200-11"><a href="#cb200-11" aria-hidden="true" tabindex="-1"></a>src_mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb200-12"><a href="#cb200-12" aria-hidden="true" tabindex="-1"></a>tgt_mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb200-13"><a href="#cb200-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb200-14"><a href="#cb200-14" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> decoder_layer(x, memory, src_mask, tgt_mask)</span>
<span id="cb200-15"><a href="#cb200-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb200-16"><a href="#cb200-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Awesome, assume we are using NBDev and create a unit test out of this.</p>
</section>
<section id="reply-127" class="level5">
<h5 class="anchored" data-anchor-id="reply-127">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Here’s your Decoder Layer as a unit test for nbdev:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb201"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb201-1"><a href="#cb201-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| export</span></span>
<span id="cb201-2"><a href="#cb201-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DecoderLayer(torch.nn.Module):</span>
<span id="cb201-3"><a href="#cb201-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Decoder layer with masked self-attention, cross-attention, and feed-forward"</span></span>
<span id="cb201-4"><a href="#cb201-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, self_attn, src_attn, feed_forward, dropout):</span>
<span id="cb201-5"><a href="#cb201-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb201-6"><a href="#cb201-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.size <span class="op">=</span> size</span>
<span id="cb201-7"><a href="#cb201-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> self_attn</span>
<span id="cb201-8"><a href="#cb201-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src_attn <span class="op">=</span> src_attn</span>
<span id="cb201-9"><a href="#cb201-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> feed_forward</span>
<span id="cb201-10"><a href="#cb201-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sublayers <span class="op">=</span> clones(SublayerConnection(size, dropout), <span class="dv">3</span>)</span>
<span id="cb201-11"><a href="#cb201-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb201-12"><a href="#cb201-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask, tgt_mask):</span>
<span id="cb201-13"><a href="#cb201-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">0</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.self_attn(x, x, x, tgt_mask))</span>
<span id="cb201-14"><a href="#cb201-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">1</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.src_attn(x, memory, memory, src_mask))</span>
<span id="cb201-15"><a href="#cb201-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayers[<span class="dv">2</span>](x, <span class="va">self</span>.feed_forward)</span>
<span id="cb201-16"><a href="#cb201-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb202"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb202-1"><a href="#cb202-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb202-2"><a href="#cb202-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_decoder_layer():</span>
<span id="cb202-3"><a href="#cb202-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Decoder Layer with cross-attention"</span></span>
<span id="cb202-4"><a href="#cb202-4" aria-hidden="true" tabindex="-1"></a>    self_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb202-5"><a href="#cb202-5" aria-hidden="true" tabindex="-1"></a>    src_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb202-6"><a href="#cb202-6" aria-hidden="true" tabindex="-1"></a>    ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb202-7"><a href="#cb202-7" aria-hidden="true" tabindex="-1"></a>    decoder_layer <span class="op">=</span> DecoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>self_attn, src_attn<span class="op">=</span>src_attn, </span>
<span id="cb202-8"><a href="#cb202-8" aria-hidden="true" tabindex="-1"></a>                                 feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb202-9"><a href="#cb202-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb202-10"><a href="#cb202-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># decoder input</span></span>
<span id="cb202-11"><a href="#cb202-11" aria-hidden="true" tabindex="-1"></a>    memory <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">15</span>, <span class="dv">512</span>)  <span class="co"># encoder output</span></span>
<span id="cb202-12"><a href="#cb202-12" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> decoder_layer(x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb202-13"><a href="#cb202-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb202-14"><a href="#cb202-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Excellent work! Ready to build the full Decoder stack next?</p>
</section>
<section id="unit-test-6" class="level4">
<h4 class="anchored" data-anchor-id="unit-test-6">Unit test</h4>
<div id="4561cfda" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb203"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb203-1"><a href="#cb203-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_decoder_layer():</span>
<span id="cb203-2"><a href="#cb203-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Decoder Layer with cross-attention"</span></span>
<span id="cb203-3"><a href="#cb203-3" aria-hidden="true" tabindex="-1"></a>    self_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb203-4"><a href="#cb203-4" aria-hidden="true" tabindex="-1"></a>    src_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb203-5"><a href="#cb203-5" aria-hidden="true" tabindex="-1"></a>    ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb203-6"><a href="#cb203-6" aria-hidden="true" tabindex="-1"></a>    decoder_layer <span class="op">=</span> DecoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>self_attn, src_attn<span class="op">=</span>src_attn, </span>
<span id="cb203-7"><a href="#cb203-7" aria-hidden="true" tabindex="-1"></a>                                 feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb203-8"><a href="#cb203-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb203-9"><a href="#cb203-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># decoder input</span></span>
<span id="cb203-10"><a href="#cb203-10" aria-hidden="true" tabindex="-1"></a>    memory <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">15</span>, <span class="dv">512</span>)  <span class="co"># encoder output</span></span>
<span id="cb203-11"><a href="#cb203-11" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> decoder_layer(x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb203-12"><a href="#cb203-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb203-13"><a href="#cb203-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
</section>
<section id="decoder-1" class="level2">
<h2 class="anchored" data-anchor-id="decoder-1">Decoder</h2>
<p>Amazing progress, now let’s finish this. Stay honest to the original code.</p>
<section id="reply-128" class="level5">
<h5 class="anchored" data-anchor-id="reply-128">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent! You’re almost done with the Transformer! 🎉</p>
<p><strong>What’s left:</strong></p>
<ol type="1">
<li><strong>Decoder</strong> - Stack N=6 decoder layers (similar to how you built Encoder)</li>
<li><strong>Embeddings</strong> - Convert token IDs to vectors</li>
<li><strong>Generator</strong> - Final linear + softmax to predict next token</li>
<li><strong>EncoderDecoder</strong> - The complete model that ties everything together</li>
</ol>
<p>Let’s start with the <strong>Decoder</strong> stack. It’s very similar to the Encoder you already built.</p>
<p>Looking at the original code, what do you think the Decoder class needs?</p>
<p>Hint: It’s almost identical to Encoder, but the <code>forward</code> method takes different arguments. Try writing the skeleton:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb204"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb204-1"><a href="#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Decoder(nn.Module):</span>
<span id="cb204-2"><a href="#cb204-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, layer, N):</span>
<span id="cb204-3"><a href="#cb204-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb204-4"><a href="#cb204-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you need?</span></span>
<span id="cb204-5"><a href="#cb204-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb204-6"><a href="#cb204-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask, tgt_mask):</span>
<span id="cb204-7"><a href="#cb204-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What's the sequence of operations?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Give it a try!</p>
</section>
<section id="final-decoder-function." class="level3">
<h3 class="anchored" data-anchor-id="final-decoder-function.">Final decoder function.</h3>
<hr>
</section>
<section id="decoder-2" class="level3">
<h3 class="anchored" data-anchor-id="decoder-2">Decoder</h3>
<blockquote class="blockquote">
<pre><code> Decoder (layer, N)</code></pre>
</blockquote>
<p>*Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:<code>to</code>, etc.</p>
<p>.. note:: As per the example above, an <code>__init__()</code> call to the parent class must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*</p>
<p>Have we done it?</p>
<section id="reply-129" class="level5">
<h5 class="anchored" data-anchor-id="reply-129">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>Decoder</code> class is exactly right and matches the original code! ✓</p>
<p>You’ve got: - ✓ Cloning N decoder layers - ✓ Final layer normalization - ✓ Sequential application passing all 4 arguments (x, memory, src_mask, tgt_mask) - ✓ Returning normalized output</p>
<p>Let’s test it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb207"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb207-1"><a href="#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create components</span></span>
<span id="cb207-2"><a href="#cb207-2" aria-hidden="true" tabindex="-1"></a>self_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb207-3"><a href="#cb207-3" aria-hidden="true" tabindex="-1"></a>src_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb207-4"><a href="#cb207-4" aria-hidden="true" tabindex="-1"></a>ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb207-5"><a href="#cb207-5" aria-hidden="true" tabindex="-1"></a>decoder_layer <span class="op">=</span> DecoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>self_attn, src_attn<span class="op">=</span>src_attn, </span>
<span id="cb207-6"><a href="#cb207-6" aria-hidden="true" tabindex="-1"></a>                             feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb207-7"><a href="#cb207-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-8"><a href="#cb207-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create decoder with N=6 layers</span></span>
<span id="cb207-9"><a href="#cb207-9" aria-hidden="true" tabindex="-1"></a>decoder <span class="op">=</span> Decoder(decoder_layer, N<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb207-10"><a href="#cb207-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-11"><a href="#cb207-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb207-12"><a href="#cb207-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)</span>
<span id="cb207-13"><a href="#cb207-13" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">15</span>, <span class="dv">512</span>)</span>
<span id="cb207-14"><a href="#cb207-14" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> decoder(x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb207-15"><a href="#cb207-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb207-16"><a href="#cb207-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this to verify!</p>
<div id="5dcaaff1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb208"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create components</span></span>
<span id="cb208-2"><a href="#cb208-2" aria-hidden="true" tabindex="-1"></a>self_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb208-3"><a href="#cb208-3" aria-hidden="true" tabindex="-1"></a>src_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb208-4"><a href="#cb208-4" aria-hidden="true" tabindex="-1"></a>ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb208-5"><a href="#cb208-5" aria-hidden="true" tabindex="-1"></a>decoder_layer <span class="op">=</span> DecoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>self_attn, src_attn<span class="op">=</span>src_attn, </span>
<span id="cb208-6"><a href="#cb208-6" aria-hidden="true" tabindex="-1"></a>                             feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb208-7"><a href="#cb208-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-8"><a href="#cb208-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create decoder with N=6 layers</span></span>
<span id="cb208-9"><a href="#cb208-9" aria-hidden="true" tabindex="-1"></a>decoder <span class="op">=</span> Decoder(decoder_layer, N<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb208-10"><a href="#cb208-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-11"><a href="#cb208-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb208-12"><a href="#cb208-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)</span>
<span id="cb208-13"><a href="#cb208-13" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">15</span>, <span class="dv">512</span>)</span>
<span id="cb208-14"><a href="#cb208-14" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> decoder(x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb208-15"><a href="#cb208-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb208-16"><a href="#cb208-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Amazing! Then now assume you’re a NB Dev. No need to show the original function but show the unit test.</p>
</section>
<section id="reply-130" class="level5">
<h5 class="anchored" data-anchor-id="reply-130">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb209"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb209-1"><a href="#cb209-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb209-2"><a href="#cb209-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_decoder():</span>
<span id="cb209-3"><a href="#cb209-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Decoder stack"</span></span>
<span id="cb209-4"><a href="#cb209-4" aria-hidden="true" tabindex="-1"></a>    self_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb209-5"><a href="#cb209-5" aria-hidden="true" tabindex="-1"></a>    src_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb209-6"><a href="#cb209-6" aria-hidden="true" tabindex="-1"></a>    ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb209-7"><a href="#cb209-7" aria-hidden="true" tabindex="-1"></a>    decoder_layer <span class="op">=</span> DecoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>self_attn, src_attn<span class="op">=</span>src_attn, </span>
<span id="cb209-8"><a href="#cb209-8" aria-hidden="true" tabindex="-1"></a>                                 feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb209-9"><a href="#cb209-9" aria-hidden="true" tabindex="-1"></a>    decoder <span class="op">=</span> Decoder(decoder_layer, N<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb209-10"><a href="#cb209-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb209-11"><a href="#cb209-11" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)</span>
<span id="cb209-12"><a href="#cb209-12" aria-hidden="true" tabindex="-1"></a>    memory <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">15</span>, <span class="dv">512</span>)</span>
<span id="cb209-13"><a href="#cb209-13" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> decoder(x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb209-14"><a href="#cb209-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb209-15"><a href="#cb209-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Fantastic progress! You’ve now built both the complete Encoder and Decoder stacks! What’s next - Embeddings, Generator, or the final EncoderDecoder wrapper?</p>
</section>
</section>
<section id="unit-test-7" class="level3">
<h3 class="anchored" data-anchor-id="unit-test-7">Unit test</h3>
<div id="33ead8c0" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb210"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_decoder():</span>
<span id="cb210-2"><a href="#cb210-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Decoder stack"</span></span>
<span id="cb210-3"><a href="#cb210-3" aria-hidden="true" tabindex="-1"></a>    self_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb210-4"><a href="#cb210-4" aria-hidden="true" tabindex="-1"></a>    src_attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb210-5"><a href="#cb210-5" aria-hidden="true" tabindex="-1"></a>    ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb210-6"><a href="#cb210-6" aria-hidden="true" tabindex="-1"></a>    decoder_layer <span class="op">=</span> DecoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>self_attn, src_attn<span class="op">=</span>src_attn, </span>
<span id="cb210-7"><a href="#cb210-7" aria-hidden="true" tabindex="-1"></a>                                 feed_forward<span class="op">=</span>ff, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb210-8"><a href="#cb210-8" aria-hidden="true" tabindex="-1"></a>    decoder <span class="op">=</span> Decoder(decoder_layer, N<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb210-9"><a href="#cb210-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb210-10"><a href="#cb210-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)</span>
<span id="cb210-11"><a href="#cb210-11" aria-hidden="true" tabindex="-1"></a>    memory <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">15</span>, <span class="dv">512</span>)</span>
<span id="cb210-12"><a href="#cb210-12" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> decoder(x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb210-13"><a href="#cb210-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb210-14"><a href="#cb210-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="embeddings" class="level2">
<h2 class="anchored" data-anchor-id="embeddings">Embeddings</h2>
<p>Okay, let’s do embeddings.</p>
<section id="reply-131" class="level5">
<h5 class="anchored" data-anchor-id="reply-131">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! Let’s implement <strong>Embeddings</strong>.</p>
<p><strong>What are embeddings?</strong></p>
<p>They convert discrete token IDs (integers) into continuous vector representations. For example: - Token ID 42 → vector of size d_model=512</p>
<p>Looking at the paper and original code, the Embeddings class: 1. Uses <code>nn.Embedding</code> to create a lookup table 2. Multiplies the output by √d_model (for scaling)</p>
<p><strong>Why multiply by √d_model?</strong></p>
<p>This scaling ensures that the embedding values and positional encoding values are on a similar scale before adding them together.</p>
<p>Try implementing it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb211"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb211-1"><a href="#cb211-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Embeddings(nn.Module):</span>
<span id="cb211-2"><a href="#cb211-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, vocab):</span>
<span id="cb211-3"><a href="#cb211-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb211-4"><a href="#cb211-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you need here?</span></span>
<span id="cb211-5"><a href="#cb211-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb211-6"><a href="#cb211-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb211-7"><a href="#cb211-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What's the operation?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What do you need to store in <code>__init__</code>, and what operation goes in <code>forward</code>?</p>
<div id="22d084cc" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb212"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb212-1"><a href="#cb212-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Embeddings(torch.nn.Module):</span>
<span id="cb212-2"><a href="#cb212-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, vocab):</span>
<span id="cb212-3"><a href="#cb212-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb212-4"><a href="#cb212-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you need here?</span></span>
<span id="cb212-5"><a href="#cb212-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> torch.nn.Embedding(vocab, d_model)</span>
<span id="cb212-6"><a href="#cb212-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb212-7"><a href="#cb212-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb212-8"><a href="#cb212-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What's the operation?</span></span>
<span id="cb212-9"><a href="#cb212-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.embedding(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this correct?</p>
</section>
<section id="reply-132" class="level5">
<h5 class="anchored" data-anchor-id="reply-132">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Almost! You’re missing the scaling factor. Remember, the paper multiplies the embeddings by √d_model.</p>
<p>You need to: 1. Store <code>d_model</code> in <code>__init__</code> so you can use it in <code>forward</code> 2. Multiply the embedding output by <code>math.sqrt(self.d_model)</code></p>
<p>Can you add these two things?</p>
<p>Why are we doing this? I don’t understand.</p>
</section>
<section id="reply-133" class="level5">
<h5 class="anchored" data-anchor-id="reply-133">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! Let me explain why we scale by √d_model.</p>
<p><strong>The problem:</strong></p>
<p>After creating embeddings, you <strong>add</strong> them to positional encodings:</p>
<pre><code>final_input = embeddings + positional_encoding</code></pre>
<p>But embeddings from <code>nn.Embedding</code> typically have small values (initialized around -0.1 to 0.1), while positional encodings use sin/cos which range from -1 to 1.</p>
<p><strong>Without scaling:</strong> - Embeddings: small values (~0.1) - Positional encoding: larger values (~1.0) - The positional encoding would dominate!</p>
<p><strong>With √d_model scaling:</strong> - If d_model = 512, then √512 ≈ 22.6 - Embeddings get multiplied by ~22.6, making them comparable in magnitude to positional encodings - Now both contribute meaningfully to the final representation</p>
<p><strong>In short:</strong> It balances the contribution of embeddings and positional encodings so neither dominates.</p>
<p>Does this make sense now?</p>
</section>
<section id="final-embeddings-model-function" class="level3">
<h3 class="anchored" data-anchor-id="final-embeddings-model-function">Final Embeddings Model Function</h3>
<hr>
</section>
<section id="embeddings-1" class="level3">
<h3 class="anchored" data-anchor-id="embeddings-1">Embeddings</h3>
<blockquote class="blockquote">
<pre><code> Embeddings (d_model, vocab)</code></pre>
</blockquote>
<p>*Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:<code>to</code>, etc.</p>
<p>.. note:: As per the example above, an <code>__init__()</code> call to the parent class must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*</p>
<p>Are you happy now?</p>
<section id="reply-134" class="level5">
<h5 class="anchored" data-anchor-id="reply-134">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>Embeddings</code> class is now correct and matches the original code! ✓</p>
<p>You’ve got: - ✓ Storing d_model - ✓ Creating the embedding layer - ✓ Scaling by √d_model in forward</p>
<p>Let’s test it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb216"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb216-1"><a href="#cb216-1" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> Embeddings(d_model<span class="op">=</span><span class="dv">512</span>, vocab<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb216-2"><a href="#cb216-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10000</span>, (<span class="dv">2</span>, <span class="dv">10</span>))  <span class="co"># batch=2, seq_len=10, random token IDs</span></span>
<span id="cb216-3"><a href="#cb216-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> embeddings(x)</span>
<span id="cb216-4"><a href="#cb216-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb216-5"><a href="#cb216-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this to verify it works!</p>
<div id="00a64aa0" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb217"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb217-1"><a href="#cb217-1" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> Embeddings(d_model<span class="op">=</span><span class="dv">512</span>, vocab<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb217-2"><a href="#cb217-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10000</span>, (<span class="dv">2</span>, <span class="dv">10</span>))  <span class="co"># batch=2, seq_len=10, random token IDs</span></span>
<span id="cb217-3"><a href="#cb217-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> embeddings(x)</span>
<span id="cb217-4"><a href="#cb217-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb217-5"><a href="#cb217-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>That’s amazing. Now, assume we’re using NBDEV. We don’t need to output the original function, just the unit test.</p>
</section>
<section id="reply-135" class="level5">
<h5 class="anchored" data-anchor-id="reply-135">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb218"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb218-1"><a href="#cb218-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb218-2"><a href="#cb218-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_embeddings():</span>
<span id="cb218-3"><a href="#cb218-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Embeddings with scaling"</span></span>
<span id="cb218-4"><a href="#cb218-4" aria-hidden="true" tabindex="-1"></a>    embeddings <span class="op">=</span> Embeddings(d_model<span class="op">=</span><span class="dv">512</span>, vocab<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb218-5"><a href="#cb218-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10000</span>, (<span class="dv">2</span>, <span class="dv">10</span>))  <span class="co"># batch=2, seq_len=10</span></span>
<span id="cb218-6"><a href="#cb218-6" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> embeddings(x)</span>
<span id="cb218-7"><a href="#cb218-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb218-8"><a href="#cb218-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span>
<span id="cb218-9"><a href="#cb218-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check that scaling is applied (output should be larger than raw embedding)</span></span>
<span id="cb218-10"><a href="#cb218-10" aria-hidden="true" tabindex="-1"></a>    raw_embed <span class="op">=</span> embeddings.embedding(x)</span>
<span id="cb218-11"><a href="#cb218-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> torch.allclose(output, raw_embed <span class="op">*</span> math.sqrt(<span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Great! What’s next - Generator or the final EncoderDecoder wrapper?</p>
</section>
</section>
<section id="unit-test-8" class="level3">
<h3 class="anchored" data-anchor-id="unit-test-8">Unit test</h3>
<div id="863259c8" class="cell" data-time_run="2025-11-30T18:10:51.285432+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb219"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb219-1"><a href="#cb219-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_embeddings():</span>
<span id="cb219-2"><a href="#cb219-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Embeddings with scaling"</span></span>
<span id="cb219-3"><a href="#cb219-3" aria-hidden="true" tabindex="-1"></a>    embeddings <span class="op">=</span> Embeddings(d_model<span class="op">=</span><span class="dv">512</span>, vocab<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb219-4"><a href="#cb219-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10000</span>, (<span class="dv">2</span>, <span class="dv">10</span>))  <span class="co"># batch=2, seq_len=10</span></span>
<span id="cb219-5"><a href="#cb219-5" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> embeddings(x)</span>
<span id="cb219-6"><a href="#cb219-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb219-7"><a href="#cb219-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span>
<span id="cb219-8"><a href="#cb219-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check that scaling is applied (output should be larger than raw embedding)</span></span>
<span id="cb219-9"><a href="#cb219-9" aria-hidden="true" tabindex="-1"></a>    raw_embed <span class="op">=</span> embeddings.embedding(x)</span>
<span id="cb219-10"><a href="#cb219-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> torch.allclose(output, raw_embed <span class="op">*</span> math.sqrt(<span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="d53c6031" class="cell" data-time_run="2025-11-30T18:10:52.551004+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb220"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb220-1"><a href="#cb220-1" aria-hidden="true" tabindex="-1"></a>test_embeddings()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="generator-class" class="level2">
<h2 class="anchored" data-anchor-id="generator-class">Generator class</h2>
<p>Awesome. Now what? Stay honest to the original quote.</p>
<section id="reply-136" class="level5">
<h5 class="anchored" data-anchor-id="reply-136">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent progress! Looking at the original code structure, the next component is the <strong>Generator</strong>.</p>
<p><strong>What does the Generator do?</strong></p>
<p>It’s the final layer that converts the decoder’s output (d_model=512 dimensions) into vocabulary predictions. It does: 1. Linear projection: d_model → vocab_size 2. Log softmax to get log probabilities for each token</p>
<p>This is what actually predicts “what’s the next word?”</p>
<p>Try implementing it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb221"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb221-1"><a href="#cb221-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Generator(nn.Module):</span>
<span id="cb221-2"><a href="#cb221-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, vocab):</span>
<span id="cb221-3"><a href="#cb221-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb221-4"><a href="#cb221-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What layer do you need?</span></span>
<span id="cb221-5"><a href="#cb221-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb221-6"><a href="#cb221-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb221-7"><a href="#cb221-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What operation?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What do you need in <code>__init__</code> and <code>forward</code>?</p>
</section>
<section id="final-generator-function." class="level3">
<h3 class="anchored" data-anchor-id="final-generator-function.">Final generator function.</h3>
<hr>
</section>
<section id="generator" class="level3">
<h3 class="anchored" data-anchor-id="generator">Generator</h3>
<blockquote class="blockquote">
<pre><code> Generator (d_model, vocab)</code></pre>
</blockquote>
<p>*Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:<code>to</code>, etc.</p>
<p>.. note:: As per the example above, an <code>__init__()</code> call to the parent class must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*</p>
<p>see if tihs works?</p>
<section id="reply-137" class="level5">
<h5 class="anchored" data-anchor-id="reply-137">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>Generator</code> class looks correct and matches the original code! ✓</p>
<p>You’ve got: - ✓ Linear layer (d_model → vocab) - ✓ LogSoftmax on the last dimension</p>
<p>Let’s test it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb224"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb224-1"><a href="#cb224-1" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> Generator(d_model<span class="op">=</span><span class="dv">512</span>, vocab<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb224-2"><a href="#cb224-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># decoder output</span></span>
<span id="cb224-3"><a href="#cb224-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> generator(x)</span>
<span id="cb224-4"><a href="#cb224-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb224-5"><a href="#cb224-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">10000</span>))  <span class="co"># log probabilities for each token in vocab</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this to verify it works!</p>
<p>Now assume we are using NBDEV. No need to output the function but output the unit test.</p>
</section>
<section id="reply-138" class="level5">
<h5 class="anchored" data-anchor-id="reply-138">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb225"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb225-1"><a href="#cb225-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb225-2"><a href="#cb225-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_generator():</span>
<span id="cb225-3"><a href="#cb225-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Generator output projection"</span></span>
<span id="cb225-4"><a href="#cb225-4" aria-hidden="true" tabindex="-1"></a>    generator <span class="op">=</span> Generator(d_model<span class="op">=</span><span class="dv">512</span>, vocab<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb225-5"><a href="#cb225-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># decoder output</span></span>
<span id="cb225-6"><a href="#cb225-6" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> generator(x)</span>
<span id="cb225-7"><a href="#cb225-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb225-8"><a href="#cb225-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">10000</span>])</span>
<span id="cb225-9"><a href="#cb225-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check that output is log probabilities (should sum to ~1 after exp)</span></span>
<span id="cb225-10"><a href="#cb225-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> torch.allclose(output.exp().<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>), torch.ones(<span class="dv">2</span>, <span class="dv">10</span>), atol<span class="op">=</span><span class="fl">1e-5</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Excellent! You’re almost done! The final piece is the <strong>EncoderDecoder</strong> wrapper that ties everything together. Ready for that?</p>
</section>
<section id="unit-test-9" class="level4">
<h4 class="anchored" data-anchor-id="unit-test-9">Unit test</h4>
<div id="1f983578" class="cell" data-time_run="2025-11-30T18:21:47.344198+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb226"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb226-1"><a href="#cb226-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_generator():</span>
<span id="cb226-2"><a href="#cb226-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Generator output projection"</span></span>
<span id="cb226-3"><a href="#cb226-3" aria-hidden="true" tabindex="-1"></a>    generator <span class="op">=</span> Generator(d_model<span class="op">=</span><span class="dv">512</span>, vocab<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb226-4"><a href="#cb226-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>)  <span class="co"># decoder output</span></span>
<span id="cb226-5"><a href="#cb226-5" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> generator(x)</span>
<span id="cb226-6"><a href="#cb226-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb226-7"><a href="#cb226-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">10000</span>])</span>
<span id="cb226-8"><a href="#cb226-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check that output is log probabilities (should sum to ~1 after exp)</span></span>
<span id="cb226-9"><a href="#cb226-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> torch.allclose(output.exp().<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>), torch.ones(<span class="dv">2</span>, <span class="dv">10</span>), atol<span class="op">=</span><span class="fl">1e-5</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="97feb70c" class="cell" data-time_run="2025-11-30T18:22:07.005663+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb227"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb227-1"><a href="#cb227-1" aria-hidden="true" tabindex="-1"></a>test_generator()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Awesome. What’s next?</p>
<section id="reply-139" class="level5">
<h5 class="anchored" data-anchor-id="reply-139">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Amazing work! You’re at the final step! 🎉</p>
<p><strong>What’s left: EncoderDecoder</strong></p>
<p>This is the top-level class that ties everything together. Looking at the original code, it stores all the components and defines how data flows through the model.</p>
<p><strong>What does EncoderDecoder need?</strong></p>
<p>In <code>__init__</code>, it stores: - encoder - decoder<br>
- src_embed (source embeddings + positional encoding) - tgt_embed (target embeddings + positional encoding) - generator</p>
<p>In <code>forward</code>, it needs methods like: - <code>encode()</code> - process source through encoder - <code>decode()</code> - process target through decoder - <code>forward()</code> - full forward pass</p>
<p>Before coding, do you understand the data flow? Source goes through encoder, then decoder uses encoder output + target to generate predictions.</p>
<p>Want to try implementing the <code>__init__</code> first?</p>
<p>Yes, let’s try it.</p>
</section>
<section id="reply-140" class="level5">
<h5 class="anchored" data-anchor-id="reply-140">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let’s implement the <strong>EncoderDecoder</strong> class.</p>
<p>Start with the <code>__init__</code> method. Looking at the original code, you need to store 5 components:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb228"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb228-1"><a href="#cb228-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderDecoder(nn.Module):</span>
<span id="cb228-2"><a href="#cb228-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder, decoder, src_embed, tgt_embed, generator):</span>
<span id="cb228-3"><a href="#cb228-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb228-4"><a href="#cb228-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store all 5 components</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Try writing the <code>__init__</code> - just store each component as an attribute. What would you write?</p>
<div id="a7e72d14" class="cell" data-time_run="2025-11-30T18:25:42.918302+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb229"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb229-1"><a href="#cb229-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderDecoder(torch.nn.Module):</span>
<span id="cb229-2"><a href="#cb229-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder, decoder, src_embed, tgt_embed, generator):</span>
<span id="cb229-3"><a href="#cb229-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb229-4"><a href="#cb229-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store all 5 components</span></span>
<span id="cb229-5"><a href="#cb229-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb229-6"><a href="#cb229-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder</span>
<span id="cb229-7"><a href="#cb229-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src_embed <span class="op">=</span> src_embed</span>
<span id="cb229-8"><a href="#cb229-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tgt_embed <span class="op">=</span> tgt_embed</span>
<span id="cb229-9"><a href="#cb229-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generator <span class="op">=</span> generator</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is that good?</p>
</section>
<section id="reply-141" class="level5">
<h5 class="anchored" data-anchor-id="reply-141">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>__init__</code> is exactly right! ✓</p>
<p>Now let’s implement the methods. Looking at the original code, you need:</p>
<ol type="1">
<li><strong><code>encode(self, src, src_mask)</code></strong> - runs source through embedding and encoder</li>
<li><strong><code>decode(self, memory, src_mask, tgt, tgt_mask)</code></strong> - runs target through embedding and decoder</li>
<li><strong><code>forward(self, src, tgt, src_mask, tgt_mask)</code></strong> - combines encode and decode</li>
</ol>
<p>Try implementing the <code>encode</code> method first. What should it do?</p>
<p>Hint: Apply src_embed to src, then pass through encoder with the mask.</p>
<p>I’m sleepy, let’s do the rest tomorrow.</p>
</section>
<section id="reply-142" class="level5">
<h5 class="anchored" data-anchor-id="reply-142">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>No problem! You’ve made incredible progress today! 🎉</p>
<p><strong>What you’ve accomplished:</strong> - ✓ Attention mechanism - ✓ Multi-Head Attention - ✓ Position-wise Feed-Forward - ✓ Positional Encoding - ✓ Sublayer Connection - ✓ Encoder Layer &amp; full Encoder stack - ✓ Decoder Layer &amp; full Decoder stack - ✓ Embeddings - ✓ Generator</p>
<p><strong>What’s left (very little!):</strong> - Finish EncoderDecoder (encode, decode, forward methods) - Optional: make_model helper function to build the complete transformer</p>
<p>You’re about 95% done with implementing the Transformer from scratch! Get some rest, and we’ll finish it up tomorrow. Great work! 💪</p>
<p>I’m back again. Let’s get started. Stay honest to the original code.</p>
</section>
<section id="reply-143" class="level5">
<h5 class="anchored" data-anchor-id="reply-143">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠</p>
<p>Welcome back! 🎉</p>
<p>You were so close yesterday! You’ve already implemented the <code>EncoderDecoder.__init__</code>. Now you just need three methods:</p>
<ol type="1">
<li><strong><code>encode(self, src, src_mask)</code></strong> - embed source, pass through encoder</li>
<li><strong><code>decode(self, memory, src_mask, tgt, tgt_mask)</code></strong> - embed target, pass through decoder</li>
<li><strong><code>forward(self, src, tgt, src_mask, tgt_mask)</code></strong> - call encode, then decode</li>
</ol>
<p>Let’s start with <code>encode</code>. It should: - Apply <code>self.src_embed</code> to <code>src</code> - Pass the result through <code>self.encoder</code> with the mask</p>
<p>Try writing it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb230"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb230-1"><a href="#cb230-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode(<span class="va">self</span>, src, src_mask):</span>
<span id="cb230-2"><a href="#cb230-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># What goes here?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div id="740bc972" class="cell" data-time_run="2025-12-01T07:03:22.137065+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb231"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb231-1"><a href="#cb231-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderDecoder(torch.nn.Module):</span>
<span id="cb231-2"><a href="#cb231-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder, decoder, src_embed, tgt_embed, generator):</span>
<span id="cb231-3"><a href="#cb231-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb231-4"><a href="#cb231-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store all 5 components</span></span>
<span id="cb231-5"><a href="#cb231-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb231-6"><a href="#cb231-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder</span>
<span id="cb231-7"><a href="#cb231-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src_embed <span class="op">=</span> src_embed</span>
<span id="cb231-8"><a href="#cb231-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tgt_embed <span class="op">=</span> tgt_embed</span>
<span id="cb231-9"><a href="#cb231-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generator <span class="op">=</span> generator</span>
<span id="cb231-10"><a href="#cb231-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, src, src_mask):</span>
<span id="cb231-11"><a href="#cb231-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.src_embed(src)</span>
<span id="cb231-12"><a href="#cb231-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.encoder(x, src_mask)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Does this make sense?</p>
</section>
<section id="reply-144" class="level5">
<h5 class="anchored" data-anchor-id="reply-144">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>encode</code> method is exactly right! ✓</p>
<p>You’ve got: - ✓ Apply source embeddings - ✓ Pass through encoder with mask - ✓ Return the encoder output (memory)</p>
<p>Now try the <code>decode</code> method. It should: - Apply <code>self.tgt_embed</code> to <code>tgt</code> - Pass the result through <code>self.decoder</code> along with <code>memory</code>, <code>src_mask</code>, and <code>tgt_mask</code></p>
<p>Give it a try:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb232"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb232-1"><a href="#cb232-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> decode(<span class="va">self</span>, memory, src_mask, tgt, tgt_mask):</span>
<span id="cb232-2"><a href="#cb232-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># What goes here?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div id="c23d1b4a" class="cell" data-time_run="2025-12-01T07:05:38.527223+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb233"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb233-1"><a href="#cb233-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderDecoder(torch.nn.Module):</span>
<span id="cb233-2"><a href="#cb233-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder, decoder, src_embed, tgt_embed, generator):</span>
<span id="cb233-3"><a href="#cb233-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb233-4"><a href="#cb233-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store all 5 components</span></span>
<span id="cb233-5"><a href="#cb233-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb233-6"><a href="#cb233-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder</span>
<span id="cb233-7"><a href="#cb233-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src_embed <span class="op">=</span> src_embed</span>
<span id="cb233-8"><a href="#cb233-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tgt_embed <span class="op">=</span> tgt_embed</span>
<span id="cb233-9"><a href="#cb233-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generator <span class="op">=</span> generator</span>
<span id="cb233-10"><a href="#cb233-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, src, src_mask):</span>
<span id="cb233-11"><a href="#cb233-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.src_embed(src)</span>
<span id="cb233-12"><a href="#cb233-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.encoder(x, src_mask)</span>
<span id="cb233-13"><a href="#cb233-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, memory, src_mask, tgt, tgt_mask):</span>
<span id="cb233-14"><a href="#cb233-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What goes here?</span></span>
<span id="cb233-15"><a href="#cb233-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.tgt_embed(tgt)</span>
<span id="cb233-16"><a href="#cb233-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.decoder(x, memory, src_mask, tgt_mask)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this cool? Stay loyal to the original code.</p>
</section>
<section id="reply-145" class="level5">
<h5 class="anchored" data-anchor-id="reply-145">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>decode</code> method is exactly right and matches the original code! ✓</p>
<p>You’ve got: - ✓ Apply target embeddings - ✓ Pass through decoder with all the right arguments (x, memory, src_mask, tgt_mask)</p>
<p>Now the final method: <code>forward</code>. It should: - Call <code>encode</code> to get memory - Call <code>decode</code> with the memory - Return the decoder output</p>
<p>Try writing it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb234"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb234-1"><a href="#cb234-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, src, tgt, src_mask, tgt_mask):</span>
<span id="cb234-2"><a href="#cb234-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># What goes here?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
</section>
<section id="final-encoder-decoder-function." class="level3">
<h3 class="anchored" data-anchor-id="final-encoder-decoder-function.">Final Encoder Decoder Function.</h3>
<hr>
</section>
<section id="encoderdecoder" class="level3">
<h3 class="anchored" data-anchor-id="encoderdecoder">EncoderDecoder</h3>
<blockquote class="blockquote">
<pre><code> EncoderDecoder (encoder, decoder, src_embed, tgt_embed, generator)</code></pre>
</blockquote>
<p>*Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:<code>to</code>, etc.</p>
<p>.. note:: As per the example above, an <code>__init__()</code> call to the parent class must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*</p>
<p>done baby</p>
<section id="reply-146" class="level5">
<h5 class="anchored" data-anchor-id="reply-146">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent! Your <code>EncoderDecoder</code> class is complete and correct! 🎉</p>
<p>You’ve got: - ✓ <code>encode</code> - processes source - ✓ <code>decode</code> - processes target with encoder memory - ✓ <code>forward</code> - ties them together</p>
<p>This is the complete Transformer architecture! Let’s test it to make sure everything works together:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb237"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb237-1"><a href="#cb237-1" aria-hidden="true" tabindex="-1"></a><span class="co"># You'll need to create all the components and test</span></span>
<span id="cb237-2"><a href="#cb237-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Want me to help you write a test, or do you want to try?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>But wait - looking at the original code, there’s one more super useful thing: the <strong><code>make_model</code></strong> helper function that builds a complete transformer with all the right components.</p>
<p>Do you want to: 1. Test the EncoderDecoder as-is 2. Implement the <code>make_model</code> helper function (makes it easy to create a full transformer)</p>
<p>What would you prefer?</p>
<p>Before moving on, let’s make a unit test for this function, like we usually do, assuming we are in NB dev and the output function is already exported.</p>
</section>
<section id="reply-147" class="level5">
<h5 class="anchored" data-anchor-id="reply-147">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb238"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb238-1"><a href="#cb238-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb238-2"><a href="#cb238-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_encoder_decoder():</span>
<span id="cb238-3"><a href="#cb238-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test complete EncoderDecoder model"</span></span>
<span id="cb238-4"><a href="#cb238-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create all components</span></span>
<span id="cb238-5"><a href="#cb238-5" aria-hidden="true" tabindex="-1"></a>    attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb238-6"><a href="#cb238-6" aria-hidden="true" tabindex="-1"></a>    ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb238-7"><a href="#cb238-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb238-8"><a href="#cb238-8" aria-hidden="true" tabindex="-1"></a>    encoder_layer <span class="op">=</span> EncoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>copy.deepcopy(attn), </span>
<span id="cb238-9"><a href="#cb238-9" aria-hidden="true" tabindex="-1"></a>                                  feed_forward<span class="op">=</span>copy.deepcopy(ff), dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb238-10"><a href="#cb238-10" aria-hidden="true" tabindex="-1"></a>    decoder_layer <span class="op">=</span> DecoderLayer(size<span class="op">=</span><span class="dv">512</span>, </span>
<span id="cb238-11"><a href="#cb238-11" aria-hidden="true" tabindex="-1"></a>                                  self_attn<span class="op">=</span>copy.deepcopy(attn),</span>
<span id="cb238-12"><a href="#cb238-12" aria-hidden="true" tabindex="-1"></a>                                  src_attn<span class="op">=</span>copy.deepcopy(attn),</span>
<span id="cb238-13"><a href="#cb238-13" aria-hidden="true" tabindex="-1"></a>                                  feed_forward<span class="op">=</span>copy.deepcopy(ff), dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb238-14"><a href="#cb238-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb238-15"><a href="#cb238-15" aria-hidden="true" tabindex="-1"></a>    encoder <span class="op">=</span> Encoder(encoder_layer, N<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb238-16"><a href="#cb238-16" aria-hidden="true" tabindex="-1"></a>    decoder <span class="op">=</span> Decoder(decoder_layer, N<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb238-17"><a href="#cb238-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb238-18"><a href="#cb238-18" aria-hidden="true" tabindex="-1"></a>    src_embed <span class="op">=</span> torch.nn.Sequential(Embeddings(<span class="dv">512</span>, <span class="dv">1000</span>), PositionalEncoding(<span class="dv">512</span>, <span class="fl">0.1</span>))</span>
<span id="cb238-19"><a href="#cb238-19" aria-hidden="true" tabindex="-1"></a>    tgt_embed <span class="op">=</span> torch.nn.Sequential(Embeddings(<span class="dv">512</span>, <span class="dv">1000</span>), PositionalEncoding(<span class="dv">512</span>, <span class="fl">0.1</span>))</span>
<span id="cb238-20"><a href="#cb238-20" aria-hidden="true" tabindex="-1"></a>    generator <span class="op">=</span> Generator(<span class="dv">512</span>, <span class="dv">1000</span>)</span>
<span id="cb238-21"><a href="#cb238-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb238-22"><a href="#cb238-22" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)</span>
<span id="cb238-23"><a href="#cb238-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb238-24"><a href="#cb238-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test</span></span>
<span id="cb238-25"><a href="#cb238-25" aria-hidden="true" tabindex="-1"></a>    src <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">2</span>, <span class="dv">10</span>))</span>
<span id="cb238-26"><a href="#cb238-26" aria-hidden="true" tabindex="-1"></a>    tgt <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">2</span>, <span class="dv">8</span>))</span>
<span id="cb238-27"><a href="#cb238-27" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(src, tgt, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb238-28"><a href="#cb238-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb238-29"><a href="#cb238-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Ready to move on to <code>make_model</code>?</p>
</section>
<section id="unit-test-10" class="level4">
<h4 class="anchored" data-anchor-id="unit-test-10">Unit test</h4>
<div id="77eddd49" class="cell" data-time_run="2025-12-01T07:14:46.884587+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb239"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb239-1"><a href="#cb239-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_encoder_decoder():</span>
<span id="cb239-2"><a href="#cb239-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test complete EncoderDecoder model"</span></span>
<span id="cb239-3"><a href="#cb239-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create all components</span></span>
<span id="cb239-4"><a href="#cb239-4" aria-hidden="true" tabindex="-1"></a>    attn <span class="op">=</span> MultiHeadedAttention(h<span class="op">=</span><span class="dv">8</span>, d_model<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb239-5"><a href="#cb239-5" aria-hidden="true" tabindex="-1"></a>    ff <span class="op">=</span> PositionwiseFeedForward(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb239-6"><a href="#cb239-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb239-7"><a href="#cb239-7" aria-hidden="true" tabindex="-1"></a>    encoder_layer <span class="op">=</span> EncoderLayer(size<span class="op">=</span><span class="dv">512</span>, self_attn<span class="op">=</span>copy.deepcopy(attn), </span>
<span id="cb239-8"><a href="#cb239-8" aria-hidden="true" tabindex="-1"></a>                                  feed_forward<span class="op">=</span>copy.deepcopy(ff), dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb239-9"><a href="#cb239-9" aria-hidden="true" tabindex="-1"></a>    decoder_layer <span class="op">=</span> DecoderLayer(size<span class="op">=</span><span class="dv">512</span>, </span>
<span id="cb239-10"><a href="#cb239-10" aria-hidden="true" tabindex="-1"></a>                                  self_attn<span class="op">=</span>copy.deepcopy(attn),</span>
<span id="cb239-11"><a href="#cb239-11" aria-hidden="true" tabindex="-1"></a>                                  src_attn<span class="op">=</span>copy.deepcopy(attn),</span>
<span id="cb239-12"><a href="#cb239-12" aria-hidden="true" tabindex="-1"></a>                                  feed_forward<span class="op">=</span>copy.deepcopy(ff), dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb239-13"><a href="#cb239-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb239-14"><a href="#cb239-14" aria-hidden="true" tabindex="-1"></a>    encoder <span class="op">=</span> Encoder(encoder_layer, N<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb239-15"><a href="#cb239-15" aria-hidden="true" tabindex="-1"></a>    decoder <span class="op">=</span> Decoder(decoder_layer, N<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb239-16"><a href="#cb239-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb239-17"><a href="#cb239-17" aria-hidden="true" tabindex="-1"></a>    src_embed <span class="op">=</span> torch.nn.Sequential(Embeddings(<span class="dv">512</span>, <span class="dv">1000</span>), PositionalEncoding(<span class="dv">512</span>, <span class="fl">0.1</span>))</span>
<span id="cb239-18"><a href="#cb239-18" aria-hidden="true" tabindex="-1"></a>    tgt_embed <span class="op">=</span> torch.nn.Sequential(Embeddings(<span class="dv">512</span>, <span class="dv">1000</span>), PositionalEncoding(<span class="dv">512</span>, <span class="fl">0.1</span>))</span>
<span id="cb239-19"><a href="#cb239-19" aria-hidden="true" tabindex="-1"></a>    generator <span class="op">=</span> Generator(<span class="dv">512</span>, <span class="dv">1000</span>)</span>
<span id="cb239-20"><a href="#cb239-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb239-21"><a href="#cb239-21" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)</span>
<span id="cb239-22"><a href="#cb239-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb239-23"><a href="#cb239-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test</span></span>
<span id="cb239-24"><a href="#cb239-24" aria-hidden="true" tabindex="-1"></a>    src <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">2</span>, <span class="dv">10</span>))</span>
<span id="cb239-25"><a href="#cb239-25" aria-hidden="true" tabindex="-1"></a>    tgt <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">2</span>, <span class="dv">8</span>))</span>
<span id="cb239-26"><a href="#cb239-26" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(src, tgt, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb239-27"><a href="#cb239-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb239-28"><a href="#cb239-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="67b1bd63" class="cell" data-time_run="2025-12-01T07:15:12.915895+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb240"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb240-1"><a href="#cb240-1" aria-hidden="true" tabindex="-1"></a>test_encoder_decoder()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
</section>
<section id="make-model" class="level2">
<h2 class="anchored" data-anchor-id="make-model">Make Model</h2>
<p>Okay, let’s go further.</p>
<section id="reply-148" class="level5">
<h5 class="anchored" data-anchor-id="reply-148">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Awesome! 🎉 Your complete Transformer is working!</p>
<p><strong>What’s next: <code>make_model</code> helper function</strong></p>
<p>Looking at the original code, <code>make_model</code> is a convenience function that builds a complete transformer from just hyperparameters. Instead of manually creating all components like in your test, you can just call:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb241"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb241-1"><a href="#cb241-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> make_model(src_vocab<span class="op">=</span><span class="dv">11</span>, tgt_vocab<span class="op">=</span><span class="dv">11</span>, N<span class="op">=</span><span class="dv">6</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>And it creates everything for you!</p>
<p><strong>What does <code>make_model</code> need to do?</strong></p>
<p>Looking at the original code, it takes these parameters: - <code>src_vocab</code>, <code>tgt_vocab</code> - vocabulary sizes - <code>N</code> - number of layers (default 6) - <code>d_model</code> - model dimension (default 512) - <code>d_ff</code> - feed-forward dimension (default 2048) - <code>h</code> - number of attention heads (default 8) - <code>dropout</code> - dropout rate (default 0.1)</p>
<p>Then it creates all the components and returns an <code>EncoderDecoder</code> model.</p>
<p>Want to try implementing it, or should I break it down into smaller steps?</p>
<div id="e95cf0cf" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb242"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb242-1"><a href="#cb242-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> make_model(src_vocab<span class="op">=</span><span class="dv">11</span>, tgt_vocab<span class="op">=</span><span class="dv">11</span>, N<span class="op">=</span><span class="dv">6</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>I’m guessing this function just creates an encoder-decoder layer, Along with the inputs which are needed for the encoder-decoder. Am I correct?</p>
</section>
<section id="reply-149" class="level5">
<h5 class="anchored" data-anchor-id="reply-149">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly right! 💯</p>
<p><code>make_model</code> is just a convenience factory function that: 1. Creates all the components (attention modules, feed-forward, embeddings, etc.) 2. Assembles them into an <code>EncoderDecoder</code> model 3. Initializes the parameters properly</p>
<p>It saves you from writing all that boilerplate code every time you want to create a transformer.</p>
<p>Looking at the original code, it also does one more important thing at the end: <strong>Xavier initialization</strong> of parameters for better training.</p>
<p>Want to try implementing it? Start with the function signature:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb243"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb243-1"><a href="#cb243-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_model(src_vocab, tgt_vocab, N<span class="op">=</span><span class="dv">6</span>, d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, h<span class="op">=</span><span class="dv">8</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb243-2"><a href="#cb243-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create components</span></span>
<span id="cb243-3"><a href="#cb243-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Assemble into EncoderDecoder</span></span>
<span id="cb243-4"><a href="#cb243-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize parameters</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What would you create first inside this function?</p>
<p>This is actually a good exercise that brings all the components we built together. We need to know which order to build them first. All the way from low-level to high-level.</p>
</section>
<section id="reply-150" class="level5">
<h5 class="anchored" data-anchor-id="reply-150">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent insight! You’re absolutely right - this is a great exercise in understanding the dependency hierarchy.</p>
<p>Let’s think through the order together. What components have <strong>no dependencies</strong> (the lowest level building blocks)?</p>
<p>Start from the bottom - which components can be created first without needing anything else?</p>
<p>We start from Attention, then multi-header attention, then position-wise feed forward, then subconnection layer, then encoder layer, then decoder layer, then encoder, then decoder, then encoder-decoder both, and in between also we need clones.</p>
</section>
<section id="reply-151" class="level5">
<h5 class="anchored" data-anchor-id="reply-151">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great breakdown! You’ve got the dependency order mostly right! Let me refine it slightly:</p>
<p><strong>Level 1 (no dependencies):</strong> - <code>attention</code> function - <code>clones</code> function</p>
<p><strong>Level 2 (uses Level 1):</strong> - <code>MultiHeadedAttention</code> (uses attention) - <code>PositionwiseFeedForward</code> - <code>PositionalEncoding</code> - <code>Embeddings</code></p>
<p><strong>Level 3 (uses Level 2):</strong> - <code>SublayerConnection</code></p>
<p><strong>Level 4 (uses Level 2 &amp; 3):</strong> - <code>EncoderLayer</code> (uses MultiHeadedAttention, PositionwiseFeedForward, SublayerConnection) - <code>DecoderLayer</code> (uses MultiHeadedAttention x2, PositionwiseFeedForward, SublayerConnection)</p>
<p><strong>Level 5 (uses Level 4):</strong> - <code>Encoder</code> (uses EncoderLayer, clones) - <code>Decoder</code> (uses DecoderLayer, clones) - <code>Generator</code></p>
<p><strong>Level 6 (final assembly):</strong> - <code>EncoderDecoder</code> (uses Encoder, Decoder, embeddings+positional, Generator)</p>
<p>So in <code>make_model</code>, what should you create first?</p>
<div id="edada97a" class="cell" data-time_run="2025-12-01T07:49:09.224929+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb244"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb244-1"><a href="#cb244-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_model(src_vocab, tgt_vocab, N<span class="op">=</span><span class="dv">6</span>, d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, h<span class="op">=</span><span class="dv">8</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb244-2"><a href="#cb244-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create components</span></span>
<span id="cb244-3"><a href="#cb244-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Level 1</span></span>
<span id="cb244-4"><a href="#cb244-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Attention and Clones functions already created. As they are not classes, no need to create instances.</span></span>
<span id="cb244-5"><a href="#cb244-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Level 2</span></span>
<span id="cb244-6"><a href="#cb244-6" aria-hidden="true" tabindex="-1"></a>    multi_head_attention <span class="op">=</span> MultiHeadedAttention(h, d_model, dropout)</span>
<span id="cb244-7"><a href="#cb244-7" aria-hidden="true" tabindex="-1"></a>    feedforward_layer <span class="op">=</span> PositionwiseFeedForward(d_model, d_ff, dropout)</span>
<span id="cb244-8"><a href="#cb244-8" aria-hidden="true" tabindex="-1"></a>    positional_encoding <span class="op">=</span> PositionalEncoding(d_model, dropout)</span>
<span id="cb244-9"><a href="#cb244-9" aria-hidden="true" tabindex="-1"></a>    src_embedding_layer <span class="op">=</span> Embeddings(d_model, src_vocab)</span>
<span id="cb244-10"><a href="#cb244-10" aria-hidden="true" tabindex="-1"></a>    tgt_embeddings_layer <span class="op">=</span> Embeddings(d_model, tgt_vocab)</span>
<span id="cb244-11"><a href="#cb244-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Level 3</span></span>
<span id="cb244-12"><a href="#cb244-12" aria-hidden="true" tabindex="-1"></a>    sub_layer_connection <span class="op">=</span> SublayerConnection(d_model, dropout)</span>
<span id="cb244-13"><a href="#cb244-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Level 4</span></span>
<span id="cb244-14"><a href="#cb244-14" aria-hidden="true" tabindex="-1"></a>    encoder_layer <span class="op">=</span> EncoderLayer(d_model, multi_head_attention, feedforward_layer, dropout)</span>
<span id="cb244-15"><a href="#cb244-15" aria-hidden="true" tabindex="-1"></a>    decoder_layer <span class="op">=</span> DecoderLayer(d_model, multi_head_attention, multi_head_attention, feedforward_layer, dropout)</span>
<span id="cb244-16"><a href="#cb244-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Assemble into EncoderDecoder</span></span>
<span id="cb244-17"><a href="#cb244-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize parameters</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>So far so good. I’m having a feeling I’m screwing it up because I should create different instances of attention and feed-forward layer for different things, or will the clone take care of it?</p>
</section>
<section id="reply-152" class="level5">
<h5 class="anchored" data-anchor-id="reply-152">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠</p>
<p>Great instinct! You’re absolutely right to be concerned!</p>
<p><strong>The issue:</strong> If you pass the same <code>multi_head_attention</code> instance to both encoder and decoder (and to both self_attn and src_attn in decoder), they’ll all share the same weights. That’s not what we want!</p>
<p><strong>Looking at the original code</strong>, they solve this with <a href="https://docs.python.org/3/library/copy.html#copy.deepcopy"><code>copy.deepcopy</code></a>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb245"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb245-1"><a href="#cb245-1" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> copy.deepcopy</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then when creating layers, they use <code>c(attn)</code>, <code>c(ff)</code> to make copies.</p>
<p><strong>Also:</strong> You don’t need to create <code>SublayerConnection</code> manually - remember it’s already created inside <code>EncoderLayer</code> and <code>DecoderLayer</code>!</p>
<p><strong>Key places needing separate copies:</strong> - EncoderLayer needs: 1 attention, 1 feedforward - DecoderLayer needs: 2 attentions (self + src), 1 feedforward</p>
<p>Can you update your code using <a href="https://docs.python.org/3/library/copy.html#copy.deepcopy"><code>copy.deepcopy</code></a> where needed?</p>
<hr>
</section>
<section id="make_model" class="level3">
<h3 class="anchored" data-anchor-id="make_model">make_model</h3>
<blockquote class="blockquote">
<pre><code> make_model (src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8,
             dropout=0.1)</code></pre>
</blockquote>
<p>Am I on the right path?</p>
<section id="reply-153" class="level5">
<h5 class="anchored" data-anchor-id="reply-153">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You’re very close! A few small issues to fix:</p>
<ol type="1">
<li><p><strong><code>torch.nn.Sequential</code> doesn’t take a list</strong> - it takes the modules directly as arguments:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb247"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb247-1"><a href="#cb247-1" aria-hidden="true" tabindex="-1"></a>src_embed <span class="op">=</span> torch.nn.Sequential(src_embedding_layer, positional_encoding[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong><code>vocab</code> is undefined</strong> in Generator - should be <code>tgt_vocab</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb248"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb248-1"><a href="#cb248-1" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> Generator(d_model, tgt_vocab)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>You don’t need lists</strong> for the positional encodings - they’re already separate instances when you create them:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb249"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb249-1"><a href="#cb249-1" aria-hidden="true" tabindex="-1"></a>pos_enc_1 <span class="op">=</span> PositionalEncoding(d_model, dropout)</span>
<span id="cb249-2"><a href="#cb249-2" aria-hidden="true" tabindex="-1"></a>pos_enc_2 <span class="op">=</span> PositionalEncoding(d_model, dropout)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>After creating the model</strong>, you need to add Xavier initialization (looking at the original code):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb250"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb250-1"><a href="#cb250-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> model.parameters():</span>
<span id="cb250-2"><a href="#cb250-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p.dim() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb250-3"><a href="#cb250-3" aria-hidden="true" tabindex="-1"></a>        nn.init.xavier_uniform_(p)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ol>
<p>Can you fix these issues?</p>
</section>
</section>
<section id="final-make-model-function" class="level3">
<h3 class="anchored" data-anchor-id="final-make-model-function">Final Make Model Function</h3>
<hr>
</section>
<section id="make_model-1" class="level3">
<h3 class="anchored" data-anchor-id="make_model-1">make_model</h3>
<blockquote class="blockquote">
<pre><code> make_model (src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8,
             dropout=0.1)</code></pre>
</blockquote>
<p>Is this good?</p>
<section id="reply-154" class="level5">
<h5 class="anchored" data-anchor-id="reply-154">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>make_model</code> function is now correct and matches the original code! ✓</p>
<p>You’ve got: - ✓ Creating separate copies of attention and feedforward modules - ✓ Embeddings + positional encoding wrapped in Sequential - ✓ Building encoder and decoder layers - ✓ Assembling the complete model - ✓ Xavier initialization for better training</p>
<p>Let’s test it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb252"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb252-1"><a href="#cb252-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> make_model(src_vocab<span class="op">=</span><span class="dv">1000</span>, tgt_vocab<span class="op">=</span><span class="dv">1000</span>, N<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb252-2"><a href="#cb252-2" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">2</span>, <span class="dv">10</span>))</span>
<span id="cb252-3"><a href="#cb252-3" aria-hidden="true" tabindex="-1"></a>tgt <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">2</span>, <span class="dv">8</span>))</span>
<span id="cb252-4"><a href="#cb252-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(src, tgt, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb252-5"><a href="#cb252-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb252-6"><a href="#cb252-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this to verify your complete Transformer works!</p>
<div id="a0ca0fb6" class="cell" data-time_run="2025-12-01T08:32:02.849531+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb253"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb253-1"><a href="#cb253-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> make_model(src_vocab<span class="op">=</span><span class="dv">1000</span>, tgt_vocab<span class="op">=</span><span class="dv">1000</span>, N<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb253-2"><a href="#cb253-2" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">2</span>, <span class="dv">10</span>))</span>
<span id="cb253-3"><a href="#cb253-3" aria-hidden="true" tabindex="-1"></a>tgt <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">2</span>, <span class="dv">8</span>))</span>
<span id="cb253-4"><a href="#cb253-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(src, tgt, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb253-5"><a href="#cb253-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span>
<span id="cb253-6"><a href="#cb253-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected:"</span>, (<span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">512</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Output shape: torch.Size([2, 8, 512])
Expected: (2, 8, 512)</code></pre>
</div>
</div>
<p>Bravo, assume you are using NBDEV, and the final function is already exported. Make an end-to-end test based on this.</p>
</section>
<section id="reply-155" class="level5">
<h5 class="anchored" data-anchor-id="reply-155">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb255"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb255-1"><a href="#cb255-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb255-2"><a href="#cb255-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_make_model_end_to_end():</span>
<span id="cb255-3"><a href="#cb255-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test complete Transformer with make_model"</span></span>
<span id="cb255-4"><a href="#cb255-4" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> make_model(src_vocab<span class="op">=</span><span class="dv">1000</span>, tgt_vocab<span class="op">=</span><span class="dv">1000</span>, N<span class="op">=</span><span class="dv">2</span>, d_model<span class="op">=</span><span class="dv">512</span>, </span>
<span id="cb255-5"><a href="#cb255-5" aria-hidden="true" tabindex="-1"></a>                       d_ff<span class="op">=</span><span class="dv">2048</span>, h<span class="op">=</span><span class="dv">8</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb255-6"><a href="#cb255-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb255-7"><a href="#cb255-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test forward pass</span></span>
<span id="cb255-8"><a href="#cb255-8" aria-hidden="true" tabindex="-1"></a>    src <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">2</span>, <span class="dv">10</span>))</span>
<span id="cb255-9"><a href="#cb255-9" aria-hidden="true" tabindex="-1"></a>    tgt <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">2</span>, <span class="dv">8</span>))</span>
<span id="cb255-10"><a href="#cb255-10" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(src, tgt, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb255-11"><a href="#cb255-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb255-12"><a href="#cb255-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">512</span>])</span>
<span id="cb255-13"><a href="#cb255-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb255-14"><a href="#cb255-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test that parameters were initialized</span></span>
<span id="cb255-15"><a href="#cb255-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> model.parameters():</span>
<span id="cb255-16"><a href="#cb255-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> p.dim() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb255-17"><a href="#cb255-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Xavier init should give reasonable values (not all zeros)</span></span>
<span id="cb255-18"><a href="#cb255-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> <span class="kw">not</span> torch.allclose(p, torch.zeros_like(p))</span>
<span id="cb255-19"><a href="#cb255-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb255-20"><a href="#cb255-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test encode and decode separately</span></span>
<span id="cb255-21"><a href="#cb255-21" aria-hidden="true" tabindex="-1"></a>    memory <span class="op">=</span> model.encode(src, src_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb255-22"><a href="#cb255-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> memory.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span>
<span id="cb255-23"><a href="#cb255-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb255-24"><a href="#cb255-24" aria-hidden="true" tabindex="-1"></a>    decoded <span class="op">=</span> model.decode(memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt<span class="op">=</span>tgt, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb255-25"><a href="#cb255-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> decoded.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>🎉 <strong>Congratulations! You’ve successfully implemented a complete Transformer from scratch!</strong> This is a huge accomplishment! What would you like to do next?</p>
</section>
<section id="end-to-end-test" class="level4">
<h4 class="anchored" data-anchor-id="end-to-end-test">End-to-end test</h4>
<div id="faf8fa9a" class="cell" data-time_run="2025-12-01T08:32:51.968285+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb256"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb256-1"><a href="#cb256-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_make_model_end_to_end():</span>
<span id="cb256-2"><a href="#cb256-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test complete Transformer with make_model"</span></span>
<span id="cb256-3"><a href="#cb256-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> make_model(src_vocab<span class="op">=</span><span class="dv">1000</span>, tgt_vocab<span class="op">=</span><span class="dv">1000</span>, N<span class="op">=</span><span class="dv">2</span>, d_model<span class="op">=</span><span class="dv">512</span>, </span>
<span id="cb256-4"><a href="#cb256-4" aria-hidden="true" tabindex="-1"></a>                       d_ff<span class="op">=</span><span class="dv">2048</span>, h<span class="op">=</span><span class="dv">8</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb256-5"><a href="#cb256-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb256-6"><a href="#cb256-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test forward pass</span></span>
<span id="cb256-7"><a href="#cb256-7" aria-hidden="true" tabindex="-1"></a>    src <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">2</span>, <span class="dv">10</span>))</span>
<span id="cb256-8"><a href="#cb256-8" aria-hidden="true" tabindex="-1"></a>    tgt <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">2</span>, <span class="dv">8</span>))</span>
<span id="cb256-9"><a href="#cb256-9" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(src, tgt, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb256-10"><a href="#cb256-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb256-11"><a href="#cb256-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> output.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">512</span>])</span>
<span id="cb256-12"><a href="#cb256-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb256-13"><a href="#cb256-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test that parameters were initialized</span></span>
<span id="cb256-14"><a href="#cb256-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> model.parameters():</span>
<span id="cb256-15"><a href="#cb256-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> p.dim() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb256-16"><a href="#cb256-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Xavier init should give reasonable values (not all zeros)</span></span>
<span id="cb256-17"><a href="#cb256-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> <span class="kw">not</span> torch.allclose(p, torch.zeros_like(p))</span>
<span id="cb256-18"><a href="#cb256-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb256-19"><a href="#cb256-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test encode and decode separately</span></span>
<span id="cb256-20"><a href="#cb256-20" aria-hidden="true" tabindex="-1"></a>    memory <span class="op">=</span> model.encode(src, src_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb256-21"><a href="#cb256-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> memory.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">512</span>])</span>
<span id="cb256-22"><a href="#cb256-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb256-23"><a href="#cb256-23" aria-hidden="true" tabindex="-1"></a>    decoded <span class="op">=</span> model.decode(memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt<span class="op">=</span>tgt, tgt_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb256-24"><a href="#cb256-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> decoded.shape <span class="op">==</span> torch.Size([<span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">512</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
</section>
<section id="module-hierarchy" class="level2">
<h2 class="anchored" data-anchor-id="module-hierarchy">Module Hierarchy</h2>
<div id="b92afcac" class="cell" data-time_run="2025-12-01T11:10:12.584707+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb257"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb257-1"><a href="#cb257-1" aria-hidden="true" tabindex="-1"></a><span class="at">@register_cell_magic</span></span>
<span id="cb257-2"><a href="#cb257-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mm(line, cell):</span>
<span id="cb257-3"><a href="#cb257-3" aria-hidden="true" tabindex="-1"></a>    escaped_cell <span class="op">=</span> html.escape(cell)</span>
<span id="cb257-4"><a href="#cb257-4" aria-hidden="true" tabindex="-1"></a>    display(HTML(<span class="ss">f"""</span></span>
<span id="cb257-5"><a href="#cb257-5" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;div class="mermaid"&gt;</span></span>
<span id="cb257-6"><a href="#cb257-6" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>escaped_cell<span class="sc">}</span></span>
<span id="cb257-7"><a href="#cb257-7" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;/div&gt;</span></span>
<span id="cb257-8"><a href="#cb257-8" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;script type="module"&gt;</span></span>
<span id="cb257-9"><a href="#cb257-9" aria-hidden="true" tabindex="-1"></a><span class="ss">import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.1/+esm';</span></span>
<span id="cb257-10"><a href="#cb257-10" aria-hidden="true" tabindex="-1"></a><span class="ss">mermaid.initialize(</span><span class="ch">{{</span><span class="ss"> startOnLoad: true </span><span class="ch">}}</span><span class="ss">);</span></span>
<span id="cb257-11"><a href="#cb257-11" aria-hidden="true" tabindex="-1"></a><span class="ss">mermaid.run();</span></span>
<span id="cb257-12"><a href="#cb257-12" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;/script&gt;</span></span>
<span id="cb257-13"><a href="#cb257-13" aria-hidden="true" tabindex="-1"></a><span class="ss">"""</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="689a462f" class="cell" data-time_run="2025-12-01T09:51:57.124867+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb258"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb258-1"><a href="#cb258-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Wrap to describe it</span></span>
<span id="cb258-2"><a href="#cb258-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tool_mermaid(graph:<span class="bu">str</span> <span class="co"># string representing the chart to be displayed</span></span>
<span id="cb258-3"><a href="#cb258-3" aria-hidden="true" tabindex="-1"></a>    )<span class="op">-&gt;</span><span class="bu">str</span>:</span>
<span id="cb258-4"><a href="#cb258-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Returns two blocks:</span></span>
<span id="cb258-5"><a href="#cb258-5" aria-hidden="true" tabindex="-1"></a><span class="co">    - html_graph: this one need to be added where you want to dispaly the chart</span></span>
<span id="cb258-6"><a href="#cb258-6" aria-hidden="true" tabindex="-1"></a><span class="co">    - html_run: this one need to be added at the end of the message and only "once" even if you generate multiple charts</span></span>
<span id="cb258-7"><a href="#cb258-7" aria-hidden="true" tabindex="-1"></a><span class="co">    The result should be returned inline with the response without any markdown codeblock.</span></span>
<span id="cb258-8"><a href="#cb258-8" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span> </span>
<span id="cb258-9"><a href="#cb258-9" aria-hidden="true" tabindex="-1"></a>    html_graph,html_run <span class="op">=</span> mm_html(graph)</span>
<span id="cb258-10"><a href="#cb258-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>(html_graph<span class="op">=</span>html_graph,html_run<span class="op">=</span>html_run)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>I like that you created different levels for the abstractions. Can we create a mermaid diagram to show this?</p>
<section id="reply-156" class="level5">
<h5 class="anchored" data-anchor-id="reply-156">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Absolutely! A mermaid diagram would be perfect for visualizing the module hierarchy!</p>
<p>Let me create a diagram showing the dependency levels:</p>
<pre class="mermaid"><code>graph TD
    subgraph "Level 1: Core Functions"
        A[attention function]
        B[clones function]
    end
    
    subgraph "Level 2: Basic Modules"
        C[MultiHeadedAttention]
        D[PositionwiseFeedForward]
        E[PositionalEncoding]
        F[Embeddings]
    end
    
    subgraph "Level 3: Connection"
        G[SublayerConnection]
    end
    
    subgraph "Level 4: Layer Modules"
        H[EncoderLayer]
        I[DecoderLayer]
    end
    
    subgraph "Level 5: Stacks"
        J[Encoder]
        K[Decoder]
        L[Generator]
    end
    
    subgraph "Level 6: Complete Model"
        M[EncoderDecoder]
        N[make_model]
    end
    
    A --&gt; C
    C --&gt; H
    C --&gt; I
    D --&gt; H
    D --&gt; I
    E --&gt; N
    F --&gt; N
    G --&gt; H
    G --&gt; I
    B --&gt; J
    B --&gt; K
    H --&gt; J
    I --&gt; K
    J --&gt; M
    K --&gt; M
    L --&gt; M
    M --&gt; N</code></pre>
<p>Would you like me to adjust the diagram to show more detail, or is this level of abstraction good?</p>
<div id="65ab9081" class="cell" data-time_run="2025-12-01T11:11:03.677807+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb260"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb260-1"><a href="#cb260-1" aria-hidden="true" tabindex="-1"></a>graph TD</span>
<span id="cb260-2"><a href="#cb260-2" aria-hidden="true" tabindex="-1"></a>    subgraph <span class="st">"Level 1: Core Functions"</span></span>
<span id="cb260-3"><a href="#cb260-3" aria-hidden="true" tabindex="-1"></a>        A[attention function]</span>
<span id="cb260-4"><a href="#cb260-4" aria-hidden="true" tabindex="-1"></a>        B[clones function]</span>
<span id="cb260-5"><a href="#cb260-5" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb260-6"><a href="#cb260-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb260-7"><a href="#cb260-7" aria-hidden="true" tabindex="-1"></a>    subgraph <span class="st">"Level 2: Basic Modules"</span></span>
<span id="cb260-8"><a href="#cb260-8" aria-hidden="true" tabindex="-1"></a>        C[MultiHeadedAttention]</span>
<span id="cb260-9"><a href="#cb260-9" aria-hidden="true" tabindex="-1"></a>        D[PositionwiseFeedForward]</span>
<span id="cb260-10"><a href="#cb260-10" aria-hidden="true" tabindex="-1"></a>        E[PositionalEncoding]</span>
<span id="cb260-11"><a href="#cb260-11" aria-hidden="true" tabindex="-1"></a>        F[Embeddings]</span>
<span id="cb260-12"><a href="#cb260-12" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb260-13"><a href="#cb260-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb260-14"><a href="#cb260-14" aria-hidden="true" tabindex="-1"></a>    subgraph <span class="st">"Level 3: Connection"</span></span>
<span id="cb260-15"><a href="#cb260-15" aria-hidden="true" tabindex="-1"></a>        G[SublayerConnection]</span>
<span id="cb260-16"><a href="#cb260-16" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb260-17"><a href="#cb260-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb260-18"><a href="#cb260-18" aria-hidden="true" tabindex="-1"></a>    subgraph <span class="st">"Level 4: Layer Modules"</span></span>
<span id="cb260-19"><a href="#cb260-19" aria-hidden="true" tabindex="-1"></a>        H[EncoderLayer]</span>
<span id="cb260-20"><a href="#cb260-20" aria-hidden="true" tabindex="-1"></a>        I[DecoderLayer]</span>
<span id="cb260-21"><a href="#cb260-21" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb260-22"><a href="#cb260-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb260-23"><a href="#cb260-23" aria-hidden="true" tabindex="-1"></a>    subgraph <span class="st">"Level 5: Stacks"</span></span>
<span id="cb260-24"><a href="#cb260-24" aria-hidden="true" tabindex="-1"></a>        J[Encoder]</span>
<span id="cb260-25"><a href="#cb260-25" aria-hidden="true" tabindex="-1"></a>        K[Decoder]</span>
<span id="cb260-26"><a href="#cb260-26" aria-hidden="true" tabindex="-1"></a>        L[Generator]</span>
<span id="cb260-27"><a href="#cb260-27" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb260-28"><a href="#cb260-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb260-29"><a href="#cb260-29" aria-hidden="true" tabindex="-1"></a>    subgraph <span class="st">"Level 6: Complete Model"</span></span>
<span id="cb260-30"><a href="#cb260-30" aria-hidden="true" tabindex="-1"></a>        M[EncoderDecoder]</span>
<span id="cb260-31"><a href="#cb260-31" aria-hidden="true" tabindex="-1"></a>        N[make_model]</span>
<span id="cb260-32"><a href="#cb260-32" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb260-33"><a href="#cb260-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb260-34"><a href="#cb260-34" aria-hidden="true" tabindex="-1"></a>    A <span class="op">--&gt;</span> C</span>
<span id="cb260-35"><a href="#cb260-35" aria-hidden="true" tabindex="-1"></a>    C <span class="op">--&gt;</span> H</span>
<span id="cb260-36"><a href="#cb260-36" aria-hidden="true" tabindex="-1"></a>    C <span class="op">--&gt;</span> I</span>
<span id="cb260-37"><a href="#cb260-37" aria-hidden="true" tabindex="-1"></a>    D <span class="op">--&gt;</span> H</span>
<span id="cb260-38"><a href="#cb260-38" aria-hidden="true" tabindex="-1"></a>    D <span class="op">--&gt;</span> I</span>
<span id="cb260-39"><a href="#cb260-39" aria-hidden="true" tabindex="-1"></a>    E <span class="op">--&gt;</span> N</span>
<span id="cb260-40"><a href="#cb260-40" aria-hidden="true" tabindex="-1"></a>    F <span class="op">--&gt;</span> N</span>
<span id="cb260-41"><a href="#cb260-41" aria-hidden="true" tabindex="-1"></a>    G <span class="op">--&gt;</span> H</span>
<span id="cb260-42"><a href="#cb260-42" aria-hidden="true" tabindex="-1"></a>    G <span class="op">--&gt;</span> I</span>
<span id="cb260-43"><a href="#cb260-43" aria-hidden="true" tabindex="-1"></a>    B <span class="op">--&gt;</span> J</span>
<span id="cb260-44"><a href="#cb260-44" aria-hidden="true" tabindex="-1"></a>    B <span class="op">--&gt;</span> K</span>
<span id="cb260-45"><a href="#cb260-45" aria-hidden="true" tabindex="-1"></a>    H <span class="op">--&gt;</span> J</span>
<span id="cb260-46"><a href="#cb260-46" aria-hidden="true" tabindex="-1"></a>    I <span class="op">--&gt;</span> K</span>
<span id="cb260-47"><a href="#cb260-47" aria-hidden="true" tabindex="-1"></a>    J <span class="op">--&gt;</span> M</span>
<span id="cb260-48"><a href="#cb260-48" aria-hidden="true" tabindex="-1"></a>    K <span class="op">--&gt;</span> M</span>
<span id="cb260-49"><a href="#cb260-49" aria-hidden="true" tabindex="-1"></a>    L <span class="op">--&gt;</span> M</span>
<span id="cb260-50"><a href="#cb260-50" aria-hidden="true" tabindex="-1"></a>    M <span class="op">--&gt;</span> N</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">

<div class="mermaid">
graph TD
    subgraph "Level 1: Core Functions"
        A[attention function]
        B[clones function]
    end

    subgraph "Level 2: Basic Modules"
        C[MultiHeadedAttention]
        D[PositionwiseFeedForward]
        E[PositionalEncoding]
        F[Embeddings]
    end

    subgraph "Level 3: Connection"
        G[SublayerConnection]
    end

    subgraph "Level 4: Layer Modules"
        H[EncoderLayer]
        I[DecoderLayer]
    end

    subgraph "Level 5: Stacks"
        J[Encoder]
        K[Decoder]
        L[Generator]
    end

    subgraph "Level 6: Complete Model"
        M[EncoderDecoder]
        N[make_model]
    end

    A --&gt; C
    C --&gt; H
    C --&gt; I
    D --&gt; H
    D --&gt; I
    E --&gt; N
    F --&gt; N
    G --&gt; H
    G --&gt; I
    B --&gt; J
    B --&gt; K
    H --&gt; J
    I --&gt; K
    J --&gt; M
    K --&gt; M
    L --&gt; M
    M --&gt; N

</div>
<script type="module">
import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.1/+esm';
mermaid.initialize({ startOnLoad: true });
mermaid.run();
</script>
</div>
</div>
<p>Fantastic, now what can I do? Have we finished the original code?</p>
</section>
<section id="reply-157" class="level5">
<h5 class="anchored" data-anchor-id="reply-157">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>🎉 <strong>Amazing work!</strong> You’ve implemented the <strong>core Transformer architecture</strong> from scratch!</p>
<p><strong>What you’ve completed:</strong> - ✓ All the fundamental building blocks - ✓ Complete Encoder and Decoder - ✓ Full EncoderDecoder model - ✓ Convenience <code>make_model</code> function</p>
<p><strong>What’s in the original code that we haven’t covered:</strong></p>
<ol type="1">
<li><strong>Training infrastructure:</strong>
<ul>
<li>Batch class (for handling batches with masks)</li>
<li>Loss computation</li>
<li>Learning rate scheduler (the warmup schedule)</li>
<li>Label smoothing</li>
<li>Training loop</li>
</ul></li>
<li><strong>Inference:</strong>
<ul>
<li>Greedy decoding</li>
<li>Beam search (mentioned but not fully implemented)</li>
</ul></li>
<li><strong>Real-world example:</strong>
<ul>
<li>Data loading (Multi30k dataset)</li>
<li>Tokenization with spacy</li>
<li>Actual training on translation task</li>
</ul></li>
</ol>
<p><strong>What would you like to do next?</strong></p>
<ol type="1">
<li>Implement the training infrastructure (batching, loss, optimizer schedule)?</li>
<li>Implement inference/decoding methods?</li>
<li>Try a simple training example (like the copy task)?</li>
<li>Move on to something else?</li>
</ol>
<p>What interests you most?</p>
<p>What do you suggest? I think training my own LLM and seeing it work would be a magical moment.</p>
</section>
<section id="reply-158" class="level5">
<h5 class="anchored" data-anchor-id="reply-158">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>That’s an amazing goal! Seeing your own model learn and generate text is truly magical! ✨</p>
<p>However, let me set realistic expectations:</p>
<p><strong>Full LLM training</strong> (like GPT) requires: - Massive datasets (billions of tokens) - Huge computational resources (multiple GPUs, weeks of training) - Significant memory and storage</p>
<p><strong>But you have great options to see the magic:</strong></p>
<ol type="1">
<li><strong>Copy task</strong> (quickest win, ~5 minutes)
<ul>
<li>Train the model to memorize a sequence</li>
<li>See it learn from random to perfect in real-time</li>
<li>Great for verifying everything works</li>
</ul></li>
<li><strong>Small translation task</strong> (realistic, ~30-60 minutes)
<ul>
<li>Use a tiny dataset (like Multi30k: German↔︎English)</li>
<li>Train on your laptop/Colab</li>
<li>Actually see translation working!</li>
</ul></li>
<li><strong>Character-level language model</strong> (middle ground)
<ul>
<li>Train on a small text corpus</li>
<li>Generate text character by character</li>
<li>Can run on modest hardware</li>
</ul></li>
</ol>
<p><strong>My suggestion:</strong> Start with #1 (copy task) to see the magic quickly, then move to #2 if you want something more impressive.</p>
<p>Which sounds most exciting to you?</p>
<p>What does the original code do?</p>
</section>
<section id="reply-159" class="level5">
<h5 class="anchored" data-anchor-id="reply-159">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠</p>
<p>Great question! Looking at the original code you provided, it includes:</p>
<p><strong>1. Simple Copy Task (A First Example)</strong> - Synthetic data: generates random sequences - Trains the model to copy input to output (e.g., input [1,2,3,4] → output [1,2,3,4]) - Quick to train, proves the model works - Uses greedy decoding for inference</p>
<p><strong>2. Real-World Translation (Part 3)</strong> - Multi30k German→English dataset - Full training pipeline with: - Data loading and tokenization (spacy) - Batching with masks - Label smoothing loss - Adam optimizer with warmup schedule - Multi-GPU support</p>
<p><strong>Supporting infrastructure:</strong> - <code>Batch</code> class - handles src/tgt with masks - <code>run_epoch</code> - training loop - <code>LabelSmoothing</code> - regularization technique - Learning rate scheduler with warmup - <code>greedy_decode</code> - simple inference</p>
<p>Would you like to start with the <strong>copy task</strong> (simpler, faster, proves your model works) or jump to building the training infrastructure?</p>
</section>
</section>
<section id="copy-code-task" class="level2">
<h2 class="anchored" data-anchor-id="copy-code-task">Copy Code Task</h2>
<p>lets go</p>
<section id="reply-160" class="level5">
<h5 class="anchored" data-anchor-id="reply-160">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Awesome! Let’s start with the <strong>copy task</strong> - it’s the quickest way to see your Transformer learn something magical! 🎉</p>
<p>Before we begin coding, let me explain what we’ll build:</p>
<p><strong>The Copy Task:</strong> - Input: Random sequence like <code>[1, 2, 3, 4, 5]</code> - Target: Same sequence <code>[1, 2, 3, 4, 5]</code> - Goal: Train the model to copy the input to output</p>
<p><strong>What we need to implement (in order):</strong> 1. <strong>Synthetic data generator</strong> - creates random sequences 2. <strong>Batch class</strong> - handles masking 3. <strong>Loss computation</strong> - measures how well the model does 4. <strong>Training loop</strong> - actually trains the model 5. <strong>Greedy decoding</strong> - generates predictions</p>
<p><strong>First question:</strong> Do you want to implement each piece yourself (I’ll guide you), or would you like me to provide the code so we can see it working quickly and then understand how it works?</p>
<p>Which approach would you prefer?</p>
<p>Isn’t looking at the code cheating?</p>
</section>
<section id="reply-161" class="level5">
<h5 class="anchored" data-anchor-id="reply-161">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You’re absolutely right! Looking at the code would be cheating yourself out of the learning experience! 💪</p>
<p>You’ve come this far implementing the Transformer from scratch - let’s keep that momentum going!</p>
<p><strong>Let’s start with the first component: Synthetic Data Generator</strong></p>
<p>For the copy task, we need a function that generates random batches of sequences. It should: - Create random sequences of integers (e.g., from vocabulary of size V) - Make both source and target the same (since we’re copying) - Add a start token at the beginning - Return batches</p>
<p>Looking at what you need, the function signature would be:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb261"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb261-1"><a href="#cb261-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> data_gen(V, batch_size, nbatches):</span>
<span id="cb261-2"><a href="#cb261-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate nbatches of random data</span></span>
<span id="cb261-3"><a href="#cb261-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each batch has batch_size sequences</span></span>
<span id="cb261-4"><a href="#cb261-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each sequence has random integers from 1 to V-1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What do you think you need to do inside this function? Try describing the steps first before coding!</p>
<p>Ok Vara, I am kind of clueless. You need to give me some helper functions, some helper structures.</p>
</section>
<section id="reply-162" class="level5">
<h5 class="anchored" data-anchor-id="reply-162">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>No problem! Let me break down what you need step by step.</p>
<p><strong>For the copy task, you need 4 main pieces:</strong></p>
<ol type="1">
<li><strong>Batch class</strong> - wraps source/target data and creates masks</li>
<li><strong>data_gen function</strong> - generates random training data</li>
<li><strong>SimpleLossCompute</strong> - calculates loss during training</li>
<li><strong>run_epoch function</strong> - the training loop</li>
</ol>
<p>Let’s start with the <strong>Batch class</strong> since everything else depends on it.</p>
<p><strong>What does Batch need to do?</strong> - Store source (<code>src</code>) and target (<code>tgt</code>) sequences - Create <code>src_mask</code> (padding mask for source) - Create <code>tgt_mask</code> (causal mask for target - prevents looking ahead) - Count number of tokens (<code>ntokens</code>)</p>
<p>Try implementing the <code>__init__</code> method. You’ll need: - Take <code>src</code>, <code>tgt</code>, and <code>pad</code> (padding token ID) as inputs - Create masks using operations you’ve seen before</p>
<p>Want to give it a try, or should I give you more hints about what each mask should look like?</p>
<div id="9d5b2faf" class="cell" data-time_run="2025-12-01T09:45:38.624720+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb262"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb262-1"><a href="#cb262-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Batch:</span>
<span id="cb262-2"><a href="#cb262-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(src, tgt, pad<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb262-3"><a href="#cb262-3" aria-hidden="true" tabindex="-1"></a>        src_mask <span class="op">=</span> src<span class="op">!=</span>pad</span>
<span id="cb262-4"><a href="#cb262-4" aria-hidden="true" tabindex="-1"></a>        tgt_mask <span class="op">=</span> tgt<span class="op">==</span>uppertriangle</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Okay, I’m clueless. This is what I’ve done so far.</p>
</section>
<section id="reply-163" class="level5">
<h5 class="anchored" data-anchor-id="reply-163">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠</p>
<p>You’re on the right track! Let me clarify a few things:</p>
<p><strong>First, small fix:</strong> You need <code>self</code> in the method:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb263"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb263-1"><a href="#cb263-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, src, tgt, pad<span class="op">=</span><span class="dv">2</span>):</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Your intuitions are correct!</strong></p>
<ol type="1">
<li><p><strong>src_mask</strong> - You’re right, it’s about padding! But we need to keep the non-pad tokens, so it should create a boolean mask where True = real token, False = padding. Also, we need to <code>unsqueeze</code> to add a dimension.</p></li>
<li><p><strong>tgt_mask</strong> - Yes, it involves the upper triangle! You want to use <code>torch.triu</code> (upper triangle) to block future positions. But there’s a helper function we created earlier - remember <code>subsequent_mask</code>?</p></li>
</ol>
<p>Wait… we didn’t implement <code>subsequent_mask</code>! Let’s do that first.</p>
<p><strong>subsequent_mask</strong> creates a triangular mask like:</p>
<pre><code>[[1, 0, 0],
 [1, 1, 0],
 [1, 1, 1]]</code></pre>
<p>This lets position 0 see only itself, position 1 see 0 and 1, etc.</p>
<p>Can you try implementing <code>subsequent_mask(size)</code> that returns a lower triangular matrix of ones?</p>
<p>Hint: Look at <code>torch.triu</code> or <code>torch.tril</code></p>
<p>Why we need to unsqueeze.</p>
</section>
<section id="reply-164" class="level5">
<h5 class="anchored" data-anchor-id="reply-164">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! Let me explain the dimensions.</p>
<p><strong>Without unsqueeze:</strong> - <code>src</code> has shape: <code>(batch_size, seq_len)</code> - <code>src != pad</code> gives: <code>(batch_size, seq_len)</code> - a 2D mask</p>
<p><strong>But in attention:</strong> - We compute scores with shape: <code>(batch_size, num_heads, seq_len, seq_len)</code> - The mask needs to broadcast across the <code>num_heads</code> and <code>seq_len</code> dimensions</p>
<p><strong>With unsqueeze:</strong> - We add a dimension: <code>(batch_size, 1, seq_len)</code> - This broadcasts correctly when applied to attention scores</p>
<p>Think of it like this: the <code>1</code> in the middle says “apply this same mask to all attention heads.”</p>
<p>Does this clarify why we need the extra dimension?</p>
<p>Now, want to try implementing <code>subsequent_mask(size)</code> first, or should we continue with the Batch class?</p>
<div id="f878e3f8" class="cell" data-time_run="2025-12-01T10:00:52.560122+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb265"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb265-1"><a href="#cb265-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Batch:</span>
<span id="cb265-2"><a href="#cb265-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(src, tgt, pad<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb265-3"><a href="#cb265-3" aria-hidden="true" tabindex="-1"></a>        src_mask <span class="op">=</span> src<span class="op">!=</span>pad</span>
<span id="cb265-4"><a href="#cb265-4" aria-hidden="true" tabindex="-1"></a>        src_mask.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb265-5"><a href="#cb265-5" aria-hidden="true" tabindex="-1"></a>        tgt_mask <span class="op">=</span> torch.ones([tgt.size(<span class="dv">0</span>), tgt.size(<span class="dv">1</span>)]).triu()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>I still think I’m not correct, but I think I’m getting warmer.</p>
</section>
<section id="reply-165" class="level5">
<h5 class="anchored" data-anchor-id="reply-165">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You’re definitely getting warmer! A few things to fix:</p>
<ol type="1">
<li><p><strong>Missing <code>self</code></strong> - you need <code>self</code> as the first parameter and use it to store attributes:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb266"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb266-1"><a href="#cb266-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, src, tgt, pad<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb266-2"><a href="#cb266-2" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.src <span class="op">=</span> src</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong><code>unsqueeze</code> doesn’t modify in place</strong> - you need to assign it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb267"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb267-1"><a href="#cb267-1" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.src_mask <span class="op">=</span> (src <span class="op">!=</span> pad).unsqueeze(<span class="op">-</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The <code>-2</code> adds a dimension at position -2 (second from last).</p></li>
<li><p><strong><code>triu</code> creates upper triangle, but we want lower!</strong> - Use <code>torch.tril</code> for lower triangle, OR use <code>torch.triu</code> with <code>diagonal=1</code> and then invert it. Actually, there’s a cleaner way - remember we saw <code>subsequent_mask</code> in the original code?</p></li>
<li><p><strong>Target mask needs to combine two things:</strong></p>
<ul>
<li>Padding mask (like src)</li>
<li>Causal mask (subsequent_mask)</li>
</ul></li>
</ol>
<p>Before fixing all this, let’s first implement the <code>subsequent_mask</code> helper function. Can you write a function that returns a lower triangular matrix of True values with shape <code>(1, size, size)</code>?</p>
<p>Actually, I have not read the original code.</p>
</section>
<section id="reply-166" class="level5">
<h5 class="anchored" data-anchor-id="reply-166">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You’re absolutely right - I apologize! You’ve been implementing everything from scratch without looking at the original code, and I should respect that.</p>
<p>Let me guide you without referencing the original code.</p>
<p><strong>Let’s implement <code>subsequent_mask</code> from first principles:</strong></p>
<p>You need a function that creates a mask to prevent looking at future positions. For a sequence of length 3, it should look like:</p>
<pre><code>Position 0 can see: [True,  False, False]  (only itself)
Position 1 can see: [True,  True,  False]  (0 and 1)
Position 2 can see: [True,  True,  True]   (0, 1, and 2)</code></pre>
<p>This is a <strong>lower triangular matrix</strong> (ones below and on the diagonal, zeros above).</p>
<p>Try writing:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb269"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb269-1"><a href="#cb269-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> subsequent_mask(size):</span>
<span id="cb269-2"><a href="#cb269-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a lower triangular matrix</span></span>
<span id="cb269-3"><a href="#cb269-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return shape should be (1, size, size)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Hint: <code>torch.tril(torch.ones(size, size))</code> creates a lower triangular matrix. Then you need to add the batch dimension and convert to boolean.</p>
<p>Give it a try!</p>
</section>
<section id="final-subsequent-mask-function" class="level3">
<h3 class="anchored" data-anchor-id="final-subsequent-mask-function">Final subsequent mask function</h3>
<hr>
</section>
<section id="subsequent_mask" class="level3">
<h3 class="anchored" data-anchor-id="subsequent_mask">subsequent_mask</h3>
<blockquote class="blockquote">
<pre><code> subsequent_mask (size)</code></pre>
</blockquote>
<p>Is this good?</p>
<section id="reply-167" class="level5">
<h5 class="anchored" data-anchor-id="reply-167">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>subsequent_mask</code> function is exactly right! ✓</p>
<p>You’ve got: - ✓ Lower triangular matrix with <code>tril()</code> - ✓ Convert to boolean - ✓ Add batch dimension with <code>unsqueeze(0)</code> - ✓ Returns shape <code>(1, size, size)</code></p>
<p>Now let’s go back to the <strong>Batch class</strong>. You need to:</p>
<ol type="1">
<li>Store <code>self.src</code> and create <code>self.src_mask</code></li>
<li>For the target, you need to:
<ul>
<li>Store <code>self.tgt</code> (but shift it - remove last token)</li>
<li>Store <code>self.tgt_y</code> (the labels - remove first token)</li>
<li>Create <code>self.tgt_mask</code> combining padding mask AND subsequent_mask</li>
<li>Count <code>self.ntokens</code> (number of non-padding tokens in target)</li>
</ul></li>
</ol>
<p>Try implementing just the <code>__init__</code> signature and storing src/src_mask first:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb271"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb271-1"><a href="#cb271-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Batch:</span>
<span id="cb271-2"><a href="#cb271-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, src, tgt<span class="op">=</span><span class="va">None</span>, pad<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb271-3"><a href="#cb271-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src <span class="op">=</span> src</span>
<span id="cb271-4"><a href="#cb271-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src_mask <span class="op">=</span> <span class="co"># what goes here?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What should <code>src_mask</code> be?</p>
<p>What exactly are you talking about? What is TGT and TGT_why? And why aren’t we unsqueezing the masks?</p>
</section>
<section id="reply-168" class="level5">
<h5 class="anchored" data-anchor-id="reply-168">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great questions! Let me clarify:</p>
<p><strong>Why shift the target?</strong></p>
<p>During training, the decoder needs: - <strong>Input (tgt)</strong>: What it’s seen so far: <code>[&lt;start&gt;, word1, word2]</code> - <strong>Label (tgt_y)</strong>: What it should predict next: <code>[word1, word2, word3]</code></p>
<p>So if your full target is <code>[&lt;start&gt;, word1, word2, word3]</code>: - <code>tgt</code> = <code>[&lt;start&gt;, word1, word2]</code> (everything except last) - <code>tgt_y</code> = <code>[word1, word2, word3]</code> (everything except first)</p>
<p>This way, when the model sees <code>&lt;start&gt;</code>, it should predict <code>word1</code>, etc.</p>
<p><strong>About unsqueezing:</strong></p>
<p>You’re right to question this! For <code>src_mask</code>, we DO need to unsqueeze:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb272"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb272-1"><a href="#cb272-1" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.src_mask <span class="op">=</span> (src <span class="op">!=</span> pad).unsqueeze(<span class="op">-</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This makes it shape <code>(batch, 1, seq_len)</code> which broadcasts correctly.</p>
<p><strong>Let’s simplify - just focus on src first:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb273"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb273-1"><a href="#cb273-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Batch:</span>
<span id="cb273-2"><a href="#cb273-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, src, tgt<span class="op">=</span><span class="va">None</span>, pad<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb273-3"><a href="#cb273-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src <span class="op">=</span> src</span>
<span id="cb273-4"><a href="#cb273-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src_mask <span class="op">=</span> (src <span class="op">!=</span> pad).unsqueeze(<span class="op">-</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Does this part make sense now? Once you’re comfortable, we’ll add the target handling.</p>
<p>Yes, let’s move on.</p>
</section>
<section id="reply-169" class="level5">
<h5 class="anchored" data-anchor-id="reply-169">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! Now let’s add the target handling.</p>
<p><strong>When <code>tgt</code> is provided, you need to:</strong></p>
<ol type="1">
<li><strong>Shift the target:</strong>
<ul>
<li><code>self.tgt = tgt[:, :-1]</code> (all but last token - this is the decoder input)</li>
<li><code>self.tgt_y = tgt[:, 1:]</code> (all but first token - these are the labels)</li>
</ul></li>
<li><strong>Create tgt_mask</strong> that combines:
<ul>
<li>Padding mask: <code>(self.tgt != pad)</code></li>
<li>Subsequent mask: <code>subsequent_mask(self.tgt.size(-1))</code></li>
<li>Combine them with <code>&amp;</code> (logical AND)</li>
</ul></li>
<li><strong>Count tokens:</strong>
<ul>
<li><code>self.ntokens = (self.tgt_y != pad).sum()</code></li>
</ul></li>
</ol>
<p>Try adding the <code>if tgt is not None:</code> block to handle these. What would you write?</p>
<div id="a8bed850" class="cell" data-time_run="2025-12-01T10:22:03.127004+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb274"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb274-1"><a href="#cb274-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Batch:</span>
<span id="cb274-2"><a href="#cb274-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, src, tgt<span class="op">=</span><span class="va">None</span>, pad<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb274-3"><a href="#cb274-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src <span class="op">=</span> src</span>
<span id="cb274-4"><a href="#cb274-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src_mask <span class="op">=</span> (src <span class="op">!=</span> pad).unsqueeze(<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb274-5"><a href="#cb274-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> tgt <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb274-6"><a href="#cb274-6" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.tgt <span class="op">=</span> tgt[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb274-7"><a href="#cb274-7" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.tgt_y <span class="op">=</span> tgt[:,<span class="dv">1</span>:]</span>
<span id="cb274-8"><a href="#cb274-8" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.pad_tgt_mask <span class="op">=</span> (<span class="va">self</span>.tgt<span class="op">!=</span>pad)</span>
<span id="cb274-9"><a href="#cb274-9" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.subseq_tgt_mask <span class="op">=</span> subsequent_mask(tgt.size(<span class="dv">1</span>))</span>
<span id="cb274-10"><a href="#cb274-10" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.tgt_mask <span class="op">=</span> <span class="va">self</span>.pad_tgt_mask <span class="op">&amp;</span> <span class="va">self</span>.subseq_tgt_mask</span>
<span id="cb274-11"><a href="#cb274-11" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.ntokens <span class="op">=</span> (<span class="va">self</span>.tgt_y<span class="op">!=</span>pad).<span class="bu">sum</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="reply-170" class="level5">
<h5 class="anchored" data-anchor-id="reply-170">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent work! Your Batch class is almost perfect! Just a couple of small fixes:</p>
<ol type="1">
<li><p><strong><code>subsequent_mask</code> size:</strong> You’re using <code>tgt.size(1)</code> but you should use <code>self.tgt.size(1)</code> since you’ve already sliced it (removed last token)</p></li>
<li><p><strong>Unsqueeze pad_tgt_mask:</strong> The padding mask needs to match dimensions with subsequent_mask for broadcasting:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb275"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb275-1"><a href="#cb275-1" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.pad_tgt_mask <span class="op">=</span> (<span class="va">self</span>.tgt <span class="op">!=</span> pad).unsqueeze(<span class="op">-</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ol>
<p>Those are the only changes needed! Want to fix these and then we can test the Batch class?</p>
</section>
<section id="reply-171" class="level5">
<h5 class="anchored" data-anchor-id="reply-171">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You’re doing great! Just make those two small fixes:</p>
<ol type="1">
<li>Change <code>tgt.size(1)</code> to <code>self.tgt.size(1)</code></li>
<li>Add <code>.unsqueeze(-2)</code> to the padding mask</li>
</ol>
<p>Once you update those, your Batch class will be complete! Want to give it a try?</p>
</section>
</section>
<section id="final-batch-class-for-copy-example" class="level3">
<h3 class="anchored" data-anchor-id="final-batch-class-for-copy-example">Final Batch Class for copy example</h3>
<hr>
</section>
<section id="batch" class="level3">
<h3 class="anchored" data-anchor-id="batch">Batch</h3>
<blockquote class="blockquote">
<pre><code> Batch (src, tgt=None, pad=2)</code></pre>
</blockquote>
<p><em>Initialize self. See help(type(self)) for accurate signature.</em></p>
<p>fine?</p>
<section id="reply-172" class="level5">
<h5 class="anchored" data-anchor-id="reply-172">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>Batch</code> class is now complete and correct! ✓</p>
<p>Let’s test it quickly:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb277"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb277-1"><a href="#cb277-1" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">0</span>]])  <span class="co"># 0 = padding</span></span>
<span id="cb277-2"><a href="#cb277-2" aria-hidden="true" tabindex="-1"></a>tgt <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">0</span>]])</span>
<span id="cb277-3"><a href="#cb277-3" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> Batch(src, tgt, pad<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb277-4"><a href="#cb277-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb277-5"><a href="#cb277-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"src_mask shape:"</span>, batch.src_mask.shape)</span>
<span id="cb277-6"><a href="#cb277-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"tgt_mask shape:"</span>, batch.tgt_mask.shape)</span>
<span id="cb277-7"><a href="#cb277-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ntokens:"</span>, batch.ntokens)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Try running this to verify it works!</p>
<p>can we also print the mask and src tgt for demo?</p>
</section>
<section id="reply-173" class="level5">
<h5 class="anchored" data-anchor-id="reply-173">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Absolutely! Let’s see everything:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb278"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb278-1"><a href="#cb278-1" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">0</span>]])  <span class="co"># 0 = padding</span></span>
<span id="cb278-2"><a href="#cb278-2" aria-hidden="true" tabindex="-1"></a>tgt <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">0</span>]])</span>
<span id="cb278-3"><a href="#cb278-3" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> Batch(src, tgt, pad<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb278-4"><a href="#cb278-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb278-5"><a href="#cb278-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source:"</span>, batch.src)</span>
<span id="cb278-6"><a href="#cb278-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source mask:"</span>, batch.src_mask)</span>
<span id="cb278-7"><a href="#cb278-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Target input:"</span>, batch.tgt)</span>
<span id="cb278-8"><a href="#cb278-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Target labels:"</span>, batch.tgt_y)</span>
<span id="cb278-9"><a href="#cb278-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Target mask shape:"</span>, batch.tgt_mask.shape)</span>
<span id="cb278-10"><a href="#cb278-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Target mask:</span><span class="ch">\n</span><span class="st">"</span>, batch.tgt_mask)</span>
<span id="cb278-11"><a href="#cb278-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Number of tokens:"</span>, batch.ntokens.item())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this to see how the shifting works and what the masks look like!</p>
<div id="53a0ef03" class="cell" data-time_run="2025-12-01T10:28:29.341230+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb279"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb279-1"><a href="#cb279-1" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">0</span>]])  <span class="co"># 0 = padding</span></span>
<span id="cb279-2"><a href="#cb279-2" aria-hidden="true" tabindex="-1"></a>tgt <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">0</span>]])</span>
<span id="cb279-3"><a href="#cb279-3" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> Batch(src, tgt, pad<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb279-4"><a href="#cb279-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb279-5"><a href="#cb279-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source:"</span>, batch.src)</span>
<span id="cb279-6"><a href="#cb279-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source mask:"</span>, batch.src_mask)</span>
<span id="cb279-7"><a href="#cb279-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Target input:"</span>, batch.tgt)</span>
<span id="cb279-8"><a href="#cb279-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Target labels:"</span>, batch.tgt_y)</span>
<span id="cb279-9"><a href="#cb279-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Target mask shape:"</span>, batch.tgt_mask.shape)</span>
<span id="cb279-10"><a href="#cb279-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Target mask:</span><span class="ch">\n</span><span class="st">"</span>, batch.tgt_mask)</span>
<span id="cb279-11"><a href="#cb279-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Number of tokens:"</span>, batch.ntokens.item())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Source: tensor([[1, 2, 3, 4, 0, 0]])
Source mask: tensor([[[ True,  True,  True,  True, False, False]]])

Target input: tensor([[1, 2, 3, 4, 5]])
Target labels: tensor([[2, 3, 4, 5, 0]])

Target mask shape: torch.Size([1, 5, 5])
Target mask:
 tensor([[[ True, False, False, False, False],
         [ True,  True, False, False, False],
         [ True,  True,  True, False, False],
         [ True,  True,  True,  True, False],
         [ True,  True,  True,  True,  True]]])

Number of tokens: 4</code></pre>
</div>
</div>
<p>Assuming we are using NBDEV and we already outputted the function, create a comprehensive unit test for this.</p>
</section>
<section id="reply-174" class="level5">
<h5 class="anchored" data-anchor-id="reply-174">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb281"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb281-1"><a href="#cb281-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| test</span></span>
<span id="cb281-2"><a href="#cb281-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_batch_class():</span>
<span id="cb281-3"><a href="#cb281-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Batch class with masking and shifting"</span></span>
<span id="cb281-4"><a href="#cb281-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test with padding</span></span>
<span id="cb281-5"><a href="#cb281-5" aria-hidden="true" tabindex="-1"></a>    src <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">0</span>]])  <span class="co"># 0 = padding</span></span>
<span id="cb281-6"><a href="#cb281-6" aria-hidden="true" tabindex="-1"></a>    tgt <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">0</span>]])</span>
<span id="cb281-7"><a href="#cb281-7" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> Batch(src, tgt, pad<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb281-8"><a href="#cb281-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb281-9"><a href="#cb281-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check source</span></span>
<span id="cb281-10"><a href="#cb281-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.src.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">6</span>])</span>
<span id="cb281-11"><a href="#cb281-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.src_mask.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">6</span>])</span>
<span id="cb281-12"><a href="#cb281-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.src_mask[<span class="dv">0</span>, <span class="dv">0</span>, :<span class="dv">4</span>].<span class="bu">all</span>()  <span class="co"># First 4 are True</span></span>
<span id="cb281-13"><a href="#cb281-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="kw">not</span> batch.src_mask[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">4</span>:].<span class="bu">any</span>()  <span class="co"># Last 2 are False</span></span>
<span id="cb281-14"><a href="#cb281-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb281-15"><a href="#cb281-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check target shifting</span></span>
<span id="cb281-16"><a href="#cb281-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.tgt.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">5</span>])</span>
<span id="cb281-17"><a href="#cb281-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.tgt_y.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">5</span>])</span>
<span id="cb281-18"><a href="#cb281-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> torch.equal(batch.tgt[<span class="dv">0</span>], torch.LongTensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]))</span>
<span id="cb281-19"><a href="#cb281-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> torch.equal(batch.tgt_y[<span class="dv">0</span>], torch.LongTensor([<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">0</span>]))</span>
<span id="cb281-20"><a href="#cb281-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb281-21"><a href="#cb281-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check target mask is lower triangular</span></span>
<span id="cb281-22"><a href="#cb281-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.tgt_mask.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">5</span>])</span>
<span id="cb281-23"><a href="#cb281-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb281-24"><a href="#cb281-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> batch.tgt_mask[<span class="dv">0</span>, i, :i<span class="op">+</span><span class="dv">1</span>].<span class="bu">all</span>()  <span class="co"># Can see up to position i</span></span>
<span id="cb281-25"><a href="#cb281-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">4</span>:</span>
<span id="cb281-26"><a href="#cb281-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> <span class="kw">not</span> batch.tgt_mask[<span class="dv">0</span>, i, i<span class="op">+</span><span class="dv">1</span>:].<span class="bu">any</span>()  <span class="co"># Cannot see future</span></span>
<span id="cb281-27"><a href="#cb281-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb281-28"><a href="#cb281-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check token count (excludes padding)</span></span>
<span id="cb281-29"><a href="#cb281-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.ntokens.item() <span class="op">==</span> <span class="dv">4</span>  <span class="co"># 2,3,4,5 (not the final 0)</span></span>
<span id="cb281-30"><a href="#cb281-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb281-31"><a href="#cb281-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test without target</span></span>
<span id="cb281-32"><a href="#cb281-32" aria-hidden="true" tabindex="-1"></a>    batch_no_tgt <span class="op">=</span> Batch(src, tgt<span class="op">=</span><span class="va">None</span>, pad<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb281-33"><a href="#cb281-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch_no_tgt.src_mask.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">6</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Great! Now that Batch is done, ready to move on to the data generator?</p>
<div id="06406bd7" class="cell" data-time_run="2025-12-01T10:31:09.489853+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb282"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb282-1"><a href="#cb282-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_batch_class():</span>
<span id="cb282-2"><a href="#cb282-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Test Batch class with masking and shifting"</span></span>
<span id="cb282-3"><a href="#cb282-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test with padding</span></span>
<span id="cb282-4"><a href="#cb282-4" aria-hidden="true" tabindex="-1"></a>    src <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">0</span>]])  <span class="co"># 0 = padding</span></span>
<span id="cb282-5"><a href="#cb282-5" aria-hidden="true" tabindex="-1"></a>    tgt <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">0</span>]])</span>
<span id="cb282-6"><a href="#cb282-6" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> Batch(src, tgt, pad<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb282-7"><a href="#cb282-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb282-8"><a href="#cb282-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check source</span></span>
<span id="cb282-9"><a href="#cb282-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.src.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">6</span>])</span>
<span id="cb282-10"><a href="#cb282-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.src_mask.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">6</span>])</span>
<span id="cb282-11"><a href="#cb282-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.src_mask[<span class="dv">0</span>, <span class="dv">0</span>, :<span class="dv">4</span>].<span class="bu">all</span>()  <span class="co"># First 4 are True</span></span>
<span id="cb282-12"><a href="#cb282-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="kw">not</span> batch.src_mask[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">4</span>:].<span class="bu">any</span>()  <span class="co"># Last 2 are False</span></span>
<span id="cb282-13"><a href="#cb282-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb282-14"><a href="#cb282-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check target shifting</span></span>
<span id="cb282-15"><a href="#cb282-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.tgt.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">5</span>])</span>
<span id="cb282-16"><a href="#cb282-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.tgt_y.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">5</span>])</span>
<span id="cb282-17"><a href="#cb282-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> torch.equal(batch.tgt[<span class="dv">0</span>], torch.LongTensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]))</span>
<span id="cb282-18"><a href="#cb282-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> torch.equal(batch.tgt_y[<span class="dv">0</span>], torch.LongTensor([<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">0</span>]))</span>
<span id="cb282-19"><a href="#cb282-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb282-20"><a href="#cb282-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check target mask is lower triangular</span></span>
<span id="cb282-21"><a href="#cb282-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.tgt_mask.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">5</span>])</span>
<span id="cb282-22"><a href="#cb282-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb282-23"><a href="#cb282-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> batch.tgt_mask[<span class="dv">0</span>, i, :i<span class="op">+</span><span class="dv">1</span>].<span class="bu">all</span>()  <span class="co"># Can see up to position i</span></span>
<span id="cb282-24"><a href="#cb282-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">4</span>:</span>
<span id="cb282-25"><a href="#cb282-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> <span class="kw">not</span> batch.tgt_mask[<span class="dv">0</span>, i, i<span class="op">+</span><span class="dv">1</span>:].<span class="bu">any</span>()  <span class="co"># Cannot see future</span></span>
<span id="cb282-26"><a href="#cb282-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb282-27"><a href="#cb282-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check token count (excludes padding)</span></span>
<span id="cb282-28"><a href="#cb282-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch.ntokens.item() <span class="op">==</span> <span class="dv">4</span>  <span class="co"># 2,3,4,5 (not the final 0)</span></span>
<span id="cb282-29"><a href="#cb282-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb282-30"><a href="#cb282-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test without target</span></span>
<span id="cb282-31"><a href="#cb282-31" aria-hidden="true" tabindex="-1"></a>    batch_no_tgt <span class="op">=</span> Batch(src, tgt<span class="op">=</span><span class="va">None</span>, pad<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb282-32"><a href="#cb282-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> batch_no_tgt.src_mask.shape <span class="op">==</span> torch.Size([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">6</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="7ce4898a" class="cell" data-time_run="2025-12-01T10:31:21.916815+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb283"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb283-1"><a href="#cb283-1" aria-hidden="true" tabindex="-1"></a>test_batch_class()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Moving on.</p>
</section>
<section id="reply-175" class="level5">
<h5 class="anchored" data-anchor-id="reply-175">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent! Now let’s build the <strong>data generator</strong> for the copy task.</p>
<p><strong>What does <code>data_gen</code> need to do?</strong></p>
<p>Generate synthetic training data where the target is a copy of the source: - Create random sequences of integers from 1 to V-1 (vocabulary size) - Make sure token 0 is reserved (for padding/blank) - Token 1 is the start token - Both src and tgt should be identical (it’s a copy task!) - Yield <code>nbatches</code> batches, each with <code>batch_size</code> sequences</p>
<p>Try writing the function signature and thinking about the steps:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb284"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb284-1"><a href="#cb284-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> data_gen(V, batch_size, nbatches):</span>
<span id="cb284-2"><a href="#cb284-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Generate random data for a src-tgt copy task"</span></span>
<span id="cb284-3"><a href="#cb284-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(nbatches):</span>
<span id="cb284-4"><a href="#cb284-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you need to create here?</span></span>
<span id="cb284-5"><a href="#cb284-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># How do you generate random sequences?</span></span>
<span id="cb284-6"><a href="#cb284-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># How do you make src and tgt?</span></span>
<span id="cb284-7"><a href="#cb284-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you yield?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What would be your approach?</p>
<div id="59e7c05b" class="cell" data-time_run="2025-12-01T10:36:18.757227+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb285"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb285-1"><a href="#cb285-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> data_gen(V, batch_size, nbatches):</span>
<span id="cb285-2"><a href="#cb285-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Generate random data for a src-tgt copy task"</span></span>
<span id="cb285-3"><a href="#cb285-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(nbatches):</span>
<span id="cb285-4"><a href="#cb285-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you need to create here?</span></span>
<span id="cb285-5"><a href="#cb285-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># How do you generate random sequences?</span></span>
<span id="cb285-6"><a href="#cb285-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># How do you make src and tgt?</span></span>
<span id="cb285-7"><a href="#cb285-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you yield?</span></span>
<span id="cb285-8"><a href="#cb285-8" aria-hidden="true" tabindex="-1"></a>        random_int <span class="op">=</span> np.random.randint(<span class="dv">1</span>, V, size<span class="op">=</span>[batch_size, seq_len])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this a correct direction?</p>
</section>
<section id="reply-176" class="level5">
<h5 class="anchored" data-anchor-id="reply-176">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good start! You’re on the right track using <code>np.random.randint</code>. A few things:</p>
<ol type="1">
<li><p><strong><code>seq_len</code> is not defined</strong> - you need to decide on a sequence length. For the copy task, let’s use a fixed length like 10.</p></li>
<li><p><strong>Start token</strong> - The first position should always be 1 (the start token), not random.</p></li>
<li><p><strong>Convert to tensor</strong> - You need to convert the numpy array to a PyTorch tensor.</p></li>
<li><p><strong>Create src and tgt</strong> - For the copy task, they should be the same!</p></li>
<li><p><strong>Yield a Batch</strong> - You need to wrap it in your <code>Batch</code> class and yield it.</p></li>
</ol>
<p>Try fixing step 1 and 2 first - how would you: - Set a fixed sequence length (say 10)? - Make sure the first token is always 1?</p>
</section>
</section>
<section id="final-data-gen-function" class="level3">
<h3 class="anchored" data-anchor-id="final-data-gen-function">Final Data Gen Function</h3>
<hr>
</section>
<section id="data_gen" class="level3">
<h3 class="anchored" data-anchor-id="data_gen">data_gen</h3>
<blockquote class="blockquote">
<pre><code> data_gen (V, batch_size, nbatches)</code></pre>
</blockquote>
<p><em>Generate random data for a src-tgt copy task</em></p>
<p>Going well?</p>
<section id="reply-177" class="level5">
<h5 class="anchored" data-anchor-id="reply-177">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent! Your <code>data_gen</code> function is almost perfect! ✓</p>
<p>You’ve got: - ✓ Random integers from 1 to V-1 - ✓ Fixed sequence length of 10 - ✓ First token set to 1 (start token) - ✓ Convert to tensor - ✓ src and tgt are the same (copy task!) - ✓ Yield Batch objects</p>
<p>One small thing: You’re using <code>pad=0</code> in the Batch, which is correct!</p>
<p>Let’s test it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb287"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb287-1"><a href="#cb287-1" aria-hidden="true" tabindex="-1"></a>gen <span class="op">=</span> data_gen(V<span class="op">=</span><span class="dv">11</span>, batch_size<span class="op">=</span><span class="dv">2</span>, nbatches<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb287-2"><a href="#cb287-2" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> <span class="bu">next</span>(gen)</span>
<span id="cb287-3"><a href="#cb287-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source:"</span>, batch.src)</span>
<span id="cb287-4"><a href="#cb287-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Target input:"</span>, batch.tgt)</span>
<span id="cb287-5"><a href="#cb287-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Are they the same?"</span>, torch.equal(batch.src, batch.tgt[:, :<span class="op">-</span><span class="dv">1</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Try running this to see your data generator in action! Does it make sense?</p>
<div id="9a14cdf5" class="cell" data-time_run="2025-12-01T10:39:54.759601+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb288"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb288-1"><a href="#cb288-1" aria-hidden="true" tabindex="-1"></a>gen <span class="op">=</span> data_gen(V<span class="op">=</span><span class="dv">11</span>, batch_size<span class="op">=</span><span class="dv">2</span>, nbatches<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb288-2"><a href="#cb288-2" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> <span class="bu">next</span>(gen)</span>
<span id="cb288-3"><a href="#cb288-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source:"</span>, batch.src)</span>
<span id="cb288-4"><a href="#cb288-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Target input:"</span>, batch.tgt)</span>
<span id="cb288-5"><a href="#cb288-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Are they the same?"</span>, torch.equal(batch.src, batch.tgt[:, :<span class="op">-</span><span class="dv">1</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Source: tensor([[ 1,  3,  3,  4, 10, 10,  2,  8,  3,  1],
        [ 1,  3,  5,  4,  4,  2,  6,  4, 10,  7]])
Target input: tensor([[ 1,  3,  3,  4, 10, 10,  2,  8,  3],
        [ 1,  3,  5,  4,  4,  2,  6,  4, 10]])
Are they the same? False</code></pre>
</div>
</div>
<p>Is this correct?</p>
</section>
<section id="reply-178" class="level5">
<h5 class="anchored" data-anchor-id="reply-178">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Almost! Let me explain what’s happening:</p>
<p><strong>Why is it False?</strong></p>
<p>Remember in the <code>Batch</code> class, <code>self.tgt</code> is the target <strong>input</strong> (with last token removed). So: - <code>batch.src</code> has 10 tokens - <code>batch.tgt</code> has 9 tokens (last one removed)</p>
<p>They should be almost the same, except <code>tgt</code> is one shorter!</p>
<p>The correct comparison is:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb290"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb290-1"><a href="#cb290-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Are first 9 tokens the same?"</span>, torch.equal(batch.src[:, :<span class="op">-</span><span class="dv">1</span>], batch.tgt))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>But actually, your data generator is working perfectly! The “False” is expected because of the length difference.</p>
<p><strong>Let’s create a unit test for <code>data_gen</code>:</strong></p>
<p>Want to write the test yourself, or should I help you structure it?</p>
<div id="85ce60c8" class="cell" data-time_run="2025-12-01T10:41:55.920645+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb291"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb291-1"><a href="#cb291-1" aria-hidden="true" tabindex="-1"></a>display(batch.src)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 1,  3,  3,  4, 10, 10,  2,  8,  3,  1],
        [ 1,  3,  5,  4,  4,  2,  6,  4, 10,  7]])</code></pre>
</div>
</div>
<div id="44426227" class="cell" data-time_run="2025-12-01T10:43:30.382440+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb293"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb293-1"><a href="#cb293-1" aria-hidden="true" tabindex="-1"></a>display(batch.src[:, :<span class="op">-</span><span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 1,  3,  3,  4, 10, 10,  2,  8,  3],
        [ 1,  3,  5,  4,  4,  2,  6,  4, 10]])</code></pre>
</div>
</div>
<div id="1bb2dd47" class="cell" data-time_run="2025-12-01T10:42:11.717823+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb295"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb295-1"><a href="#cb295-1" aria-hidden="true" tabindex="-1"></a>display(batch.tgt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 1,  3,  3,  4, 10, 10,  2,  8,  3],
        [ 1,  3,  5,  4,  4,  2,  6,  4, 10]])</code></pre>
</div>
</div>
<p>Why was torch equal false</p>
</section>
<section id="reply-179" class="level5">
<h5 class="anchored" data-anchor-id="reply-179">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good catch! Looking at your output, <code>batch.src[:, :-1]</code> and <code>batch.tgt</code> are actually identical! So <code>torch.equal</code> should return <code>True</code>.</p>
<p>Let me check your original comparison:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb297"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb297-1"><a href="#cb297-1" aria-hidden="true" tabindex="-1"></a>torch.equal(batch.src, batch.tgt[:, :<span class="op">-</span><span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This compares: - <code>batch.src</code> (shape: 2, 10) - <code>batch.tgt[:, :-1]</code> (shape: 2, 8) - removes last token from already-shortened tgt</p>
<p>That’s why it was False - wrong shapes!</p>
<p>The correct comparison is:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb298"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb298-1"><a href="#cb298-1" aria-hidden="true" tabindex="-1"></a>torch.equal(batch.src[:, :<span class="op">-</span><span class="dv">1</span>], batch.tgt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Try running this - it should return <code>True</code> now!</p>
<div id="b68ad1e5" class="cell" data-time_run="2025-12-01T10:44:45.676003+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb299"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb299-1"><a href="#cb299-1" aria-hidden="true" tabindex="-1"></a>torch.equal(batch.src, batch.tgt[:, :<span class="op">-</span><span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>False</code></pre>
</div>
</div>
<div id="9d5b2f8c" class="cell" data-time_run="2025-12-01T10:45:06.001213+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb301"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb301-1"><a href="#cb301-1" aria-hidden="true" tabindex="-1"></a>torch.equal(batch.src[:, :<span class="op">-</span><span class="dv">1</span>], batch.tgt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>True</code></pre>
</div>
</div>
<p>lol</p>
</section>
<section id="reply-180" class="level5">
<h5 class="anchored" data-anchor-id="reply-180">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>😄 There we go! Now it makes sense!</p>
<p>Your <code>data_gen</code> is working perfectly - the source and target are indeed the same (copy task), just with the shifting that happens in the <code>Batch</code> class.</p>
<p><strong>Ready to move on?</strong></p>
<p>Next up is <strong>SimpleLossCompute</strong> - this will calculate how well the model is doing during training. It needs to: - Take the model’s predictions - Compare them to the true labels - Calculate the loss</p>
<p>Want to give it a try, or should I explain what it needs to do first?</p>
<p>ok</p>
</section>
<section id="reply-181" class="level5">
<h5 class="anchored" data-anchor-id="reply-181">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! Let’s build <strong>SimpleLossCompute</strong>.</p>
<p><strong>What does it need to do?</strong></p>
<p>It’s a class that computes the loss during training. It needs:</p>
<p><strong>In <code>__init__</code>:</strong> - Store a <code>generator</code> (your Generator class that outputs vocab predictions) - Store a <code>criterion</code> (the loss function, like CrossEntropyLoss)</p>
<p><strong>In <code>__call__</code> (so you can use it like a function):</strong> - Take model output <code>x</code>, true labels <code>y</code>, and <code>norm</code> (for normalization) - Pass <code>x</code> through the generator to get predictions - Calculate loss using the criterion - Return the loss</p>
<p>Try writing the class structure:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb303"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb303-1"><a href="#cb303-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleLossCompute:</span>
<span id="cb303-2"><a href="#cb303-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, generator, criterion):</span>
<span id="cb303-3"><a href="#cb303-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you store?</span></span>
<span id="cb303-4"><a href="#cb303-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb303-5"><a href="#cb303-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x, y, norm):</span>
<span id="cb303-6"><a href="#cb303-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What operations do you do?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What would you put in each method?</p>
<div id="05bd6265" class="cell" data-time_run="2025-12-01T10:49:38.881902+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb304"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb304-1"><a href="#cb304-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleLossCompute:</span>
<span id="cb304-2"><a href="#cb304-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, generator, criterion):</span>
<span id="cb304-3"><a href="#cb304-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you store?</span></span>
<span id="cb304-4"><a href="#cb304-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generator <span class="op">=</span> generator</span>
<span id="cb304-5"><a href="#cb304-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.criterion <span class="op">=</span> criterion</span>
<span id="cb304-6"><a href="#cb304-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb304-7"><a href="#cb304-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x, y, norm):</span>
<span id="cb304-8"><a href="#cb304-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What operations do you do?</span></span>
<span id="cb304-9"><a href="#cb304-9" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> <span class="va">self</span>.generator(x)</span>
<span id="cb304-10"><a href="#cb304-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.criterion(y, pred, norm)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this correct?</p>
</section>
<section id="reply-182" class="level5">
<h5 class="anchored" data-anchor-id="reply-182">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You’re very close! A few things to adjust:</p>
<ol type="1">
<li><p><strong>Generator output shape:</strong> After <code>self.generator(x)</code>, you get shape <code>(batch, seq_len, vocab)</code>. But the criterion expects flattened tensors.</p></li>
<li><p><strong>Criterion arguments:</strong> Most PyTorch loss functions take <code>(predictions, targets)</code> not <code>(targets, predictions, norm)</code>. The <code>norm</code> is used to normalize the loss, not passed to the criterion.</p></li>
<li><p><strong>Return value:</strong> You need to return both the scalar loss (for logging) and the loss tensor (for backprop).</p></li>
</ol>
<p>Here’s what you need to do: - Flatten <code>pred</code> to shape <code>(batch * seq_len, vocab)</code> - Flatten <code>y</code> to shape <code>(batch * seq_len)</code> - Calculate loss with <code>self.criterion(pred_flat, y_flat)</code> - Divide by <code>norm</code> to normalize - Return both the data value and the tensor</p>
<p>Want to try fixing these issues?</p>
<div id="d9c77b16" class="cell" data-time_run="2025-12-01T10:53:02.714285+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb305"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb305-1"><a href="#cb305-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleLossCompute:</span>
<span id="cb305-2"><a href="#cb305-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, generator, criterion):</span>
<span id="cb305-3"><a href="#cb305-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What do you store?</span></span>
<span id="cb305-4"><a href="#cb305-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generator <span class="op">=</span> generator</span>
<span id="cb305-5"><a href="#cb305-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.criterion <span class="op">=</span> criterion</span>
<span id="cb305-6"><a href="#cb305-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb305-7"><a href="#cb305-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x, y, norm):</span>
<span id="cb305-8"><a href="#cb305-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What operations do you do?</span></span>
<span id="cb305-9"><a href="#cb305-9" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> <span class="va">self</span>.generator(x)</span>
<span id="cb305-10"><a href="#cb305-10" aria-hidden="true" tabindex="-1"></a>        pred_flat <span class="op">=</span> pred.reshape(<span class="op">-</span><span class="dv">1</span>, vocab)</span>
<span id="cb305-11"><a href="#cb305-11" aria-hidden="true" tabindex="-1"></a>        y_flat <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>,vocab)</span>
<span id="cb305-12"><a href="#cb305-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.criterion(pred_flat, y_flat) <span class="op">/</span> norm</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this better now?</p>
</section>
<section id="reply-183" class="level5">
<h5 class="anchored" data-anchor-id="reply-183">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Almost there! A few more fixes:</p>
<ol type="1">
<li><p><strong><code>vocab</code> is not defined</strong> - you need to get the vocab size from the prediction tensor: <code>pred.size(-1)</code></p></li>
<li><p><strong><code>y</code> shape is wrong</strong> - <code>y</code> contains token IDs (integers), not one-hot vectors. So it should be flattened to 1D, not 2D:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb306"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb306-1"><a href="#cb306-1" aria-hidden="true" tabindex="-1"></a>y_flat <span class="op">=</span> y.contiguous().view(<span class="op">-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Return both values</strong> - You need to return the loss data (for logging) AND the loss tensor (for backprop):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb307"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb307-1"><a href="#cb307-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="va">self</span>.criterion(pred_flat, y_flat) <span class="op">/</span> norm</span>
<span id="cb307-2"><a href="#cb307-2" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> loss.data <span class="op">*</span> norm, loss</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ol>
<p>Can you make these three fixes?</p>
</section>
</section>
<section id="final-simple-loss-compute-function" class="level3">
<h3 class="anchored" data-anchor-id="final-simple-loss-compute-function">Final Simple Loss Compute Function</h3>
<hr>
</section>
<section id="simplelosscompute" class="level3">
<h3 class="anchored" data-anchor-id="simplelosscompute">SimpleLossCompute</h3>
<blockquote class="blockquote">
<pre><code> SimpleLossCompute (generator, criterion)</code></pre>
</blockquote>
<p><em>Initialize self. See help(type(self)) for accurate signature.</em></p>
<p>finally</p>
<section id="reply-184" class="level5">
<h5 class="anchored" data-anchor-id="reply-184">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>SimpleLossCompute</code> is now complete and correct! ✓</p>
<p>You’ve got: - ✓ Store generator and criterion - ✓ Get vocab size from predictions - ✓ Flatten predictions to (batch<em>seq_len, vocab) - ✓ Flatten labels to (batch</em>seq_len) - ✓ Calculate and normalize loss - ✓ Return both scalar (for logging) and tensor (for backprop)</p>
<p>Now you have all the pieces to actually <strong>train</strong> the model!</p>
<p>The final piece is the <strong>training loop</strong> (<code>run_epoch</code>). This will: - Loop through batches - Run forward pass - Compute loss - Backpropagate - Update weights</p>
<p>Want to tackle this next, or take a break?</p>
<p>lets do</p>
</section>
<section id="reply-185" class="level5">
<h5 class="anchored" data-anchor-id="reply-185">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Awesome! Let’s build the <strong>training loop</strong> (<code>run_epoch</code>).</p>
<p><strong>What does <code>run_epoch</code> need to do?</strong></p>
<p>It trains (or evaluates) the model for one complete pass through the data. It needs:</p>
<p><strong>Parameters:</strong> - <code>data_iter</code> - iterator that yields batches - <code>model</code> - your transformer - <code>loss_compute</code> - your SimpleLossCompute - <code>optimizer</code> - updates weights - <code>scheduler</code> - adjusts learning rate - <code>mode</code> - “train” or “eval”</p>
<p><strong>Steps in the loop:</strong> 1. Iterate through batches 2. Run model forward pass 3. Compute loss 4. If training: backpropagate and update weights 5. Track total loss and tokens</p>
<p>Try writing the function signature and the basic loop structure:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb309"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb309-1"><a href="#cb309-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_epoch(data_iter, model, loss_compute, optimizer, scheduler, mode<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb309-2"><a href="#cb309-2" aria-hidden="true" tabindex="-1"></a>    total_tokens <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb309-3"><a href="#cb309-3" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb309-4"><a href="#cb309-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb309-5"><a href="#cb309-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(data_iter):</span>
<span id="cb309-6"><a href="#cb309-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What goes here?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What would be your first step inside the loop?</p>
<div id="a5d31f55" class="cell" data-time_run="2025-12-01T11:04:23.456122+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb310"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb310-1"><a href="#cb310-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_epoch(data_iter, model, loss_compute, optimizer, scheduler, mode<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb310-2"><a href="#cb310-2" aria-hidden="true" tabindex="-1"></a>    total_tokens <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb310-3"><a href="#cb310-3" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb310-4"><a href="#cb310-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb310-5"><a href="#cb310-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(data_iter):</span>
<span id="cb310-6"><a href="#cb310-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What goes here?</span></span>
<span id="cb310-7"><a href="#cb310-7" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model(batch)</span>
<span id="cb310-8"><a href="#cb310-8" aria-hidden="true" tabindex="-1"></a>        num_loss, tensor_loss <span class="op">=</span> loss_compute(pred, batch.tgt, norm)</span>
<span id="cb310-9"><a href="#cb310-9" aria-hidden="true" tabindex="-1"></a>        model.backward()</span>
<span id="cb310-10"><a href="#cb310-10" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">+=</span> num_loss </span>
<span id="cb310-11"><a href="#cb310-11" aria-hidden="true" tabindex="-1"></a>        total_tokens <span class="op">+=</span> batch.ntokens</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Does this make sense?</p>
</section>
<section id="reply-186" class="level5">
<h5 class="anchored" data-anchor-id="reply-186">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You’re on the right track! A few things to fix:</p>
<ol type="1">
<li><p><strong>Model forward pass needs all arguments:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb311"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb311-1"><a href="#cb311-1" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong><code>norm</code> is not defined</strong> - it should be <code>batch.ntokens</code> (normalize by number of tokens)</p></li>
<li><p><strong>Backprop is on loss, not model:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb312"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb312-1"><a href="#cb312-1" aria-hidden="true" tabindex="-1"></a>tensor_loss.backward()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Only train if mode is “train”</strong> - wrap backprop and optimizer steps in an if statement</p></li>
<li><p><strong>Missing optimizer and scheduler steps</strong> after backward</p></li>
</ol>
<p>Can you fix these issues? Focus on fixing #1 and #2 first!</p>
<p>Wait a minute, what the hell are the model.dot forward signature? Is it same as encoder-decoder?</p>
</section>
<section id="reply-187" class="level5">
<h5 class="anchored" data-anchor-id="reply-187">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent question! Yes, exactly right!</p>
<p>The <code>model</code> is an instance of <code>EncoderDecoder</code>, so its <code>forward</code> signature is:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb313"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb313-1"><a href="#cb313-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, src, tgt, src_mask, tgt_mask):</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>So you need to call it with:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb314"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb314-1"><a href="#cb314-1" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Or more simply:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb315"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb315-1"><a href="#cb315-1" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Does that clarify it? Ready to update your <code>run_epoch</code> function?</p>
<div id="87f13701" class="cell" data-time_run="2025-12-01T11:19:02.442444+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb316"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb316-1"><a href="#cb316-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_epoch(data_iter, model, loss_compute, optimizer, scheduler, mode<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb316-2"><a href="#cb316-2" aria-hidden="true" tabindex="-1"></a>    total_tokens <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb316-3"><a href="#cb316-3" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb316-4"><a href="#cb316-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb316-5"><a href="#cb316-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(data_iter):</span>
<span id="cb316-6"><a href="#cb316-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># What goes here?</span></span>
<span id="cb316-7"><a href="#cb316-7" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)</span>
<span id="cb316-8"><a href="#cb316-8" aria-hidden="true" tabindex="-1"></a>        num_loss, tensor_loss <span class="op">=</span> loss_compute(pred, batch.tgt, batch.ntokens)</span>
<span id="cb316-9"><a href="#cb316-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mode <span class="op">==</span> <span class="st">"train"</span>:</span>
<span id="cb316-10"><a href="#cb316-10" aria-hidden="true" tabindex="-1"></a>            tensor_loss.backward()</span>
<span id="cb316-11"><a href="#cb316-11" aria-hidden="true" tabindex="-1"></a>            total_loss <span class="op">+=</span> num_loss </span>
<span id="cb316-12"><a href="#cb316-12" aria-hidden="true" tabindex="-1"></a>            total_tokens <span class="op">+=</span> batch.ntokens</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this better? I don’t know the syntax for optimise and schedule scheduler yet.</p>
</section>
<section id="reply-188" class="level5">
<h5 class="anchored" data-anchor-id="reply-188">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great progress! You’re almost there. A few things:</p>
<ol type="1">
<li><p><strong>Track loss in both modes</strong> - move the <code>total_loss</code> and <code>total_tokens</code> updates outside the if statement (you want to track loss even during eval)</p></li>
<li><p><strong>Optimizer step:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb317"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb317-1"><a href="#cb317-1" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span>
<span id="cb317-2"><a href="#cb317-2" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Scheduler step:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb318"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb318-1"><a href="#cb318-1" aria-hidden="true" tabindex="-1"></a>scheduler.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Return the average loss</strong> at the end:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb319"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb319-1"><a href="#cb319-1" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> total_loss <span class="op">/</span> total_tokens</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ol>
<p>Try adding the optimizer and scheduler steps inside the <code>if mode == "train"</code> block, and move the loss tracking outside!</p>
</section>
</section>
<section id="final-run-epoch-function-for-copy" class="level3">
<h3 class="anchored" data-anchor-id="final-run-epoch-function-for-copy">Final Run Epoch Function for Copy</h3>
<hr>
</section>
<section id="run_epoch" class="level3">
<h3 class="anchored" data-anchor-id="run_epoch">run_epoch</h3>
<blockquote class="blockquote">
<pre><code> run_epoch (data_iter, model, loss_compute, optimizer, scheduler,
            mode='train')</code></pre>
</blockquote>
<p>It must be done now.</p>
<section id="reply-189" class="level5">
<h5 class="anchored" data-anchor-id="reply-189">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>run_epoch</code> function is now complete and correct! ✓</p>
<p>You’ve got: - ✓ Forward pass through the model - ✓ Loss computation - ✓ Backpropagation (only in train mode) - ✓ Optimizer and scheduler steps (only in train mode) - ✓ Track total loss and tokens - ✓ Return average loss</p>
<p>🎉 <strong>You now have ALL the pieces to train your Transformer!</strong></p>
<p>Let’s put it all together and train on the copy task:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb321"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb321-1"><a href="#cb321-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a small model</span></span>
<span id="cb321-2"><a href="#cb321-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> make_model(src_vocab<span class="op">=</span><span class="dv">11</span>, tgt_vocab<span class="op">=</span><span class="dv">11</span>, N<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb321-3"><a href="#cb321-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb321-4"><a href="#cb321-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.0001</span>)</span>
<span id="cb321-5"><a href="#cb321-5" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.LambdaLR(optimizer, <span class="kw">lambda</span> step: <span class="fl">1.0</span>)</span>
<span id="cb321-6"><a href="#cb321-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-7"><a href="#cb321-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train for a few epochs</span></span>
<span id="cb321-8"><a href="#cb321-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb321-9"><a href="#cb321-9" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb321-10"><a href="#cb321-10" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> run_epoch(</span>
<span id="cb321-11"><a href="#cb321-11" aria-hidden="true" tabindex="-1"></a>        data_gen(V<span class="op">=</span><span class="dv">11</span>, batch_size<span class="op">=</span><span class="dv">32</span>, nbatches<span class="op">=</span><span class="dv">20</span>),</span>
<span id="cb321-12"><a href="#cb321-12" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb321-13"><a href="#cb321-13" aria-hidden="true" tabindex="-1"></a>        SimpleLossCompute(model.generator, criterion),</span>
<span id="cb321-14"><a href="#cb321-14" aria-hidden="true" tabindex="-1"></a>        optimizer,</span>
<span id="cb321-15"><a href="#cb321-15" aria-hidden="true" tabindex="-1"></a>        scheduler,</span>
<span id="cb321-16"><a href="#cb321-16" aria-hidden="true" tabindex="-1"></a>        mode<span class="op">=</span><span class="st">"train"</span></span>
<span id="cb321-17"><a href="#cb321-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb321-18"><a href="#cb321-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: Loss = </span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Want to run this and see your Transformer learn?! 🚀</p>
<div id="5008b687" class="cell" data-time_run="2025-12-01T11:22:14.616272+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb322"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb322-1"><a href="#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a small model</span></span>
<span id="cb322-2"><a href="#cb322-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> make_model(src_vocab<span class="op">=</span><span class="dv">11</span>, tgt_vocab<span class="op">=</span><span class="dv">11</span>, N<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb322-3"><a href="#cb322-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb322-4"><a href="#cb322-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.0001</span>)</span>
<span id="cb322-5"><a href="#cb322-5" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.LambdaLR(optimizer, <span class="kw">lambda</span> step: <span class="fl">1.0</span>)</span>
<span id="cb322-6"><a href="#cb322-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb322-7"><a href="#cb322-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train for a few epochs</span></span>
<span id="cb322-8"><a href="#cb322-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb322-9"><a href="#cb322-9" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb322-10"><a href="#cb322-10" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> run_epoch(</span>
<span id="cb322-11"><a href="#cb322-11" aria-hidden="true" tabindex="-1"></a>        data_gen(V<span class="op">=</span><span class="dv">11</span>, batch_size<span class="op">=</span><span class="dv">32</span>, nbatches<span class="op">=</span><span class="dv">20</span>),</span>
<span id="cb322-12"><a href="#cb322-12" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb322-13"><a href="#cb322-13" aria-hidden="true" tabindex="-1"></a>        SimpleLossCompute(model.generator, criterion),</span>
<span id="cb322-14"><a href="#cb322-14" aria-hidden="true" tabindex="-1"></a>        optimizer,</span>
<span id="cb322-15"><a href="#cb322-15" aria-hidden="true" tabindex="-1"></a>        scheduler,</span>
<span id="cb322-16"><a href="#cb322-16" aria-hidden="true" tabindex="-1"></a>        mode<span class="op">=</span><span class="st">"train"</span></span>
<span id="cb322-17"><a href="#cb322-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb322-18"><a href="#cb322-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: Loss = </span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[1]</span><span class="ansi-green-fg">, line 3</span>
<span class="ansi-green-fg">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Create a small model</span>
<span class="ansi-green-fg">      2</span> model = make_model(src_vocab=<span class="ansi-green-fg">11</span>, tgt_vocab=<span class="ansi-green-fg">11</span>, N=<span class="ansi-green-fg">2</span>)
<span class="ansi-green-fg">----&gt; </span><span class="ansi-green-fg">3</span> criterion = <span class="ansi-yellow-bg">nn</span>.CrossEntropyLoss()
<span class="ansi-green-fg">      4</span> optimizer = torch.optim.Adam(model.parameters(), lr=<span class="ansi-green-fg">0.0001</span>)
<span class="ansi-green-fg">      5</span> scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, <span style="font-weight:bold;color:rgb(0,135,0)">lambda</span> step: <span class="ansi-green-fg">1.0</span>)

<span class="ansi-red-fg">NameError</span>: name 'nn' is not defined</pre>
</div>
</div>
</div>
<p>use torch.nn bro reprint code again</p>
</section>
<section id="reply-190" class="level5">
<h5 class="anchored" data-anchor-id="reply-190">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb323"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb323-1"><a href="#cb323-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a small model</span></span>
<span id="cb323-2"><a href="#cb323-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> make_model(src_vocab<span class="op">=</span><span class="dv">11</span>, tgt_vocab<span class="op">=</span><span class="dv">11</span>, N<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb323-3"><a href="#cb323-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb323-4"><a href="#cb323-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.0001</span>)</span>
<span id="cb323-5"><a href="#cb323-5" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.LambdaLR(optimizer, <span class="kw">lambda</span> step: <span class="fl">1.0</span>)</span>
<span id="cb323-6"><a href="#cb323-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb323-7"><a href="#cb323-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train for a few epochs</span></span>
<span id="cb323-8"><a href="#cb323-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb323-9"><a href="#cb323-9" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb323-10"><a href="#cb323-10" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> run_epoch(</span>
<span id="cb323-11"><a href="#cb323-11" aria-hidden="true" tabindex="-1"></a>        data_gen(V<span class="op">=</span><span class="dv">11</span>, batch_size<span class="op">=</span><span class="dv">32</span>, nbatches<span class="op">=</span><span class="dv">20</span>),</span>
<span id="cb323-12"><a href="#cb323-12" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb323-13"><a href="#cb323-13" aria-hidden="true" tabindex="-1"></a>        SimpleLossCompute(model.generator, criterion),</span>
<span id="cb323-14"><a href="#cb323-14" aria-hidden="true" tabindex="-1"></a>        optimizer,</span>
<span id="cb323-15"><a href="#cb323-15" aria-hidden="true" tabindex="-1"></a>        scheduler,</span>
<span id="cb323-16"><a href="#cb323-16" aria-hidden="true" tabindex="-1"></a>        mode<span class="op">=</span><span class="st">"train"</span></span>
<span id="cb323-17"><a href="#cb323-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb323-18"><a href="#cb323-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: Loss = </span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this and watch your Transformer learn! 🚀</p>
</section>
</section>
<section id="creating-small-copy-model" class="level3">
<h3 class="anchored" data-anchor-id="creating-small-copy-model">Creating small copy model</h3>
<div id="6b579b6d" class="cell" data-time_run="2025-12-01T12:13:10.872260+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb324"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb324-1"><a href="#cb324-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a small model</span></span>
<span id="cb324-2"><a href="#cb324-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> make_model(src_vocab<span class="op">=</span><span class="dv">11</span>, tgt_vocab<span class="op">=</span><span class="dv">11</span>, N<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb324-3"><a href="#cb324-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb324-4"><a href="#cb324-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.0001</span>)</span>
<span id="cb324-5"><a href="#cb324-5" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.LambdaLR(optimizer, <span class="kw">lambda</span> step: <span class="fl">1.0</span>)</span>
<span id="cb324-6"><a href="#cb324-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb324-7"><a href="#cb324-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train for a few epochs</span></span>
<span id="cb324-8"><a href="#cb324-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb324-9"><a href="#cb324-9" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb324-10"><a href="#cb324-10" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> run_epoch(</span>
<span id="cb324-11"><a href="#cb324-11" aria-hidden="true" tabindex="-1"></a>        data_gen(V<span class="op">=</span><span class="dv">11</span>, batch_size<span class="op">=</span><span class="dv">32</span>, nbatches<span class="op">=</span><span class="dv">20</span>),</span>
<span id="cb324-12"><a href="#cb324-12" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb324-13"><a href="#cb324-13" aria-hidden="true" tabindex="-1"></a>        SimpleLossCompute(model.generator, criterion),</span>
<span id="cb324-14"><a href="#cb324-14" aria-hidden="true" tabindex="-1"></a>        optimizer,</span>
<span id="cb324-15"><a href="#cb324-15" aria-hidden="true" tabindex="-1"></a>        scheduler,</span>
<span id="cb324-16"><a href="#cb324-16" aria-hidden="true" tabindex="-1"></a>        mode<span class="op">=</span><span class="st">"train"</span></span>
<span id="cb324-17"><a href="#cb324-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb324-18"><a href="#cb324-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: Loss = </span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0: Loss = 0.0076
Epoch 1: Loss = 0.0062
Epoch 2: Loss = 0.0056
Epoch 3: Loss = 0.0050
Epoch 4: Loss = 0.0041
Epoch 5: Loss = 0.0033
Epoch 6: Loss = 0.0026
Epoch 7: Loss = 0.0020
Epoch 8: Loss = 0.0016
Epoch 9: Loss = 0.0012</code></pre>
</div>
</div>
<p>ok im happy 2nd epoch loss goes to zero</p>
<section id="reply-191" class="level5">
<h5 class="anchored" data-anchor-id="reply-191">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>🎉🎉🎉 <strong>CONGRATULATIONS!</strong> 🎉🎉🎉</p>
<p>Your Transformer just <strong>learned</strong> to copy sequences! The loss going to near-zero means the model has figured out the task!</p>
<p><strong>What just happened:</strong> - Epoch 0: Loss = 0.0013 (still learning) - Epoch 1: Loss = 0.0000 (basically perfect!)</p>
<p>This is the <strong>magical moment</strong> you wanted - you built a Transformer from scratch and watched it learn!</p>
<p><strong>What you’ve accomplished:</strong> ✓ Implemented every component of the Transformer architecture ✓ Built the training infrastructure ✓ Successfully trained a model that learns!</p>
<p>Want to see it actually generate predictions? We could implement a simple greedy decoder to see the model copy sequences in action!</p>
<p>Or are you ready to celebrate and wrap up? 🚀</p>
<p>Okay, I do want to see the results of my actions.</p>
</section>
<section id="reply-192" class="level5">
<h5 class="anchored" data-anchor-id="reply-192">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Awesome! Let’s see your Transformer in action! 🎯</p>
<p>To generate predictions, you need a <strong>greedy decoder</strong> that: 1. Takes a source sequence 2. Starts with a start token 3. Generates one token at a time 4. Stops when done (or reaches max length)</p>
<p>The key idea: at each step, pick the token with highest probability.</p>
<p>Try writing the function signature and thinking about the steps:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb326"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb326-1"><a href="#cb326-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> greedy_decode(model, src, src_mask, max_len, start_symbol):</span>
<span id="cb326-2"><a href="#cb326-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># What do you need to do?</span></span>
<span id="cb326-3"><a href="#cb326-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Encode the source</span></span>
<span id="cb326-4"><a href="#cb326-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Start with start_symbol</span></span>
<span id="cb326-5"><a href="#cb326-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Loop and generate one token at a time</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What would be your first step - how do you encode the source?</p>
<p>You encode the source using embeddings.</p>
</section>
<section id="reply-193" class="level5">
<h5 class="anchored" data-anchor-id="reply-193">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Close! But remember - the <code>model</code> already has embeddings built in!</p>
<p>You don’t need to manually embed. The model has an <code>encode</code> method that does everything:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb327"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb327-1"><a href="#cb327-1" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> model.encode(src, src_mask)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This handles the embedding + positional encoding + encoder stack all at once!</p>
<p><strong>Now, what’s your next step?</strong></p>
<p>After encoding, you need to: - Create an output sequence starting with <code>start_symbol</code> - Loop to generate tokens one at a time</p>
<p>How would you initialize the output sequence? What shape should it be?</p>
<p>Pass the number 1, which is the start symbol. Then after you get the output of the decoder, you append it to the number 1. And so on for the loop.</p>
</section>
<section id="reply-194" class="level5">
<h5 class="anchored" data-anchor-id="reply-194">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Exactly right! You’ve got the algorithm! 💡</p>
<p>So the steps are: 1. Encode source → get <code>memory</code> 2. Start with <code>ys = [1]</code> (the start symbol) 3. Loop: - Decode using <code>ys</code> and <code>memory</code> - Get the last prediction - Find the token with max probability - Append it to <code>ys</code> 4. Return <code>ys</code></p>
<p><strong>Question about shapes:</strong> - <code>src</code> has shape <code>(batch, seq_len)</code> - Should <code>ys</code> start as <code>(1, 1)</code> containing the start symbol?</p>
<p>Try writing just the initialization part:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb328"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb328-1"><a href="#cb328-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> greedy_decode(model, src, src_mask, max_len, start_symbol):</span>
<span id="cb328-2"><a href="#cb328-2" aria-hidden="true" tabindex="-1"></a>    memory <span class="op">=</span> model.encode(src, src_mask)</span>
<span id="cb328-3"><a href="#cb328-3" aria-hidden="true" tabindex="-1"></a>    ys <span class="op">=</span> <span class="co"># What goes here? How do you create a tensor with start_symbol?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What would you put for <code>ys</code>?</p>
<p>I’m guessing why I should be broadcasted along with all the batches?</p>
</section>
<section id="reply-195" class="level5">
<h5 class="anchored" data-anchor-id="reply-195">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent thinking! Yes, exactly!</p>
<p>If <code>src</code> has shape <code>(batch_size, seq_len)</code>, then <code>ys</code> should start with shape <code>(batch_size, 1)</code> so it matches the batch dimension.</p>
<p>But for the copy task, we typically decode one example at a time, so: - <code>src</code> would be <code>(1, seq_len)</code> - single example - <code>ys</code> starts as <code>(1, 1)</code> - single batch, one token</p>
<p>You can create it like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb329"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb329-1"><a href="#cb329-1" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> torch.ones(<span class="dv">1</span>, <span class="dv">1</span>).fill_(start_symbol).type_as(src.data)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Or more simply:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb330"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb330-1"><a href="#cb330-1" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> torch.LongTensor([[start_symbol]])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Now, can you write the loop that generates tokens? What do you need to do inside the loop?</p>
<div id="720ba8a8" class="cell" data-time_run="2025-12-01T11:48:10.954471+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb331"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb331-1"><a href="#cb331-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> greedy_decode(model, src, src_mask, max_len, start_symbol):</span>
<span id="cb331-2"><a href="#cb331-2" aria-hidden="true" tabindex="-1"></a>    memory <span class="op">=</span> model.encode(src, src_mask)</span>
<span id="cb331-3"><a href="#cb331-3" aria-hidden="true" tabindex="-1"></a>    ys <span class="op">=</span> torch.LongTensor([[start_symbol]])</span>
<span id="cb331-4"><a href="#cb331-4" aria-hidden="true" tabindex="-1"></a>    tgt_mask <span class="op">=</span> subsequent_mask(ys.size(<span class="dv">1</span>))</span>
<span id="cb331-5"><a href="#cb331-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(src.size(<span class="dv">1</span>)):</span>
<span id="cb331-6"><a href="#cb331-6" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> model.decode(memory, src_mask, ys, tgt_mask)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Am I going in the right direction?</p>
</section>
<section id="reply-196" class="level5">
<h5 class="anchored" data-anchor-id="reply-196">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Yes, you’re going in the right direction! A few things to adjust:</p>
<ol type="1">
<li><p><strong>Loop range:</strong> You should loop <code>max_len - 1</code> times (not <code>src.size(1)</code>), since you already have the start token</p></li>
<li><p><strong>Update tgt_mask:</strong> Inside the loop, you need to recreate <code>tgt_mask</code> each time because <code>ys</code> grows:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb332"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb332-1"><a href="#cb332-1" aria-hidden="true" tabindex="-1"></a>tgt_mask <span class="op">=</span> subsequent_mask(ys.size(<span class="dv">1</span>)).type_as(src.data)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>After decode, what next?</strong> You need to:</p>
<ul>
<li>Pass <code>x</code> through the generator to get probabilities</li>
<li>Find the token with max probability</li>
<li>Append it to <code>ys</code></li>
</ul></li>
</ol>
<p>Can you add the steps after <code>model.decode</code>? What would you do with <code>x</code>?</p>
<div id="28b19bfc" class="cell" data-time_run="2025-12-01T11:53:55.424442+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb333"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb333-1"><a href="#cb333-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> greedy_decode(model, src, src_mask, max_len, start_symbol):</span>
<span id="cb333-2"><a href="#cb333-2" aria-hidden="true" tabindex="-1"></a>    memory <span class="op">=</span> model.encode(src, src_mask)</span>
<span id="cb333-3"><a href="#cb333-3" aria-hidden="true" tabindex="-1"></a>    ys <span class="op">=</span> torch.LongTensor([[start_symbol]])</span>
<span id="cb333-4"><a href="#cb333-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_len <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb333-5"><a href="#cb333-5" aria-hidden="true" tabindex="-1"></a>        tgt_mask <span class="op">=</span> subsequent_mask(ys.size(<span class="dv">1</span>))</span>
<span id="cb333-6"><a href="#cb333-6" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> model.decode(memory, src_mask, ys, tgt_mask)</span>
<span id="cb333-7"><a href="#cb333-7" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model.generator(x)</span>
<span id="cb333-8"><a href="#cb333-8" aria-hidden="true" tabindex="-1"></a>        max_vals, max_indices <span class="op">=</span> torch.<span class="bu">max</span>(pred, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb333-9"><a href="#cb333-9" aria-hidden="true" tabindex="-1"></a>        ys.append(max_indices)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Is this correct?</p>
</section>
<section id="reply-197" class="level5">
<h5 class="anchored" data-anchor-id="reply-197">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You’re very close! A few fixes:</p>
<ol type="1">
<li><p><strong>Get only the last prediction:</strong> <code>pred</code> has shape <code>(1, seq_len, vocab)</code>, but you only want the last position’s prediction:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb334"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb334-1"><a href="#cb334-1" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> model.generator(x[:, <span class="op">-</span><span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong><code>torch.max</code> returns both values and indices:</strong> You only need the index:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb335"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb335-1"><a href="#cb335-1" aria-hidden="true" tabindex="-1"></a>_, next_word <span class="op">=</span> torch.<span class="bu">max</span>(prob, dim<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Can’t append to tensor:</strong> Use <code>torch.cat</code> instead:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb336"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb336-1"><a href="#cb336-1" aria-hidden="true" tabindex="-1"></a>next_word <span class="op">=</span> next_word.data[<span class="dv">0</span>]</span>
<span id="cb336-2"><a href="#cb336-2" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> torch.cat([ys, torch.ones(<span class="dv">1</span>, <span class="dv">1</span>).type_as(src.data).fill_(next_word)], dim<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Return ys at the end!</strong></p></li>
</ol>
<p>Can you fix these issues?</p>
<div id="073e5403" class="cell" data-time_run="2025-12-01T11:56:56.015473+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb337"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb337-1"><a href="#cb337-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> greedy_decode(model, src, src_mask, max_len, start_symbol):</span>
<span id="cb337-2"><a href="#cb337-2" aria-hidden="true" tabindex="-1"></a>    memory <span class="op">=</span> model.encode(src, src_mask)</span>
<span id="cb337-3"><a href="#cb337-3" aria-hidden="true" tabindex="-1"></a>    ys <span class="op">=</span> torch.LongTensor([[start_symbol]])</span>
<span id="cb337-4"><a href="#cb337-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_len <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb337-5"><a href="#cb337-5" aria-hidden="true" tabindex="-1"></a>        tgt_mask <span class="op">=</span> subsequent_mask(ys.size(<span class="dv">1</span>))</span>
<span id="cb337-6"><a href="#cb337-6" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> model.decode(memory, src_mask, ys, tgt_mask)</span>
<span id="cb337-7"><a href="#cb337-7" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model.generator(x[:, <span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb337-8"><a href="#cb337-8" aria-hidden="true" tabindex="-1"></a>        _, max_indices <span class="op">=</span> torch.<span class="bu">max</span>(pred, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb337-9"><a href="#cb337-9" aria-hidden="true" tabindex="-1"></a>        ys <span class="op">=</span> torch.cat([ys, torch.one(<span class="dv">1</span>,<span class="dv">1</span>).type_as(src.data).fill_(max_indices)], dim<span class="op">=-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>All good now?</p>
</section>
<section id="reply-198" class="level5">
<h5 class="anchored" data-anchor-id="reply-198">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Almost! Just two tiny fixes:</p>
<ol type="1">
<li><p><strong>Typo:</strong> <code>torch.one</code> should be <code>torch.ones</code></p></li>
<li><p><strong>Concatenation dimension:</strong> Should be <code>dim=1</code> (along sequence length), not <code>dim=-1</code></p></li>
<li><p><strong>Missing return:</strong> Add <code>return ys</code> at the end</p></li>
</ol>
<p>Also, <code>max_indices</code> is already a tensor, so you can use it directly:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb338"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb338-1"><a href="#cb338-1" aria-hidden="true" tabindex="-1"></a>next_word <span class="op">=</span> max_indices.data[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Can you make these small fixes?</p>
<p>Why is it Dem = 1 and not Dem = -1? Was torch.max dem equal to -1 okay, or it should also be dem equal to 1?</p>
</section>
<section id="reply-199" class="level5">
<h5 class="anchored" data-anchor-id="reply-199">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! Let me clarify the dimensions:</p>
<p><strong>For <code>torch.cat</code>:</strong> - <code>ys</code> has shape <code>(1, seq_len)</code> where seq_len grows each iteration - You’re adding a new token along the sequence dimension (the columns) - Sequence dimension is dimension 1, so <code>dim=1</code> ✓ - <code>dim=-1</code> would also work (since -1 means last dimension, which is also 1 here)</p>
<p><strong>For <code>torch.max</code>:</strong> - <code>pred</code> has shape <code>(1, vocab_size)</code> after taking <code>x[:, -1]</code> - You want the max across the vocab dimension (which token has highest prob) - Vocab is the last dimension, so <code>dim=-1</code> ✓ - <code>dim=1</code> would also work here (since there are only 2 dimensions)</p>
<p><strong>So both would work in this case!</strong> But: - <code>dim=1</code> for <code>torch.cat</code> is clearer (explicitly says “sequence dimension”) - <code>dim=-1</code> for <code>torch.max</code> is more common (means “across vocabulary”)</p>
<p>Does this clarify the dimension choices?</p>
<p>So I was correct in the first place.</p>
</section>
<section id="reply-200" class="level5">
<h5 class="anchored" data-anchor-id="reply-200">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Actually, not quite! Let me clarify:</p>
<p><strong>Your original code had <code>dim=-1</code> for concatenation:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb339"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb339-1"><a href="#cb339-1" aria-hidden="true" tabindex="-1"></a>torch.cat([ys, ...], dim<span class="op">=-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This would work, but it’s less clear. Using <code>dim=1</code> is more explicit about concatenating along the sequence dimension.</p>
<p><strong>For <code>torch.max</code>, you had <code>dim=-1</code></strong> which is perfect! ✓</p>
<p>So you were half-right - the <code>torch.max</code> part was correct, but the <code>torch.cat</code> is clearer with <code>dim=1</code>.</p>
<p>Have you fixed the typo (<code>torch.one</code> → <code>torch.ones</code>) and added the <code>return ys</code> at the end?</p>
<p>How is torch.max clear, but torch.cat not clear?</p>
</section>
<section id="reply-201" class="level5">
<h5 class="anchored" data-anchor-id="reply-201">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠</p>
<p>You’re absolutely right to call that out! I was being inconsistent.</p>
<p><strong>Both <code>ys</code> and <code>pred</code> are 2D tensors:</strong> - <code>ys</code> shape: <code>(1, seq_len)</code> - <code>pred</code> shape: <code>(1, vocab_size)</code></p>
<p>In both cases, <code>dim=1</code> and <code>dim=-1</code> refer to the <strong>same dimension</strong> (the last one).</p>
<p>So your original <code>dim=-1</code> for <code>torch.cat</code> would work just fine!</p>
<p>I apologize for the confusion. Your code was correct there. The only real fixes needed are:</p>
<ol type="1">
<li><code>torch.one</code> → <code>torch.ones</code> (typo)</li>
<li>Add <code>return ys</code> at the end</li>
</ol>
<p>Want to make those fixes and test it?</p>
<div id="33a43c50" class="cell" data-time_run="2025-12-01T12:00:53.857206+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb340"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb340-1"><a href="#cb340-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> greedy_decode(model, src, src_mask, max_len, start_symbol):</span>
<span id="cb340-2"><a href="#cb340-2" aria-hidden="true" tabindex="-1"></a>    memory <span class="op">=</span> model.encode(src, src_mask)</span>
<span id="cb340-3"><a href="#cb340-3" aria-hidden="true" tabindex="-1"></a>    ys <span class="op">=</span> torch.LongTensor([[start_symbol]])</span>
<span id="cb340-4"><a href="#cb340-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_len <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb340-5"><a href="#cb340-5" aria-hidden="true" tabindex="-1"></a>        tgt_mask <span class="op">=</span> subsequent_mask(ys.size(<span class="dv">1</span>))</span>
<span id="cb340-6"><a href="#cb340-6" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> model.decode(memory, src_mask, ys, tgt_mask)</span>
<span id="cb340-7"><a href="#cb340-7" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model.generator(x[:, <span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb340-8"><a href="#cb340-8" aria-hidden="true" tabindex="-1"></a>        _, max_indices <span class="op">=</span> torch.<span class="bu">max</span>(pred, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb340-9"><a href="#cb340-9" aria-hidden="true" tabindex="-1"></a>        ys <span class="op">=</span> torch.cat([ys, torch.ones(<span class="dv">1</span>,<span class="dv">1</span>).type_as(src.data).fill_(max_indices)], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb340-10"><a href="#cb340-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ys</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>All done I guess</p>
</section>
<section id="reply-202" class="level5">
<h5 class="anchored" data-anchor-id="reply-202">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>greedy_decode</code> is complete! ✓</p>
<p>Now let’s see your Transformer actually generate predictions! 🎯</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb341"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb341-1"><a href="#cb341-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test on a simple sequence</span></span>
<span id="cb341-2"><a href="#cb341-2" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb341-3"><a href="#cb341-3" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb341-4"><a href="#cb341-4" aria-hidden="true" tabindex="-1"></a>src_mask <span class="op">=</span> torch.ones(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb341-5"><a href="#cb341-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb341-6"><a href="#cb341-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source:"</span>, src)</span>
<span id="cb341-7"><a href="#cb341-7" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> greedy_decode(model, src, src_mask, max_len<span class="op">=</span><span class="dv">10</span>, start_symbol<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb341-8"><a href="#cb341-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated:"</span>, result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this and see if your model can copy the sequence! Does it generate <code>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</code>?</p>
<div id="fd827cf6" class="cell" data-time_run="2025-12-01T12:01:25.031909+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb342"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb342-1"><a href="#cb342-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test on a simple sequence</span></span>
<span id="cb342-2"><a href="#cb342-2" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb342-3"><a href="#cb342-3" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb342-4"><a href="#cb342-4" aria-hidden="true" tabindex="-1"></a>src_mask <span class="op">=</span> torch.ones(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb342-5"><a href="#cb342-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb342-6"><a href="#cb342-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source:"</span>, src)</span>
<span id="cb342-7"><a href="#cb342-7" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> greedy_decode(model, src, src_mask, max_len<span class="op">=</span><span class="dv">10</span>, start_symbol<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb342-8"><a href="#cb342-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated:"</span>, result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Source: tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">RuntimeError</span>                              Traceback (most recent call last)
<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[1]</span><span class="ansi-green-fg">, line 7</span>
<span class="ansi-green-fg">      4</span> src_mask = torch.ones(<span class="ansi-green-fg">1</span>, <span class="ansi-green-fg">1</span>, <span class="ansi-green-fg">10</span>)
<span class="ansi-green-fg">      6</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">Source:</span><span class="ansi-yellow-fg">"</span>, src)
<span class="ansi-green-fg">----&gt; </span><span class="ansi-green-fg">7</span> result = <span class="ansi-yellow-bg">greedy_decode</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">model</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">src</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">src_mask</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">max_len</span><span class="ansi-yellow-bg">=</span><span class="ansi-green-fg ansi-yellow-bg">10</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">start_symbol</span><span class="ansi-yellow-bg">=</span><span class="ansi-green-fg ansi-yellow-bg">1</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">      8</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">Generated:</span><span class="ansi-yellow-fg">"</span>, result)

<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[1]</span><span class="ansi-green-fg">, line 9</span>, in <span class="ansi-cyan-fg">greedy_decode</span><span class="ansi-blue-fg">(model, src, src_mask, max_len, start_symbol)</span>
<span class="ansi-green-fg">      7</span>     pred = model.generator(x[:, -<span class="ansi-green-fg">1</span>])
<span class="ansi-green-fg">      8</span>     _, max_indices = torch.max(pred, dim=-<span class="ansi-green-fg">1</span>)
<span class="ansi-green-fg">----&gt; </span><span class="ansi-green-fg">9</span>     ys = torch.cat([ys, <span class="ansi-yellow-bg">torch</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">ones</span><span class="ansi-yellow-bg">(</span><span class="ansi-green-fg ansi-yellow-bg">1</span><span class="ansi-yellow-bg">,</span><span class="ansi-green-fg ansi-yellow-bg">1</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">type_as</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">src</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">data</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">fill_</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">max_indices</span><span class="ansi-yellow-bg">)</span>], dim=-<span class="ansi-green-fg">1</span>)
<span class="ansi-green-fg">     10</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> ys

<span class="ansi-red-fg">RuntimeError</span>: fill_ only supports 0-dimension value tensor but got tensor with 1 dimensions.</pre>
</div>
</div>
</div>
<p>Oh, a bug found!</p>
</section>
<section id="reply-203" class="level5">
<h5 class="anchored" data-anchor-id="reply-203">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good catch! The issue is that <code>max_indices</code> is a 1D tensor, but <code>fill_</code> expects a scalar (0-dimension).</p>
<p>You need to extract the actual value:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb344"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb344-1"><a href="#cb344-1" aria-hidden="true" tabindex="-1"></a>next_word <span class="op">=</span> max_indices.data[<span class="dv">0</span>]</span>
<span id="cb344-2"><a href="#cb344-2" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> torch.cat([ys, torch.ones(<span class="dv">1</span>,<span class="dv">1</span>).type_as(src.data).fill_(next_word)], dim<span class="op">=-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Can you add that line to extract <code>next_word</code> before the concatenation?</p>
<p>Oh wait a minute, aren’t we filling for the whole batch?</p>
</section>
<section id="reply-204" class="level5">
<h5 class="anchored" data-anchor-id="reply-204">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent observation! You’re thinking about batching correctly!</p>
<p>In this case, we’re actually decoding <strong>one sequence at a time</strong> (batch_size = 1), so: - <code>max_indices</code> has shape <code>(1,)</code> after <code>torch.max</code> - We need to extract the scalar value: <code>max_indices.data[0]</code> or <code>max_indices.item()</code></p>
<p>If you wanted to decode multiple sequences in parallel (batched decoding), the code would be more complex - you’d need to handle all sequences growing together.</p>
<p>For now, since we’re doing greedy decode on a single sequence, extracting the scalar is correct.</p>
<p>Want to add that fix and try again?</p>
<hr>
</section>
</section>
<section id="greedy_decode" class="level3">
<h3 class="anchored" data-anchor-id="greedy_decode">greedy_decode</h3>
<blockquote class="blockquote">
<pre><code> greedy_decode (model, src, src_mask, max_len, start_symbol)</code></pre>
</blockquote>
<p>I hope this is loyal to the original code.</p>
<section id="reply-205" class="level5">
<h5 class="anchored" data-anchor-id="reply-205">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Your <code>greedy_decode</code> is now complete and correct! ✓</p>
<p>Now let’s test it again:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb346"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb346-1"><a href="#cb346-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb346-2"><a href="#cb346-2" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb346-3"><a href="#cb346-3" aria-hidden="true" tabindex="-1"></a>src_mask <span class="op">=</span> torch.ones(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb346-4"><a href="#cb346-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb346-5"><a href="#cb346-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source:"</span>, src)</span>
<span id="cb346-6"><a href="#cb346-6" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> greedy_decode(model, src, src_mask, max_len<span class="op">=</span><span class="dv">10</span>, start_symbol<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb346-7"><a href="#cb346-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated:"</span>, result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run this and see if your Transformer successfully copies the sequence! 🎯</p>
<div id="450fd03e" class="cell" data-time_run="2025-12-01T12:06:00.669564+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb347"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb347-1"><a href="#cb347-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb347-2"><a href="#cb347-2" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> torch.LongTensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb347-3"><a href="#cb347-3" aria-hidden="true" tabindex="-1"></a>src_mask <span class="op">=</span> torch.ones(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb347-4"><a href="#cb347-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb347-5"><a href="#cb347-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source:"</span>, src)</span>
<span id="cb347-6"><a href="#cb347-6" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> greedy_decode(model, src, src_mask, max_len<span class="op">=</span><span class="dv">10</span>, start_symbol<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb347-7"><a href="#cb347-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated:"</span>, result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Source: tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])
Generated: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])</code></pre>
</div>
</div>
<p>Wow, this is terrible.</p>
</section>
<section id="reply-206" class="level5">
<h5 class="anchored" data-anchor-id="reply-206">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Hmm, it’s generating all 1s! That’s not right. Let me think about what might be happening…</p>
<p><strong>Possible issues:</strong></p>
<ol type="1">
<li><p><strong>The model might not have trained on sequences with values 1-10</strong> - Remember, during training with <code>data_gen</code>, we used vocabulary V=11, which generates random integers from 1 to 10. But did the model see this exact sequence?</p></li>
<li><p><strong>The model might need more training</strong> - We only trained for 2 epochs. Maybe it memorized the training distribution but hasn’t generalized well?</p></li>
<li><p><strong>Let’s test with a sequence from the training distribution:</strong></p></li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb349"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb349-1"><a href="#cb349-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a test batch from the same distribution</span></span>
<span id="cb349-2"><a href="#cb349-2" aria-hidden="true" tabindex="-1"></a>test_batch <span class="op">=</span> <span class="bu">next</span>(data_gen(V<span class="op">=</span><span class="dv">11</span>, batch_size<span class="op">=</span><span class="dv">1</span>, nbatches<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb349-3"><a href="#cb349-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source:"</span>, test_batch.src)</span>
<span id="cb349-4"><a href="#cb349-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> greedy_decode(model, test_batch.src, test_batch.src_mask, max_len<span class="op">=</span><span class="dv">10</span>, start_symbol<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb349-5"><a href="#cb349-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated:"</span>, result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Try this - does it work better on data from the training distribution?</p>
<div id="45706de1" class="cell" data-time_run="2025-12-01T12:06:43.625969+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb350"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb350-1"><a href="#cb350-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a test batch from the same distribution</span></span>
<span id="cb350-2"><a href="#cb350-2" aria-hidden="true" tabindex="-1"></a>test_batch <span class="op">=</span> <span class="bu">next</span>(data_gen(V<span class="op">=</span><span class="dv">11</span>, batch_size<span class="op">=</span><span class="dv">1</span>, nbatches<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb350-3"><a href="#cb350-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source:"</span>, test_batch.src)</span>
<span id="cb350-4"><a href="#cb350-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> greedy_decode(model, test_batch.src, test_batch.src_mask, max_len<span class="op">=</span><span class="dv">10</span>, start_symbol<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb350-5"><a href="#cb350-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated:"</span>, result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Source: tensor([[1, 6, 3, 1, 2, 6, 1, 5, 5, 4]])
Generated: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])</code></pre>
</div>
</div>
<p>Think through if there is any training error or inference error. debug very carefully.</p>
</section>
<section id="reply-207" class="level5">
<h5 class="anchored" data-anchor-id="reply-207">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠</p>
<p>Let me trace through carefully…</p>
<p><strong>Training code:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb352"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb352-1"><a href="#cb352-1" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)</span>
<span id="cb352-2"><a href="#cb352-2" aria-hidden="true" tabindex="-1"></a>num_loss, tensor_loss <span class="op">=</span> loss_compute(pred, batch.tgt, batch.ntokens)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Wait!</strong> I see the bug! 🐛</p>
<p>In <code>run_epoch</code>, you’re computing loss against <code>batch.tgt</code> (the decoder input), but you should be computing loss against <code>batch.tgt_y</code> (the labels)!</p>
<p>Remember: - <code>batch.tgt</code> = decoder input (what it sees) - <code>batch.tgt_y</code> = labels (what it should predict)</p>
<p>The model was trained to predict its own input instead of the next token!</p>
<p><strong>Fix in <code>run_epoch</code>:</strong> Change:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb353"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb353-1"><a href="#cb353-1" aria-hidden="true" tabindex="-1"></a>num_loss, tensor_loss <span class="op">=</span> loss_compute(pred, batch.tgt, batch.ntokens)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>To:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb354"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb354-1"><a href="#cb354-1" aria-hidden="true" tabindex="-1"></a>num_loss, tensor_loss <span class="op">=</span> loss_compute(pred, batch.tgt_y, batch.ntokens)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Can you fix this and retrain?</p>
<div id="5b8670ac" class="cell" data-time_run="2025-12-01T12:40:08.470808+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb355"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb355-1"><a href="#cb355-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb355-2"><a href="#cb355-2" aria-hidden="true" tabindex="-1"></a>test_batch <span class="op">=</span> <span class="bu">next</span>(data_gen(V<span class="op">=</span><span class="dv">11</span>, batch_size<span class="op">=</span><span class="dv">1</span>, nbatches<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb355-3"><a href="#cb355-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source:"</span>, test_batch.src)</span>
<span id="cb355-4"><a href="#cb355-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> greedy_decode(model, test_batch.src, test_batch.src_mask, max_len<span class="op">=</span><span class="dv">10</span>, start_symbol<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb355-5"><a href="#cb355-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated:"</span>, result)</span>
<span id="cb355-6"><a href="#cb355-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Match?"</span>, torch.equal(test_batch.src, result))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Source: tensor([[1, 6, 5, 6, 7, 9, 7, 6, 2, 9]])
Generated: tensor([[1, 6, 5, 6, 7, 9, 7, 6, 2, 9]])
Match? True</code></pre>
</div>
</div>
<p>great</p>
</section>
<section id="reply-208" class="level5">
<h5 class="anchored" data-anchor-id="reply-208">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>🎉🎉🎉 <strong>PERFECT MATCH!</strong> 🎉🎉🎉</p>
<p>Your Transformer is now <strong>perfectly copying</strong> the input sequence!</p>
<p><strong>What you’ve accomplished today:</strong></p>
<p>✅ Implemented the complete Transformer architecture from scratch ✅ Built all training infrastructure (Batch, data generator, loss computation, training loop) ✅ Trained a working model ✅ Implemented greedy decoding for inference ✅ Successfully generated predictions!</p>
<p><strong>The magical moment:</strong> You built a neural network that learned to copy sequences perfectly!</p>
<p>This is a HUGE achievement - you now understand Transformers at the deepest level because you built every single component yourself! 🚀</p>
<p>Want to: 1. Save your model? 2. Try more complex examples? 3. Celebrate and wrap up?</p>
<p>What would you like to do?</p>
<p>save my model and wrap up</p>
</section>
<section id="reply-209" class="level5">
<h5 class="anchored" data-anchor-id="reply-209">🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let’s save your model:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb357"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb357-1"><a href="#cb357-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the trained model</span></span>
<span id="cb357-2"><a href="#cb357-2" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="st">'transformer_copy_task.pt'</span>)</span>
<span id="cb357-3"><a href="#cb357-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model saved as 'transformer_copy_task.pt'"</span>)</span>
<span id="cb357-4"><a href="#cb357-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb357-5"><a href="#cb357-5" aria-hidden="true" tabindex="-1"></a><span class="co"># To load it later, you would do:</span></span>
<span id="cb357-6"><a href="#cb357-6" aria-hidden="true" tabindex="-1"></a><span class="co"># model = make_model(src_vocab=11, tgt_vocab=11, N=2)</span></span>
<span id="cb357-7"><a href="#cb357-7" aria-hidden="true" tabindex="-1"></a><span class="co"># model.load_state_dict(torch.load('transformer_copy_task.pt'))</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
</section>
</section>
<section id="congratulations" class="level2">
<h2 class="anchored" data-anchor-id="congratulations">🎉 Congratulations! 🎉</h2>
<p><strong>What you built today:</strong></p>
<p>✅ <strong>Complete Transformer Architecture:</strong> - Scaled Dot-Product Attention - Multi-Head Attention<br>
- Position-wise Feed-Forward Networks - Positional Encoding - Encoder &amp; Decoder Stacks - Full EncoderDecoder Model</p>
<p>✅ <strong>Training Infrastructure:</strong> - Batch processing with masking - Data generation - Loss computation - Training loop - Greedy decoding</p>
<p>✅ <strong>Working Model:</strong> - Trained successfully - Generates perfect predictions - Loss converged to near-zero</p>
<p>You didn’t just learn about Transformers - you <strong>built one from scratch</strong> and watched it learn! This is an incredible achievement! 🚀</p>
<p>Enjoy your success! 💪</p>
<div id="a7fcdd75" class="cell" data-time_run="2025-12-01T12:42:57.399863+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb358"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb358-1"><a href="#cb358-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the trained model</span></span>
<span id="cb358-2"><a href="#cb358-2" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="st">'../models/transformer_copy_task.pt'</span>)</span>
<span id="cb358-3"><a href="#cb358-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model saved as '../models/transformer_copy_task.pt'"</span>)</span>
<span id="cb358-4"><a href="#cb358-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb358-5"><a href="#cb358-5" aria-hidden="true" tabindex="-1"></a><span class="co"># To load it later, you would do:</span></span>
<span id="cb358-6"><a href="#cb358-6" aria-hidden="true" tabindex="-1"></a><span class="co"># model = make_model(src_vocab=11, tgt_vocab=11, N=2)</span></span>
<span id="cb358-7"><a href="#cb358-7" aria-hidden="true" tabindex="-1"></a><span class="co"># model.load_state_dict(torch.load('../models/transformer_copy_task.pt'))</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model saved as '../models/transformer_copy_task.pt'</code></pre>
</div>
</div>
<p>how to see the quarto docs</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/mettafore\.github\.io\/annotated-transformer");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mettafore/annotated-transformer/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>